{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrWgroYshpiRgFdGXtg0qq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codingniket/Python-Training/blob/main/18-12-2025/Excercise_2_18_12_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATASET 1 — USER REGISTRATION\n",
        "(CORRUPTED SCHEMA)"
      ],
      "metadata": {
        "id": "dqk32W--1cbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"StructType\").getOrCreate()\n",
        "\n",
        "from pyspark.sql.types import (StructType, StructField, StringType,LongType,IntegerType,ArrayType,MapType)"
      ],
      "metadata": {
        "id": "hwuYL-C35ifi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EC3hLOPMumIj"
      },
      "outputs": [],
      "source": [
        "raw_users = [\n",
        "(\"U001\",\"Amit\",\"28\",\"Hyderabad\",\"['AI','ML','Cloud']\"),\n",
        "(\"U002\",\"Neha\",\"Thirty\",\"Delhi\",\"AI,Testing\"),\n",
        "(\"U003\",\"Ravi\",None,\"Bangalore\",[\"Data\",\"Spark\"]),\n",
        "(\"U004\",\"Pooja\",\"29\",\"Mumbai\",None),\n",
        "(\"U005\",\"\", \"31\",\"Chennai\",\"['DevOps']\")\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), nullable=False),\n",
        "    StructField(\"name\", StringType(), nullable=True),\n",
        "    StructField(\"age\", StringType(), nullable=True),\n",
        "    StructField(\"city\", StringType(), nullable=True),\n",
        "    StructField(\"skills\", StringType(), nullable=True)\n",
        "])\n",
        "\n",
        "\n",
        "skills_schema = StructType([\n",
        "    StructField(\"domain\", StringType(), nullable=False),\n",
        "])\n",
        "\n",
        "print(\"Defined user and skills schema.\")"
      ],
      "metadata": {
        "id": "NfWSJ5BI5Sky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01f8af44-7fc1-4c5c-f236-e84b81808d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined user and skills schema.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_data = spark.createDataFrame(raw_users, user_schema)\n",
        "df_data.show()\n",
        "df_data.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mz9WnxpK6aoh",
        "outputId": "909f8bed-7972-4766-86ae-9452fc61eb7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+------+---------+-------------------+\n",
            "|user_id| name|   age|     city|             skills|\n",
            "+-------+-----+------+---------+-------------------+\n",
            "|   U001| Amit|    28|Hyderabad|['AI','ML','Cloud']|\n",
            "|   U002| Neha|Thirty|    Delhi|         AI,Testing|\n",
            "|   U003| Ravi|  NULL|Bangalore|      [Data, Spark]|\n",
            "|   U004|Pooja|    29|   Mumbai|               NULL|\n",
            "|   U005|     |    31|  Chennai|         ['DevOps']|\n",
            "+-------+-----+------+---------+-------------------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- skills: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "\n",
        "clean_data = df_data.withColumn(\n",
        "    \"name\",\n",
        "    when(col(\"name\") == \"\", None).otherwise(col(\"name\"))\n",
        ")\n",
        "\n",
        "clean_data = clean_data.withColumn(\"age\", when(col(\"age\") == \"\", None)\n",
        "    .when(col(\"age\").rlike(r\"^\\d+$\"),\n",
        "          col(\"age\").cast(IntegerType()))\n",
        "    .otherwise(None))\n",
        "\n",
        "clean_data.show()\n",
        "clean_data.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69J1mYWr6jX8",
        "outputId": "ba82f622-6486-4f3a-ceae-02376d547a2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+----+---------+-------------------+\n",
            "|user_id| name| age|     city|             skills|\n",
            "+-------+-----+----+---------+-------------------+\n",
            "|   U001| Amit|  28|Hyderabad|['AI','ML','Cloud']|\n",
            "|   U002| Neha|NULL|    Delhi|         AI,Testing|\n",
            "|   U003| Ravi|NULL|Bangalore|      [Data, Spark]|\n",
            "|   U004|Pooja|  29|   Mumbai|               NULL|\n",
            "|   U005| NULL|  31|  Chennai|         ['DevOps']|\n",
            "+-------+-----+----+---------+-------------------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- skills: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, regexp_replace, split, trim, array_compact, transform\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "clean_data = clean_data.withColumn(\n",
        "    \"skills\",\n",
        "    (when(\n",
        "        col(\"skills\").isNull(),\n",
        "        None\n",
        "    ).otherwise(\n",
        "        array_compact(\n",
        "            transform(\n",
        "                split(\n",
        "                    regexp_replace(col(\"skills\"), \"[\\[\\]']\", \"\"),\n",
        "                    \",\"\n",
        "                ),\n",
        "                lambda x: trim(x)\n",
        "            )\n",
        "        )\n",
        "    )).cast(ArrayType(StringType()))\n",
        ")\n",
        "\n",
        "clean_data.show(truncate=False)\n",
        "clean_data.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Cl69eES8kQJ",
        "outputId": "9da05f3c-4d06-4611-9283-9a5bd8681981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:13: SyntaxWarning: invalid escape sequence '\\['\n",
            "<>:13: SyntaxWarning: invalid escape sequence '\\['\n",
            "/tmp/ipython-input-3482429054.py:13: SyntaxWarning: invalid escape sequence '\\['\n",
            "  regexp_replace(col(\"skills\"), \"[\\[\\]']\", \"\"),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+----+---------+---------------+\n",
            "|user_id|name |age |city     |skills         |\n",
            "+-------+-----+----+---------+---------------+\n",
            "|U001   |Amit |28  |Hyderabad|[AI, ML, Cloud]|\n",
            "|U002   |Neha |NULL|Delhi    |[AI, Testing]  |\n",
            "|U003   |Ravi |NULL|Bangalore|[Data, Spark]  |\n",
            "|U004   |Pooja|29  |Mumbai   |NULL           |\n",
            "|U005   |NULL |31  |Chennai  |[DevOps]       |\n",
            "+-------+-----+----+---------+---------------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- skills: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "users_df = clean_data"
      ],
      "metadata": {
        "id": "XJUDuZqQecgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATASET 2 — COURSE CATALOG (NESTED\n",
        "STRUCT)"
      ],
      "metadata": {
        "id": "MeAk2EA6U2mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = [\n",
        " (\"C001\",\"PySpark Mastery\",(\"Data Engineering\",\"Advanced\"),\"₹9999\"),\n",
        " (\"C002\",\"AI for Testers\",{\"domain\":\"QA\",\"level\":\"Beginner\"},\"8999\"),\n",
        " (\"C003\",\"ML Foundations\",(\"AI\",\"Intermediate\"),None),\n",
        " (\"C004\",\"Data Engineering Bootcamp\",\"Data|Advanced\",\"₹14999\")\n",
        "]"
      ],
      "metadata": {
        "id": "S0HaQYhBCxNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Create nested StructType for course metadata\n",
        "2. Normalize domain and level\n",
        "3. Convert price to IntegerType\n",
        "4. Handle missing prices\n",
        "5. Produce courses_df"
      ],
      "metadata": {
        "id": "10fMwNq4VK39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"StructType\").getOrCreate()\n",
        "\n",
        "from pyspark.sql.types import (StructType, StructField, StringType,LongType,IntegerType,ArrayType,MapType)"
      ],
      "metadata": {
        "id": "OKKlrDynVt5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "course_schema = StructType([\n",
        "    StructField(\"course_id\", StringType(), nullable=False),\n",
        "    StructField(\"course\", StringType(), nullable=True),\n",
        "    StructField(\"skills\", StringType(), nullable=True),\n",
        "     StructField(\"amount\", StringType(), nullable=True),\n",
        "])\n"
      ],
      "metadata": {
        "id": "BVItIds7VJYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_skills(skill):\n",
        "    if isinstance(skill, (list, tuple)):\n",
        "        return \"|\".join(skill)\n",
        "    elif isinstance(skill, dict):\n",
        "        return \"|\".join(skill.values())\n",
        "    else:\n",
        "        return skill\n",
        "data2_cleaned = [\n",
        "    (u, c, normalize_skills(s), a)\n",
        "    for u, c, s, a in data2\n",
        "]\n",
        "df_clean = spark.createDataFrame(data2_cleaned, course_schema)\n",
        "df_clean.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys7bGjgjaFO3",
        "outputId": "e047923b-b2f2-425f-c464-833e9130ef2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------------+-------------------------+------+\n",
            "|course_id|course                   |skills                   |amount|\n",
            "+---------+-------------------------+-------------------------+------+\n",
            "|C001     |PySpark Mastery          |Data Engineering|Advanced|₹9999 |\n",
            "|C002     |AI for Testers           |QA|Beginner              |8999  |\n",
            "|C003     |ML Foundations           |AI|Intermediate          |NULL  |\n",
            "|C004     |Data Engineering Bootcamp|Data|Advanced            |₹14999|\n",
            "+---------+-------------------------+-------------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, substring, split, trim,regexp_replace\n",
        "clean_data = df_clean.withColumn(\n",
        "    \"amount\",\n",
        "    when(col(\"amount\").isNull() , 0 ).otherwise(regexp_replace(col(\"amount\"), \"₹\", \"\"))\n",
        "    .cast('int')\n",
        ")\n",
        "\n",
        "\n",
        "clean_data.show(truncate=False)\n",
        "clean_data.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x20rfy4-Vqsc",
        "outputId": "5bb8695c-a372-457d-a314-7a841db9d179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------------+-------------------------+------+\n",
            "|course_id|course                   |skills                   |amount|\n",
            "+---------+-------------------------+-------------------------+------+\n",
            "|C001     |PySpark Mastery          |Data Engineering|Advanced|9999  |\n",
            "|C002     |AI for Testers           |QA|Beginner              |8999  |\n",
            "|C003     |ML Foundations           |AI|Intermediate          |0     |\n",
            "|C004     |Data Engineering Bootcamp|Data|Advanced            |14999 |\n",
            "+---------+-------------------------+-------------------------+------+\n",
            "\n",
            "root\n",
            " |-- course_id: string (nullable = false)\n",
            " |-- course: string (nullable = true)\n",
            " |-- skills: string (nullable = true)\n",
            " |-- amount: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_array = clean_data.withColumn(\n",
        "    \"skills\",\n",
        "    split(col(\"skills\"), \"\\\\|\")\n",
        ")\n",
        "\n",
        "clean_data.show(truncate=False)\n",
        "clean_data.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5F2LC2IYwAI",
        "outputId": "959c9133-ba8c-4b48-94de-6a1dd88f8117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------------+-------------------------+------+\n",
            "|course_id|course                   |skills                   |amount|\n",
            "+---------+-------------------------+-------------------------+------+\n",
            "|C001     |PySpark Mastery          |Data Engineering|Advanced|9999  |\n",
            "|C002     |AI for Testers           |QA|Beginner              |8999  |\n",
            "|C003     |ML Foundations           |AI|Intermediate          |0     |\n",
            "|C004     |Data Engineering Bootcamp|Data|Advanced            |14999 |\n",
            "+---------+-------------------------+-------------------------+------+\n",
            "\n",
            "root\n",
            " |-- course_id: string (nullable = false)\n",
            " |-- course: string (nullable = true)\n",
            " |-- skills: string (nullable = true)\n",
            " |-- amount: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, create_map\n",
        "from pyspark.sql.types import MapType, StringType\n",
        "\n",
        "df_map = df_array.withColumn(\n",
        "    \"skills\",\n",
        "    create_map(col(\"skills\").getItem(0), col(\"skills\").getItem(1)).cast(MapType(StringType(), StringType()))\n",
        ")\n",
        "\n",
        "df_map.show(truncate=False)\n",
        "df_map.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCwB_tFIZlq_",
        "outputId": "696a486a-42f2-41b8-b791-78b9b0a24d18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------------+------------------------------+------+\n",
            "|course_id|course                   |skills                        |amount|\n",
            "+---------+-------------------------+------------------------------+------+\n",
            "|C001     |PySpark Mastery          |{Data Engineering -> Advanced}|9999  |\n",
            "|C002     |AI for Testers           |{QA -> Beginner}              |8999  |\n",
            "|C003     |ML Foundations           |{AI -> Intermediate}          |0     |\n",
            "|C004     |Data Engineering Bootcamp|{Data -> Advanced}            |14999 |\n",
            "+---------+-------------------------+------------------------------+------+\n",
            "\n",
            "root\n",
            " |-- course_id: string (nullable = false)\n",
            " |-- course: string (nullable = true)\n",
            " |-- skills: map (nullable = false)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            " |-- amount: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "courses_df = df_map"
      ],
      "metadata": {
        "id": "ZoyyrsRqegpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATASET 3 — USER COURSE ENROLLMENTS\n",
        "(JOIN + BROADCAST)"
      ],
      "metadata": {
        "id": "1fhdlNabeRnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data3 = [\n",
        " (\"U001\",\"C001\",\"2024-01-05\"),\n",
        " (\"U002\",\"C002\",\"05/01/2024\"),\n",
        " (\"U003\",\"C001\",\"2024/06/01\"),\n",
        " (\"U004\",\"C003\",\"invalid_date\"),\n",
        " (\"U001\",\"C004\",\"2024-01-10\")\n",
        "]\n"
      ],
      "metadata": {
        "id": "v_F3hGcRcC_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enroll_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), False),\n",
        "    StructField(\"course_id\", StringType(), False),\n",
        "    StructField(\"enroll_date_raw\", StringType(), True)\n",
        "])\n",
        "\n",
        "enroll_df = spark.createDataFrame(data3, enroll_schema)\n",
        "\n",
        "enroll_df.show(truncate=False)\n",
        "enroll_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLxhKU4UeWG4",
        "outputId": "997ac2d7-1f44-42fb-b332-463d2beddf16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+---------------+\n",
            "|user_id|course_id|enroll_date_raw|\n",
            "+-------+---------+---------------+\n",
            "|U001   |C001     |2024-01-05     |\n",
            "|U002   |C002     |05/01/2024     |\n",
            "|U003   |C001     |2024/06/01     |\n",
            "|U004   |C003     |invalid_date   |\n",
            "|U001   |C004     |2024-01-10     |\n",
            "+-------+---------+---------------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- course_id: string (nullable = false)\n",
            " |-- enroll_date_raw: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fad6836e",
        "outputId": "fda3b540-7858-4de2-d5eb-960a7a9f3d86"
      },
      "source": [
        "from pyspark.sql.functions import coalesce, col, try_to_timestamp, lit\n",
        "from pyspark.sql.types import DateType\n",
        "\n",
        "enroll_normalized = enroll_df.withColumn(\n",
        "    \"enroll_date\",\n",
        "    coalesce(\n",
        "        try_to_timestamp(col(\"enroll_date_raw\"), lit(\"yyyy-MM-dd\")).cast(DateType()),\n",
        "        try_to_timestamp(col(\"enroll_date_raw\"), lit(\"dd/MM/yyyy\")).cast(DateType()),\n",
        "        try_to_timestamp(col(\"enroll_date_raw\"), lit(\"yyyy/MM/dd\")).cast(DateType())\n",
        "    )\n",
        ").drop(\"enroll_date_raw\")\n",
        "\n",
        "enroll_normalized.show(truncate=False)\n",
        "enroll_normalized.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+-----------+\n",
            "|user_id|course_id|enroll_date|\n",
            "+-------+---------+-----------+\n",
            "|U001   |C001     |2024-01-05 |\n",
            "|U002   |C002     |2024-01-05 |\n",
            "|U003   |C001     |2024-06-01 |\n",
            "|U004   |C003     |NULL       |\n",
            "|U001   |C004     |2024-01-10 |\n",
            "+-------+---------+-----------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- course_id: string (nullable = false)\n",
            " |-- enroll_date: date (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_enrollments = enroll_normalized.filter(\n",
        "    col(\"enroll_date\").isNotNull()\n",
        ")\n",
        "\n",
        "\n",
        "enroll_users_df = valid_enrollments.join(\n",
        "    users_df,\n",
        "    on=\"user_id\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "final_df = enroll_users_df.join(\n",
        "    courses_df,\n",
        "    on=\"course_id\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "final_df.show(truncate=False)\n",
        "\n",
        "# users_df.show(truncate=False)\n",
        "# courses_df.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRwOm-cGfJnN",
        "outputId": "e89b9ce0-85c5-4da7-a116-df4271756c2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+-----------+------+------+------+-------------------------+------------------------------+------+\n",
            "|course_id|user_id|enroll_date|course|skills|amount|course                   |skills                        |amount|\n",
            "+---------+-------+-----------+------+------+------+-------------------------+------------------------------+------+\n",
            "|C002     |U002   |2024-01-05 |NULL  |NULL  |NULL  |AI for Testers           |{QA -> Beginner}              |8999  |\n",
            "|C001     |U001   |2024-01-05 |NULL  |NULL  |NULL  |PySpark Mastery          |{Data Engineering -> Advanced}|9999  |\n",
            "|C001     |U003   |2024-06-01 |NULL  |NULL  |NULL  |PySpark Mastery          |{Data Engineering -> Advanced}|9999  |\n",
            "|C004     |U001   |2024-01-10 |NULL  |NULL  |NULL  |Data Engineering Bootcamp|{Data -> Advanced}            |14999 |\n",
            "+---------+-------+-----------+------+------+------+-------------------------+------------------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATASET 4 — USER ACTIVITY LOGS (ARRAY +\n",
        "MAP)"
      ],
      "metadata": {
        "id": "hhHeWlXvnDVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercises\n",
        "1. Normalize actions into ArrayType\n",
        "2. Normalize metadata into MapType\n",
        "3. Handle missing actions safely\n",
        "4. Explode actions and count frequency\n",
        "5. Produce activity_df\n"
      ],
      "metadata": {
        "id": "dPAz6Pcqtre0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, split, when, explode, from_json, create_map, lit\n",
        "from pyspark.sql.types import ArrayType, StringType, MapType\n",
        "import json\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Raw data\n",
        "raw_activity = [\n",
        "    (\"U001\",\"login,watch,logout\",\"{'device':'mobile','ip':'1.1.1.1'}\",120),\n",
        "    (\"U002\",[\"login\",\"watch\"],\"device=laptop;ip=2.2.2.2\",90),\n",
        "    (\"U003\",\"login|logout\",None,30),\n",
        "    (\"U004\",None,\"{'device':'tablet'}\",60)\n",
        "]\n",
        "\n",
        "\n",
        "course_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), nullable=False),\n",
        "    StructField(\"actions\", StringType(), nullable=True),\n",
        "    StructField(\"metadata\", StringType(), nullable=True),\n",
        "     StructField(\"duration\", IntegerType(), nullable=True),\n",
        "])\n",
        "\n",
        "df = spark.createDataFrame(raw_activity, course_schema)\n",
        "\n",
        "df.show(truncate=False)\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFpXQJvIl0uT",
        "outputId": "27424b11-3227-4fe4-bf6f-fb2473c2304b"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+----------------------------------+--------+\n",
            "|user_id|actions           |metadata                          |duration|\n",
            "+-------+------------------+----------------------------------+--------+\n",
            "|U001   |login,watch,logout|{'device':'mobile','ip':'1.1.1.1'}|120     |\n",
            "|U002   |[login, watch]    |device=laptop;ip=2.2.2.2          |90      |\n",
            "|U003   |login|logout      |NULL                              |30      |\n",
            "|U004   |NULL              |{'device':'tablet'}               |60      |\n",
            "+-------+------------------+----------------------------------+--------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- actions: string (nullable = true)\n",
            " |-- metadata: string (nullable = true)\n",
            " |-- duration: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import (\n",
        "    StructType, StructField, StringType, IntegerType,\n",
        "    ArrayType, MapType\n",
        ")\n",
        "from pyspark.sql.functions import (\n",
        "    col, split, regexp_replace, trim, when, coalesce,\n",
        "    from_json, explode, count, lit, array, size,\n",
        "    expr, udf\n",
        ")\n",
        "df_raw = df\n",
        "df_with_actions = df_raw.withColumn(\n",
        "    \"actions_normalized\",\n",
        "    when(col(\"actions\").isNull(), array())  # Empty array for null\n",
        "    .when(col(\"actions\").contains(\"['\"),\n",
        "          # Handle Python list format: ['login', 'watch']\n",
        "          split(\n",
        "              regexp_replace(\n",
        "                  regexp_replace(col(\"actions\"), \"\\\\[|\\\\]|'\", \"\"),\n",
        "                  \" \", \"\"\n",
        "              ),\n",
        "              \",\"\n",
        "          )\n",
        "    )\n",
        "    .when(col(\"actions\").contains(\"|\"),\n",
        "          # Handle pipe delimiter\n",
        "          split(col(\"actions\"), \"\\\\|\")\n",
        "    )\n",
        "    .otherwise(\n",
        "          # Handle comma delimiter (default)\n",
        "          split(col(\"actions\"), \",\")\n",
        "    )\n",
        ").withColumn(\n",
        "    # Trim whitespace from each action\n",
        "    \"actions_array\",\n",
        "    expr(\"transform(actions_normalized, x -> trim(x))\")\n",
        ").withColumn(\n",
        "    # Filter out empty strings\n",
        "    \"actions_array\",\n",
        "    expr(\"filter(actions_array, x -> x != '')\")\n",
        ")"
      ],
      "metadata": {
        "id": "tUpU9u01n8cL"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_actions.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyL9KbRQtMOO",
        "outputId": "b316af27-f6f1-4687-f8f4-1abbe49c7052"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+----------------------------------+--------+----------------------+----------------------+\n",
            "|user_id|actions           |metadata                          |duration|actions_normalized    |actions_array         |\n",
            "+-------+------------------+----------------------------------+--------+----------------------+----------------------+\n",
            "|U001   |login,watch,logout|{'device':'mobile','ip':'1.1.1.1'}|120     |[login, watch, logout]|[login, watch, logout]|\n",
            "|U002   |[login, watch]    |device=laptop;ip=2.2.2.2          |90      |[[login,  watch]]     |[[login, watch]]      |\n",
            "|U003   |login|logout      |NULL                              |30      |[login, logout]       |[login, logout]       |\n",
            "|U004   |NULL              |{'device':'tablet'}               |60      |[]                    |[]                    |\n",
            "+-------+------------------+----------------------------------+--------+----------------------+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_metadata(metadata_str) -> dict[str, str]:\n",
        "    if metadata_str is None:\n",
        "        return {}\n",
        "\n",
        "    metadata_str = metadata_str.strip()\n",
        "    result = {}\n",
        "\n",
        "    try:\n",
        "        # Try parsing as JSON-like string (with single quotes)\n",
        "        if metadata_str.startswith(\"{\") and metadata_str.endswith(\"}\"):\n",
        "            # Replace single quotes with double quotes for valid JSON\n",
        "            json_str = metadata_str.replace(\"'\", '\"')\n",
        "            result = json.loads(json_str)\n",
        "        # Parse key=value;key=value format\n",
        "        elif \"=\" in metadata_str:\n",
        "            pairs = metadata_str.split(\";\")\n",
        "            for pair in pairs:\n",
        "                if \"=\" in pair:\n",
        "                    key, value = pair.split(\"=\", 1)\n",
        "                    result[key.strip()] = value.strip()\n",
        "    except Exception as e:\n",
        "        # Return empty dict if parsing fails\n",
        "        pass\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "dhdENJPztchO"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parse_metadata_udf = udf(parse_metadata, MapType(StringType(), StringType()))\n",
        "\n",
        "df_with_metadata = df_with_actions.withColumn(\n",
        "    \"metadata_map\",\n",
        "    parse_metadata_udf(col(\"metadata\"))\n",
        ")"
      ],
      "metadata": {
        "id": "9Z3ZZU1ot9ik"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activity_df = df_with_metadata.select(\n",
        "    col(\"user_id\"),\n",
        "    col(\"actions_array\").alias(\"actions\"),\n",
        "    col(\"metadata_map\").alias(\"metadata\"),\n",
        "    col(\"duration\")\n",
        ").withColumn(\n",
        "    \"action_count\",\n",
        "    size(col(\"actions\"))\n",
        ").withColumn(\n",
        "    \"has_actions\",\n",
        "    col(\"action_count\") > 0\n",
        ")\n"
      ],
      "metadata": {
        "id": "6DgY1QFNuEAY"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activity_df.show(truncate=False)\n",
        "activity_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rOLv7eauGRi",
        "outputId": "7454c1d8-1365-4df4-f338-ef0a6fdde67a"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------------+---------------------------------+--------+------------+-----------+\n",
            "|user_id|actions               |metadata                         |duration|action_count|has_actions|\n",
            "+-------+----------------------+---------------------------------+--------+------------+-----------+\n",
            "|U001   |[login, watch, logout]|{device -> mobile, ip -> 1.1.1.1}|120     |3           |true       |\n",
            "|U002   |[[login, watch]]      |{device -> laptop, ip -> 2.2.2.2}|90      |2           |true       |\n",
            "|U003   |[login, logout]       |{}                               |30      |2           |true       |\n",
            "|U004   |[]                    |{device -> tablet}               |60      |0           |false      |\n",
            "+-------+----------------------+---------------------------------+--------+------------+-----------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- actions: array (nullable = true)\n",
            " |    |-- element: string (containsNull = false)\n",
            " |-- metadata: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            " |-- duration: integer (nullable = true)\n",
            " |-- action_count: integer (nullable = true)\n",
            " |-- has_actions: boolean (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_exploded = activity_df.filter(col(\"has_actions\") == True).select(\n",
        "    col(\"user_id\"),\n",
        "    explode(col(\"actions\")).alias(\"action\"),\n",
        "    col(\"metadata\"),\n",
        "    col(\"duration\")\n",
        ")"
      ],
      "metadata": {
        "id": "feAJfjh4uJha"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_frequency = df_exploded.groupBy(\"action\").agg(\n",
        "    count(\"*\").alias(\"frequency\")\n",
        ").orderBy(col(\"frequency\").desc())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ACTION FREQUENCY\")\n",
        "print(\"=\" * 80)\n",
        "action_frequency.show(truncate=False)\n",
        "\n",
        "# User-Action frequency (actions per user)\n",
        "user_action_frequency = df_exploded.groupBy(\"user_id\", \"action\").agg(\n",
        "    count(\"*\").alias(\"frequency\")\n",
        ").orderBy(\"user_id\", col(\"frequency\").desc())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"USER-ACTION FREQUENCY\")\n",
        "print(\"=\" * 80)\n",
        "user_action_frequency.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ebILezauQlg",
        "outputId": "90bcaa4f-949e-4be9-886b-715ce8b7878b"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ACTION FREQUENCY\n",
            "================================================================================\n",
            "+------+---------+\n",
            "|action|frequency|\n",
            "+------+---------+\n",
            "|logout|2        |\n",
            "|login |2        |\n",
            "|watch |1        |\n",
            "|[login|1        |\n",
            "|watch]|1        |\n",
            "+------+---------+\n",
            "\n",
            "\n",
            "================================================================================\n",
            "USER-ACTION FREQUENCY\n",
            "================================================================================\n",
            "+-------+------+---------+\n",
            "|user_id|action|frequency|\n",
            "+-------+------+---------+\n",
            "|U001   |login |1        |\n",
            "|U001   |watch |1        |\n",
            "|U001   |logout|1        |\n",
            "|U002   |watch]|1        |\n",
            "|U002   |[login|1        |\n",
            "|U003   |logout|1        |\n",
            "|U003   |login |1        |\n",
            "+-------+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATASET 5 — PAYMENTS (WINDOW +\n",
        "AGGREGATES)"
      ],
      "metadata": {
        "id": "L3q2U5w7usMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercises\n",
        "1. Convert dates properly\n",
        "2. Compute total spend per user (GroupBy)\n",
        "3. Compute running spend per user (Window)\n",
        "4. Rank users by total spend\n",
        "5. Compare GroupBy vs Window outputs"
      ],
      "metadata": {
        "id": "07MkiqGAutSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_payments = [\n",
        " (\"U001\",\"2024-01-05\",9999),\n",
        " (\"U001\",\"2024-01-10\",14999),\n",
        " (\"U002\",\"2024-01-06\",8999),\n",
        " (\"U003\",\"2024-01-07\",0),\n",
        " (\"U004\",\"2024-01-08\",7999),\n",
        " (\"U001\",\"2024-01-15\",1999)\n",
        "]\n"
      ],
      "metadata": {
        "id": "ekqjyrtvuX0v"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), False),\n",
        "    StructField(\"payment_date\", StringType(), False),\n",
        "    StructField(\"amount\", IntegerType(), False)\n",
        "])\n",
        "\n",
        "df = spark.createDataFrame(raw_payments, schema)"
      ],
      "metadata": {
        "id": "bvMGXsZvuxjC"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_processed = df.withColumn(\n",
        "    \"date_normalized\",\n",
        "    coalesce(\n",
        "        try_to_timestamp(col(\"payment_date\"), lit(\"yyyy-MM-dd\")).cast(DateType()),\n",
        "        try_to_timestamp(col(\"payment_date\"), lit(\"dd/MM/yyyy\")).cast(DateType()),\n",
        "        try_to_timestamp(col(\"payment_date\"), lit(\"yyyy/MM/dd\")).cast(DateType())\n",
        "    ))\n",
        "\n",
        "df_processed.show(truncate=False)\n",
        "df_processed.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZKmmeb2vG14",
        "outputId": "4940f691-9e44-4411-d044-2f1a88a617c4"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+------+---------------+\n",
            "|user_id|payment_date|amount|date_normalized|\n",
            "+-------+------------+------+---------------+\n",
            "|U001   |2024-01-05  |9999  |2024-01-05     |\n",
            "|U001   |2024-01-10  |14999 |2024-01-10     |\n",
            "|U002   |2024-01-06  |8999  |2024-01-06     |\n",
            "|U003   |2024-01-07  |0     |2024-01-07     |\n",
            "|U004   |2024-01-08  |7999  |2024-01-08     |\n",
            "|U001   |2024-01-15  |1999  |2024-01-15     |\n",
            "+-------+------------+------+---------------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- payment_date: string (nullable = false)\n",
            " |-- amount: integer (nullable = false)\n",
            " |-- date_normalized: date (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import (\n",
        "    col, sum, count, avg, max, min, coalesce, lit,\n",
        "    row_number, rank, dense_rank, percent_rank\n",
        ")\n",
        "total_spend_per_user = df_processed.groupBy(\"user_id\").agg(\n",
        "    sum(\"amount\").alias(\"total_spend\"),\n",
        "    count(\"amount\").alias(\"transaction_count\"),\n",
        "    avg(\"amount\").alias(\"avg_spend\"),\n",
        "    max(\"amount\").alias(\"max_spend\"),\n",
        "    min(\"amount\").alias(\"min_spend\")\n",
        ").orderBy(col(\"total_spend\").desc())\n",
        "\n",
        "total_spend_per_user.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQEeGn3lvNX5",
        "outputId": "ee914fcb-155f-45c4-a39f-fb125692105a"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+-----------------+---------+---------+---------+\n",
            "|user_id|total_spend|transaction_count|avg_spend|max_spend|min_spend|\n",
            "+-------+-----------+-----------------+---------+---------+---------+\n",
            "|U001   |26997      |3                |8999.0   |14999    |1999     |\n",
            "|U002   |8999       |1                |8999.0   |8999     |8999     |\n",
            "|U004   |7999       |1                |7999.0   |7999     |7999     |\n",
            "|U003   |0          |1                |0.0      |0        |0        |\n",
            "+-------+-----------+-----------------+---------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "window_spec = Window.partitionBy(\"user_id\").orderBy(\"date_normalized\")\n",
        "\n",
        "# Calculate running totals and other window analytics\n",
        "df_with_running_totals = df_processed.withColumn(\n",
        "    \"running_total\",\n",
        "    sum(\"amount\").over(window_spec)\n",
        ").withColumn(\n",
        "    \"transaction_number\",\n",
        "    row_number().over(window_spec)\n",
        ").withColumn(\n",
        "    \"cumulative_avg\",\n",
        "    avg(\"amount\").over(window_spec)\n",
        ")\n",
        "\n",
        "df_with_running_totals.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GCOFcZTvrnR",
        "outputId": "cbee6f75-7efc-400d-a8d5-985156617247"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+------+---------------+-------------+------------------+--------------+\n",
            "|user_id|payment_date|amount|date_normalized|running_total|transaction_number|cumulative_avg|\n",
            "+-------+------------+------+---------------+-------------+------------------+--------------+\n",
            "|U001   |2024-01-05  |9999  |2024-01-05     |9999         |1                 |9999.0        |\n",
            "|U001   |2024-01-10  |14999 |2024-01-10     |24998        |2                 |12499.0       |\n",
            "|U001   |2024-01-15  |1999  |2024-01-15     |26997        |3                 |8999.0        |\n",
            "|U002   |2024-01-06  |8999  |2024-01-06     |8999         |1                 |8999.0        |\n",
            "|U003   |2024-01-07  |0     |2024-01-07     |0            |1                 |0.0           |\n",
            "|U004   |2024-01-08  |7999  |2024-01-08     |7999         |1                 |7999.0        |\n",
            "+-------+------------+------+---------------+-------------+------------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions.builtin import user\n",
        "window_rank_spec = Window.orderBy(col(\"total_spend\").desc())\n",
        "\n",
        "user_rankings = total_spend_per_user.withColumn(\n",
        "    \"rank\",\n",
        "    rank().over(window_rank_spec)\n",
        ").withColumn(\n",
        "    \"dense_rank\",\n",
        "    dense_rank().over(window_rank_spec)\n",
        ").withColumn(\n",
        "    \"row_number\",\n",
        "    row_number().over(window_rank_spec)\n",
        ").withColumn(\n",
        "    \"percent_rank\",\n",
        "    percent_rank().over(window_rank_spec)\n",
        ")\n",
        "\n",
        "user_rankings.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHn8pExBv63Q",
        "outputId": "460d65c0-2e11-413b-88ba-b611e414adc7"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+-----------------+---------+---------+---------+----+----------+----------+------------------+\n",
            "|user_id|total_spend|transaction_count|avg_spend|max_spend|min_spend|rank|dense_rank|row_number|percent_rank      |\n",
            "+-------+-----------+-----------------+---------+---------+---------+----+----------+----------+------------------+\n",
            "|U001   |26997      |3                |8999.0   |14999    |1999     |1   |1         |1         |0.0               |\n",
            "|U002   |8999       |1                |8999.0   |8999     |8999     |2   |2         |2         |0.3333333333333333|\n",
            "|U004   |7999       |1                |7999.0   |7999     |7999     |3   |3         |3         |0.6666666666666666|\n",
            "|U003   |0          |1                |0.0      |0        |0        |4   |4         |4         |1.0               |\n",
            "+-------+-----------+-----------------+---------+---------+---------+----+----------+----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hUx1x8jdwOf7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}