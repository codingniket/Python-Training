{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4-egDzYG6BwW"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"E-CommerceAnalytics\")\\\n",
        ".getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_customers = [\n",
        "(\"C001\",\"Rahul\",\"29\",\"Bangalore\",\"Electronics,Fashion\"),\n",
        "(\"C002\",\"Sneha\",\"Thirty Two\",\"Delhi\",\"Fashion\"),\n",
        "(\"C003\",\"Aman\",None,\"Mumbai\",[\"Home\",\"Electronics\"]),\n",
        "(\"C004\",\"Pallavi\",\"27\",\"Pune\",\"Electronics|Beauty\"),\n",
        "\n",
        "(\"C005\",\"\", \"35\",\"Chennai\",None)\n",
        "]"
      ],
      "metadata": {
        "id": "4wyZisQf6tVO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_sellers = [\n",
        "(\"S001\",\"TechWorld\",\"Electronics\",\"2019-06-01\"),\n",
        "(\"S002\",\"FashionHub\",\"Fashion\",\"01/07/2020\"),\n",
        "(\"S003\",\"HomeEssentials\",\"Home\",\"2018/09/15\"),\n",
        "(\"S004\",\"BeautyStore\",\"Beauty\",\"invalid_date\")\n",
        "]"
      ],
      "metadata": {
        "id": "Vv6dO5c46s9T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_products = [\n",
        "(\"P001\",\"Laptop\",\"Electronics\",\"S001\",\"55000\"),\n",
        "(\"P002\",\"Headphones\",\"Electronics\",\"S001\",\"2500\"),\n",
        "(\"P003\",\"T-Shirt\",\"Fashion\",\"S002\",\"1200\"),\n",
        "(\"P004\",\"Sofa\",\"Home\",\"S003\",\"45000\"),\n",
        "(\"P005\",\"Face Cream\",\"Beauty\",\"S004\",\"800\")\n",
        "]"
      ],
      "metadata": {
        "id": "BCjZMppv6y9V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_orders = [\n",
        "(\"O001\",\"C001\",\"P001\",\"2024-01-05\",\"Delivered\",\"55000\"),\n",
        "(\"O002\",\"C002\",\"P003\",\"05/01/2024\",\"Cancelled\",\"0\"),\n",
        "(\"O003\",\"C003\",\"P004\",\"2024/01/06\",\"Delivered\",\"45000\"),\n",
        "(\"O004\",\"C004\",\"P005\",\"invalid_date\",\"Delivered\",\"800\"),\n",
        "(\"O005\",\"C001\",\"P002\",\"2024-01-10\",\"Delivered\",\"2500\"),\n",
        "(\"O006\",\"C005\",\"P003\",\"2024-01-12\",\"Delivered\",\"1200\")\n",
        "]"
      ],
      "metadata": {
        "id": "fadFnu-k7GD5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_activity = [\n",
        "(\"C001\",\"search,view,add_to_cart\",\"{'device':'mobile'}\",180),\n",
        "(\"C002\",[\"search\",\"view\"],\"device=laptop\",90),\n",
        "(\"C003\",\"search|view|purchase\",None,120),\n",
        "(\"C004\",None,\"{'device':'tablet'}\",60),\n",
        "(\"C005\",\"search\",\"{'device':'mobile'}\",30)\n",
        "]"
      ],
      "metadata": {
        "id": "XvStutdT614H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PART A — DATA CLEANING & STRUCTURING"
      ],
      "metadata": {
        "id": "KnMVqvs2_5Gy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Design explicit schemas for all datasets"
      ],
      "metadata": {
        "id": "rp7Io_Pp_60W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "customer_schema = StructType([\n",
        "StructField(\"customer_id\", StringType(), True),\n",
        "StructField(\"name\", StringType(), True),\n",
        "StructField(\"age\", StringType(), True),\n",
        "StructField(\"city\", StringType(), True),\n",
        "StructField(\"interests\", StringType(), True)\n",
        "])\n",
        "raw_customers_df = spark.createDataFrame(raw_customers, schema=customer_schema)"
      ],
      "metadata": {
        "id": "GGCTSBgv6WUe"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seller_schema = StructType([\n",
        "StructField(\"seller_id\", StringType(), True),\n",
        "StructField(\"seller_name\", StringType(), True),\n",
        "StructField(\"category\", StringType(), True),\n",
        "StructField(\"start_date\", StringType(), True)\n",
        "])\n",
        "raw_sellers_df = spark.createDataFrame(raw_sellers, schema=seller_schema)"
      ],
      "metadata": {
        "id": "xd28TFID7eGE"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "product_schema = StructType([\n",
        "StructField(\"product_id\", StringType(), True),\n",
        "StructField(\"product\", StringType(), True),\n",
        "StructField(\"category\", StringType(), True),\n",
        "StructField(\"seller_id\", StringType(), True),\n",
        "StructField(\"start_date\", StringType(), True)\n",
        "])\n",
        "raw_products_df = spark.createDataFrame(raw_products, schema=product_schema)"
      ],
      "metadata": {
        "id": "Ukfbag4j7m6s"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "order_schema = StructType([\n",
        "StructField(\"order_id\", StringType(), True),\n",
        "StructField(\"customer_id\", StringType(), True),\n",
        "StructField(\"product_id\", StringType(), True),\n",
        "StructField(\"order_date\", StringType(), True),\n",
        "StructField(\"status\", StringType(), True),\n",
        "StructField(\"amount\", StringType(), True)\n",
        "])\n",
        "raw_orders_df = spark.createDataFrame(raw_orders, schema=order_schema)"
      ],
      "metadata": {
        "id": "dMzLXllE7xXC"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activity_schema = StructType([\n",
        "StructField(\"customer_id\", StringType(), True),\n",
        "StructField(\"actions\", StringType(), True),\n",
        "StructField(\"metadata\", StringType(), True),\n",
        "StructField(\"duration\", IntegerType(), True)\n",
        "])\n",
        "raw_activity_df = spark.createDataFrame(raw_activity, schema=activity_schema)"
      ],
      "metadata": {
        "id": "WqxhkXoD75cv"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Normalize data types (age, price, dates)"
      ],
      "metadata": {
        "id": "S2k-uYv_AEL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import regexp_extract, col, when\n",
        "\n",
        "customers_df = raw_customers_df \\\n",
        "    .withColumn(\"age\", when(regexp_extract(\"age\", \"\\\\d+\", 0) == \"\", None)\n",
        "                .otherwise(regexp_extract(\"age\", \"\\\\d+\", 0)).cast(\"int\")) \\\n",
        "    .withColumn(\"name\", when(col(\"name\") == \"\", None).otherwise(col(\"name\")))\n",
        "\n",
        "customers_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDjZPOsV9Kta",
        "outputId": "a1b0d3fa-51ca-4801-ed9f-021c90ebf7c1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+----+---------+-------------------+\n",
            "|customer_id|   name| age|     city|          interests|\n",
            "+-----------+-------+----+---------+-------------------+\n",
            "|       C001|  Rahul|  29|Bangalore|Electronics,Fashion|\n",
            "|       C002|  Sneha|NULL|    Delhi|            Fashion|\n",
            "|       C003|   Aman|NULL|   Mumbai|[Home, Electronics]|\n",
            "|       C004|Pallavi|  27|     Pune| Electronics|Beauty|\n",
            "|       C005|   NULL|  35|  Chennai|               NULL|\n",
            "+-----------+-------+----+---------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "products_df = raw_products_df.withColumnRenamed(\"start_date\", \"price\").withColumn(\"price\", col(\"price\").cast(\"int\"))\n",
        "products_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pU-r6BAeB3gj",
        "outputId": "7b8fcb67-1aec-4506-ffe6-eb20b4327947"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+-----------+---------+-----+\n",
            "|product_id|   product|   category|seller_id|price|\n",
            "+----------+----------+-----------+---------+-----+\n",
            "|      P001|    Laptop|Electronics|     S001|55000|\n",
            "|      P002|Headphones|Electronics|     S001| 2500|\n",
            "|      P003|   T-Shirt|    Fashion|     S002| 1200|\n",
            "|      P004|      Sofa|       Home|     S003|45000|\n",
            "|      P005|Face Cream|     Beauty|     S004|  800|\n",
            "+----------+----------+-----------+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orders_df =raw_orders_df.withColumn(\"order_date\",\n",
        "                                    coalesce(\n",
        "                                        to_date(try_to_timestamp(col(\"order_date\"),lit(\"yyyy-MM-dd\"))),\n",
        "                                        to_date(try_to_timestamp(col(\"order_date\"),lit(\"dd/MM/yyyy\"))),\n",
        "                                        to_date(try_to_timestamp(col(\"order_date\"),lit(\"yyyy/MM/dd\")))\n",
        "                                    )\n",
        "                                   )\n",
        "orders_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxW3G2zQCH-d",
        "outputId": "94099ac7-fa07-4876-96c7-20907ea17aff"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+----------+----------+---------+------+\n",
            "|order_id|customer_id|product_id|order_date|   status|amount|\n",
            "+--------+-----------+----------+----------+---------+------+\n",
            "|    O001|       C001|      P001|2024-01-05|Delivered| 55000|\n",
            "|    O002|       C002|      P003|2024-01-05|Cancelled|     0|\n",
            "|    O003|       C003|      P004|2024-01-06|Delivered| 45000|\n",
            "|    O004|       C004|      P005|      NULL|Delivered|   800|\n",
            "|    O005|       C001|      P002|2024-01-10|Delivered|  2500|\n",
            "|    O006|       C005|      P003|2024-01-12|Delivered|  1200|\n",
            "+--------+-----------+----------+----------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Convert interests and actions into arrays"
      ],
      "metadata": {
        "id": "3UyZqFUoC-91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import split, regexp_replace\n",
        "\n",
        "customers_df = customers_df.withColumn(\n",
        "    \"interests\",\n",
        "    split(regexp_replace(\"interests\", \"[|]\", \",\"), \",\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "57NWg1EeC7M_"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split, regexp_replace\n",
        "\n",
        "activity_df = raw_activity_df.withColumn(\n",
        "    \"actions\",\n",
        "    split(regexp_replace(\"actions\", \"[|]\", \",\"), \",\")\n",
        ")"
      ],
      "metadata": {
        "id": "Vt-WXgiODeZO"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Handle missing and invalid records gracefully"
      ],
      "metadata": {
        "id": "vid5Rn7cEfjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, to_date, coalesce, split, lit, array_remove, try_to_timestamp\n",
        "\n",
        "# Make an empty string array: split(\"\", \",\") -> [\"\"] then remove \"\" -> []\n",
        "empty_string_array = array_remove(split(lit(\"\"), \",\"), \"\")\n",
        "\n",
        "customers_df = customers_df.withColumn(\n",
        "    \"interests\",\n",
        "    coalesce(col(\"interests\"), empty_string_array)\n",
        ")\n",
        "\n",
        "orders_df = orders_df.filter(col(\"order_date\").isNotNull())\n",
        "\n",
        "sellers_df = raw_sellers_df.withColumn(\n",
        "    \"start_date\",\n",
        "    coalesce(\n",
        "        to_date(try_to_timestamp(col(\"start_date\"), lit(\"yyyy-MM-dd\"))),\n",
        "        to_date(try_to_timestamp(col(\"start_date\"), lit(\"dd/MM/yyyy\"))),\n",
        "        to_date(try_to_timestamp(col(\"start_date\"), lit(\"yyyy/MM/dd\")))\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "fWUErUWIGJm4"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Produce clean DataFrames:\n",
        "users_df\n",
        "courses_df\n",
        "enrollments_df\n",
        "activity_df"
      ],
      "metadata": {
        "id": "aVgMM0vgAT1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customers_df.printSchema()\n",
        "customers_df.show()\n",
        "sellers_df.printSchema()\n",
        "sellers_df.show()\n",
        "products_df.printSchema()\n",
        "products_df.show()\n",
        "orders_df.printSchema()\n",
        "orders_df.show()\n",
        "activity_df.printSchema()\n",
        "activity_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuHHKe6VE8Me",
        "outputId": "233fb2c3-9910-47d7-b32a-c1b9fff149d7"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- interests: array (nullable = false)\n",
            " |    |-- element: string (containsNull = false)\n",
            "\n",
            "+-----------+-------+----+---------+--------------------+\n",
            "|customer_id|   name| age|     city|           interests|\n",
            "+-----------+-------+----+---------+--------------------+\n",
            "|       C001|  Rahul|  29|Bangalore|[Electronics, Fas...|\n",
            "|       C002|  Sneha|NULL|    Delhi|           [Fashion]|\n",
            "|       C003|   Aman|NULL|   Mumbai|[[Home,  Electron...|\n",
            "|       C004|Pallavi|  27|     Pune|[Electronics, Bea...|\n",
            "|       C005|   NULL|  35|  Chennai|                  []|\n",
            "+-----------+-------+----+---------+--------------------+\n",
            "\n",
            "root\n",
            " |-- seller_id: string (nullable = true)\n",
            " |-- seller_name: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- start_date: date (nullable = true)\n",
            "\n",
            "+---------+--------------+-----------+----------+\n",
            "|seller_id|   seller_name|   category|start_date|\n",
            "+---------+--------------+-----------+----------+\n",
            "|     S001|     TechWorld|Electronics|2019-06-01|\n",
            "|     S002|    FashionHub|    Fashion|2020-07-01|\n",
            "|     S003|HomeEssentials|       Home|2018-09-15|\n",
            "|     S004|   BeautyStore|     Beauty|      NULL|\n",
            "+---------+--------------+-----------+----------+\n",
            "\n",
            "root\n",
            " |-- product_id: string (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- seller_id: string (nullable = true)\n",
            " |-- price: integer (nullable = true)\n",
            "\n",
            "+----------+----------+-----------+---------+-----+\n",
            "|product_id|   product|   category|seller_id|price|\n",
            "+----------+----------+-----------+---------+-----+\n",
            "|      P001|    Laptop|Electronics|     S001|55000|\n",
            "|      P002|Headphones|Electronics|     S001| 2500|\n",
            "|      P003|   T-Shirt|    Fashion|     S002| 1200|\n",
            "|      P004|      Sofa|       Home|     S003|45000|\n",
            "|      P005|Face Cream|     Beauty|     S004|  800|\n",
            "+----------+----------+-----------+---------+-----+\n",
            "\n",
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- product_id: string (nullable = true)\n",
            " |-- order_date: date (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- amount: string (nullable = true)\n",
            "\n",
            "+--------+-----------+----------+----------+---------+------+\n",
            "|order_id|customer_id|product_id|order_date|   status|amount|\n",
            "+--------+-----------+----------+----------+---------+------+\n",
            "|    O001|       C001|      P001|2024-01-05|Delivered| 55000|\n",
            "|    O002|       C002|      P003|2024-01-05|Cancelled|     0|\n",
            "|    O003|       C003|      P004|2024-01-06|Delivered| 45000|\n",
            "|    O005|       C001|      P002|2024-01-10|Delivered|  2500|\n",
            "|    O006|       C005|      P003|2024-01-12|Delivered|  1200|\n",
            "+--------+-----------+----------+----------+---------+------+\n",
            "\n",
            "root\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- actions: array (nullable = true)\n",
            " |    |-- element: string (containsNull = false)\n",
            " |-- metadata: string (nullable = true)\n",
            " |-- duration: integer (nullable = true)\n",
            "\n",
            "+-----------+--------------------+-------------------+--------+\n",
            "|customer_id|             actions|           metadata|duration|\n",
            "+-----------+--------------------+-------------------+--------+\n",
            "|       C001|[search, view, ad...|{'device':'mobile'}|     180|\n",
            "|       C002|   [[search,  view]]|      device=laptop|      90|\n",
            "|       C003|[search, view, pu...|               NULL|     120|\n",
            "|       C004|                NULL|{'device':'tablet'}|      60|\n",
            "|       C005|            [search]|{'device':'mobile'}|      30|\n",
            "+-----------+--------------------+-------------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PART B — DATA INTEGRATION (JOINS)"
      ],
      "metadata": {
        "id": "w1NGzUYNGmNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Join orders with products"
      ],
      "metadata": {
        "id": "WmeEpXLDGp0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders_products_df  = orders_df.join(products_df, \"product_id\", \"inner\")\n",
        "orders_products_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBGnTxaaGvqc",
        "outputId": "2ea6173d-b557-4f6c-cd46-286f199ce8ea"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+-----------+----------+---------+------+----------+-----------+---------+-----+\n",
            "|product_id|order_id|customer_id|order_date|   status|amount|   product|   category|seller_id|price|\n",
            "+----------+--------+-----------+----------+---------+------+----------+-----------+---------+-----+\n",
            "|      P001|    O001|       C001|2024-01-05|Delivered| 55000|    Laptop|Electronics|     S001|55000|\n",
            "|      P002|    O005|       C001|2024-01-10|Delivered|  2500|Headphones|Electronics|     S001| 2500|\n",
            "|      P003|    O002|       C002|2024-01-05|Cancelled|     0|   T-Shirt|    Fashion|     S002| 1200|\n",
            "|      P003|    O006|       C005|2024-01-12|Delivered|  1200|   T-Shirt|    Fashion|     S002| 1200|\n",
            "|      P004|    O003|       C003|2024-01-06|Delivered| 45000|      Sofa|       Home|     S003|45000|\n",
            "+----------+--------+-----------+----------+---------+------+----------+-----------+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Join products with sellers"
      ],
      "metadata": {
        "id": "n1mKuPHNHQdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "products_seller_df  = products_df.join(broadcast(sellers_df), \"seller_id\", \"inner\")\n",
        "products_seller_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzIbudWuHIjm",
        "outputId": "e91a637a-8912-4d8a-c320-b376c0d0a13e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+----------+-----------+-----+--------------+-----------+----------+\n",
            "|seller_id|product_id|   product|   category|price|   seller_name|   category|start_date|\n",
            "+---------+----------+----------+-----------+-----+--------------+-----------+----------+\n",
            "|     S001|      P001|    Laptop|Electronics|55000|     TechWorld|Electronics|2019-06-01|\n",
            "|     S001|      P002|Headphones|Electronics| 2500|     TechWorld|Electronics|2019-06-01|\n",
            "|     S002|      P003|   T-Shirt|    Fashion| 1200|    FashionHub|    Fashion|2020-07-01|\n",
            "|     S003|      P004|      Sofa|       Home|45000|HomeEssentials|       Home|2018-09-15|\n",
            "|     S004|      P005|Face Cream|     Beauty|  800|   BeautyStore|     Beauty|      NULL|\n",
            "+---------+----------+----------+-----------+-----+--------------+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Join orders with customers"
      ],
      "metadata": {
        "id": "A7fRP18RH2HF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders_customers_df  = orders_df.join(customers_df, \"customer_id\", \"inner\")\n",
        "orders_customers_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GLnTiWYHwgB",
        "outputId": "11b0c0c6-bc71-40dd-c48c-cfb783573ec0"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+----------+----------+---------+------+-----+----+---------+--------------------+\n",
            "|customer_id|order_id|product_id|order_date|   status|amount| name| age|     city|           interests|\n",
            "+-----------+--------+----------+----------+---------+------+-----+----+---------+--------------------+\n",
            "|       C001|    O001|      P001|2024-01-05|Delivered| 55000|Rahul|  29|Bangalore|[Electronics, Fas...|\n",
            "|       C001|    O005|      P002|2024-01-10|Delivered|  2500|Rahul|  29|Bangalore|[Electronics, Fas...|\n",
            "|       C002|    O002|      P003|2024-01-05|Cancelled|     0|Sneha|NULL|    Delhi|           [Fashion]|\n",
            "|       C003|    O003|      P004|2024-01-06|Delivered| 45000| Aman|NULL|   Mumbai|[[Home,  Electron...|\n",
            "|       C005|    O006|      P003|2024-01-12|Delivered|  1200| NULL|  35|  Chennai|                  []|\n",
            "+-----------+--------+----------+----------+---------+------+-----+----+---------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Decide which table(s) should be broadcast"
      ],
      "metadata": {
        "id": "5dMbIJkCIERa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "# Join orders_df with broadcasted customers_df\n",
        "orders_customers_broadcast_df = orders_df.join(broadcast(customers_df), \"customer_id\", \"inner\")\n",
        "\n",
        "print(\"Physical plan for join with broadcasted customers_df:\")\n",
        "orders_customers_broadcast_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdCfoJwqIC-B",
        "outputId": "51c1e40d-5757-401a-c340-3b60c9ddaa24"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Physical plan for join with broadcasted customers_df:\n",
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [customer_id])\n",
            ":- Filter isnotnull(order_date#263)\n",
            ":  +- Filter isnotnull(order_date#263)\n",
            ":     +- Filter isnotnull(order_date#263)\n",
            ":        +- Project [order_id#180, customer_id#181, product_id#182, coalesce(to_date(try_to_timestamp(order_date#183, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#183, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#183, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS order_date#263, status#184, amount#185]\n",
            ":           +- LogicalRDD [order_id#180, customer_id#181, product_id#182, order_date#183, status#184, amount#185], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- Project [customer_id#165, name#227, age#226, city#168, coalesce(interests#317, array_remove(split(, ,, -1), )) AS interests#396]\n",
            "      +- Project [customer_id#165, name#227, age#226, city#168, coalesce(interests#285, array_remove(split(, ,, -1), )) AS interests#317]\n",
            "         +- Project [customer_id#165, name#227, age#226, city#168, coalesce(interests#283, array_remove(split(, ,, -1), )) AS interests#285]\n",
            "            +- Project [customer_id#165, name#227, age#226, city#168, split(regexp_replace(interests#169, [|], ,, 1), ,, -1) AS interests#283]\n",
            "               +- Project [customer_id#165, CASE WHEN (name#166 = ) THEN cast(null as string) ELSE name#166 END AS name#227, age#226, city#168, interests#169]\n",
            "                  +- Project [customer_id#165, name#166, cast(CASE WHEN (regexp_extract(age#167, \\d+, 0) = ) THEN cast(null as string) ELSE regexp_extract(age#167, \\d+, 0) END as int) AS age#226, city#168, interests#169]\n",
            "                     +- LogicalRDD [customer_id#165, name#166, age#167, city#168, interests#169], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "customer_id: string, order_id: string, product_id: string, order_date: date, status: string, amount: string, name: string, age: int, city: string, interests: array<string>\n",
            "Project [customer_id#181, order_id#180, product_id#182, order_date#263, status#184, amount#185, name#227, age#226, city#168, interests#396]\n",
            "+- Join Inner, (customer_id#181 = customer_id#165)\n",
            "   :- Filter isnotnull(order_date#263)\n",
            "   :  +- Filter isnotnull(order_date#263)\n",
            "   :     +- Filter isnotnull(order_date#263)\n",
            "   :        +- Project [order_id#180, customer_id#181, product_id#182, coalesce(to_date(try_to_timestamp(order_date#183, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#183, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#183, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS order_date#263, status#184, amount#185]\n",
            "   :           +- LogicalRDD [order_id#180, customer_id#181, product_id#182, order_date#183, status#184, amount#185], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- Project [customer_id#165, name#227, age#226, city#168, coalesce(interests#317, array_remove(split(, ,, -1), )) AS interests#396]\n",
            "         +- Project [customer_id#165, name#227, age#226, city#168, coalesce(interests#285, array_remove(split(, ,, -1), )) AS interests#317]\n",
            "            +- Project [customer_id#165, name#227, age#226, city#168, coalesce(interests#283, array_remove(split(, ,, -1), )) AS interests#285]\n",
            "               +- Project [customer_id#165, name#227, age#226, city#168, split(regexp_replace(interests#169, [|], ,, 1), ,, -1) AS interests#283]\n",
            "                  +- Project [customer_id#165, CASE WHEN (name#166 = ) THEN cast(null as string) ELSE name#166 END AS name#227, age#226, city#168, interests#169]\n",
            "                     +- Project [customer_id#165, name#166, cast(CASE WHEN (regexp_extract(age#167, \\d+, 0) = ) THEN cast(null as string) ELSE regexp_extract(age#167, \\d+, 0) END as int) AS age#226, city#168, interests#169]\n",
            "                        +- LogicalRDD [customer_id#165, name#166, age#167, city#168, interests#169], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [customer_id#181, order_id#180, product_id#182, order_date#263, status#184, amount#185, name#227, age#226, city#168, interests#396]\n",
            "+- Join Inner, (customer_id#181 = customer_id#165), rightHint=(strategy=broadcast)\n",
            "   :- Project [order_id#180, customer_id#181, product_id#182, coalesce(cast(gettimestamp(order_date#183, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#183, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#183, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date#263, status#184, amount#185]\n",
            "   :  +- Filter (isnotnull(coalesce(cast(gettimestamp(order_date#183, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#183, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#183, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date))) AND isnotnull(customer_id#181))\n",
            "   :     +- LogicalRDD [order_id#180, customer_id#181, product_id#182, order_date#183, status#184, amount#185], false\n",
            "   +- Project [customer_id#165, CASE WHEN (name#166 = ) THEN null ELSE name#166 END AS name#227, CASE WHEN (regexp_extract(age#167, \\d+, 0) = ) THEN null ELSE cast(regexp_extract(age#167, \\d+, 0) as int) END AS age#226, city#168, coalesce(split(regexp_replace(interests#169, [|], ,, 1), ,, -1), []) AS interests#396]\n",
            "      +- Filter isnotnull(customer_id#165)\n",
            "         +- LogicalRDD [customer_id#165, name#166, age#167, city#168, interests#169], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [customer_id#181, order_id#180, product_id#182, order_date#263, status#184, amount#185, name#227, age#226, city#168, interests#396]\n",
            "   +- BroadcastHashJoin [customer_id#181], [customer_id#165], Inner, BuildRight, false\n",
            "      :- Project [order_id#180, customer_id#181, product_id#182, coalesce(cast(gettimestamp(order_date#183, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#183, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#183, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date#263, status#184, amount#185]\n",
            "      :  +- Filter (isnotnull(coalesce(cast(gettimestamp(order_date#183, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#183, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#183, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date))) AND isnotnull(customer_id#181))\n",
            "      :     +- Scan ExistingRDD[order_id#180,customer_id#181,product_id#182,order_date#183,status#184,amount#185]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=682]\n",
            "         +- Project [customer_id#165, CASE WHEN (name#166 = ) THEN null ELSE name#166 END AS name#227, CASE WHEN (regexp_extract(age#167, \\d+, 0) = ) THEN null ELSE cast(regexp_extract(age#167, \\d+, 0) as int) END AS age#226, city#168, coalesce(split(regexp_replace(interests#169, [|], ,, 1), ,, -1), []) AS interests#396]\n",
            "            +- Filter isnotnull(customer_id#165)\n",
            "               +- Scan ExistingRDD[customer_id#165,name#166,age#167,city#168,interests#169]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Prove your decision using explain(True)"
      ],
      "metadata": {
        "id": "8iMwSgXNIVsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "# Join orders_df with broadcasted customers_df\n",
        "orders_customers_broadcast_df = orders_df.join(broadcast(customers_df), \"customer_id\", \"inner\")\n",
        "\n",
        "print(\"Physical plan for join with broadcasted customers_df:\")\n",
        "orders_customers_broadcast_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3qLDXA7Ifmj",
        "outputId": "98314d2a-b4f3-4cd3-df57-34cf5900d1f2"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Physical plan for join with broadcasted customers_df:\n",
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [customer_id])\n",
            ":- Filter isnotnull(order_date#263)\n",
            ":  +- Filter isnotnull(order_date#263)\n",
            ":     +- Filter isnotnull(order_date#263)\n",
            ":        +- Project [order_id#180, customer_id#181, product_id#182, coalesce(to_date(try_to_timestamp(order_date#183, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#183, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#183, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS order_date#263, status#184, amount#185]\n",
            ":           +- LogicalRDD [order_id#180, customer_id#181, product_id#182, order_date#183, status#184, amount#185], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- Project [customer_id#165, name#227, age#226, city#168, coalesce(interests#317, array_remove(split(, ,, -1), )) AS interests#396]\n",
            "      +- Project [customer_id#165, name#227, age#226, city#168, coalesce(interests#285, array_remove(split(, ,, -1), )) AS interests#317]\n",
            "         +- Project [customer_id#165, name#227, age#226, city#168, coalesce(interests#283, array_remove(split(, ,, -1), )) AS interests#285]\n",
            "            +- Project [customer_id#165, name#227, age#226, city#168, split(regexp_replace(interests#169, [|], ,, 1), ,, -1) AS interests#283]\n",
            "               +- Project [customer_id#165, CASE WHEN (name#166 = ) THEN cast(null as string) ELSE name#166 END AS name#227, age#226, city#168, interests#169]\n",
            "                  +- Project [customer_id#165, name#166, cast(CASE WHEN (regexp_extract(age#167, \\d+, 0) = ) THEN cast(null as string) ELSE regexp_extract(age#167, \\d+, 0) END as int) AS age#226, city#168, interests#169]\n",
            "                     +- LogicalRDD [customer_id#165, name#166, age#167, city#168, interests#169], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "customer_id: string, order_id: string, product_id: string, order_date: date, status: string, amount: string, name: string, age: int, city: string, interests: array<string>\n",
            "Project [customer_id#181, order_id#180, product_id#182, order_date#263, status#184, amount#185, name#227, age#226, city#168, interests#396]\n",
            "+- Join Inner, (customer_id#181 = customer_id#165)\n",
            "   :- Filter isnotnull(order_date#263)\n",
            "   :  +- Filter isnotnull(order_date#263)\n",
            "   :     +- Filter isnotnull(order_date#263)\n",
            "   :        +- Project [order_id#180, customer_id#181, product_id#182, coalesce(to_date(try_to_timestamp(order_date#183, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#183, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#183, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS order_date#263, status#184, amount#185]\n",
            "   :           +- LogicalRDD [order_id#180, customer_id#181, product_id#182, order_date#183, status#184, amount#185], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- Project [customer_id#165, name#227, age#226, city#168, coalesce(interests#317, array_remove(split(, ,, -1), )) AS interests#396]\n",
            "         +- Project [customer_id#165, name#227, age#226, city#168, coalesce(interests#285, array_remove(split(, ,, -1), )) AS interests#317]\n",
            "            +- Project [customer_id#165, name#227, age#226, city#168, coalesce(interests#283, array_remove(split(, ,, -1), )) AS interests#285]\n",
            "               +- Project [customer_id#165, name#227, age#226, city#168, split(regexp_replace(interests#169, [|], ,, 1), ,, -1) AS interests#283]\n",
            "                  +- Project [customer_id#165, CASE WHEN (name#166 = ) THEN cast(null as string) ELSE name#166 END AS name#227, age#226, city#168, interests#169]\n",
            "                     +- Project [customer_id#165, name#166, cast(CASE WHEN (regexp_extract(age#167, \\d+, 0) = ) THEN cast(null as string) ELSE regexp_extract(age#167, \\d+, 0) END as int) AS age#226, city#168, interests#169]\n",
            "                        +- LogicalRDD [customer_id#165, name#166, age#167, city#168, interests#169], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [customer_id#181, order_id#180, product_id#182, order_date#263, status#184, amount#185, name#227, age#226, city#168, interests#396]\n",
            "+- Join Inner, (customer_id#181 = customer_id#165), rightHint=(strategy=broadcast)\n",
            "   :- Project [order_id#180, customer_id#181, product_id#182, coalesce(cast(gettimestamp(order_date#183, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#183, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#183, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date#263, status#184, amount#185]\n",
            "   :  +- Filter (isnotnull(coalesce(cast(gettimestamp(order_date#183, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#183, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#183, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date))) AND isnotnull(customer_id#181))\n",
            "   :     +- LogicalRDD [order_id#180, customer_id#181, product_id#182, order_date#183, status#184, amount#185], false\n",
            "   +- Project [customer_id#165, CASE WHEN (name#166 = ) THEN null ELSE name#166 END AS name#227, CASE WHEN (regexp_extract(age#167, \\d+, 0) = ) THEN null ELSE cast(regexp_extract(age#167, \\d+, 0) as int) END AS age#226, city#168, coalesce(split(regexp_replace(interests#169, [|], ,, 1), ,, -1), []) AS interests#396]\n",
            "      +- Filter isnotnull(customer_id#165)\n",
            "         +- LogicalRDD [customer_id#165, name#166, age#167, city#168, interests#169], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [customer_id#181, order_id#180, product_id#182, order_date#263, status#184, amount#185, name#227, age#226, city#168, interests#396]\n",
            "   +- BroadcastHashJoin [customer_id#181], [customer_id#165], Inner, BuildRight, false\n",
            "      :- Project [order_id#180, customer_id#181, product_id#182, coalesce(cast(gettimestamp(order_date#183, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#183, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#183, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date#263, status#184, amount#185]\n",
            "      :  +- Filter (isnotnull(coalesce(cast(gettimestamp(order_date#183, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#183, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#183, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date))) AND isnotnull(customer_id#181))\n",
            "      :     +- Scan ExistingRDD[order_id#180,customer_id#181,product_id#182,order_date#183,status#184,amount#185]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=717]\n",
            "         +- Project [customer_id#165, CASE WHEN (name#166 = ) THEN null ELSE name#166 END AS name#227, CASE WHEN (regexp_extract(age#167, \\d+, 0) = ) THEN null ELSE cast(regexp_extract(age#167, \\d+, 0) as int) END AS age#226, city#168, coalesce(split(regexp_replace(interests#169, [|], ,, 1), ,, -1), []) AS interests#396]\n",
            "            +- Filter isnotnull(customer_id#165)\n",
            "               +- Scan ExistingRDD[customer_id#165,name#166,age#167,city#168,interests#169]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Eliminate orphan records"
      ],
      "metadata": {
        "id": "t6uSdevsItsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Eliminate customers without any orders\n",
        "customers_with_orders_df = customers_df.join(orders_df, \"customer_id\", \"left_semi\")\n",
        "orphan_customers_df = customers_df.join(orders_df, \"customer_id\", \"left_anti\")\n",
        "\n",
        "print(\"Orphan Customers:\")\n",
        "orphan_customers_df.show()\n",
        "\n",
        "# Update customers_df to only include customers with orders\n",
        "customers_df = customers_with_orders_df\n",
        "\n",
        "# 2. Eliminate products without any orders\n",
        "products_with_orders_df = products_df.join(orders_df, \"product_id\", \"left_semi\")\n",
        "orphan_products_df = products_df.join(orders_df, \"product_id\", \"left_anti\")\n",
        "\n",
        "print(\"Orphan Products:\")\n",
        "orphan_products_df.show()\n",
        "\n",
        "# Update products_df to only include products with orders\n",
        "products_df = products_with_orders_df\n",
        "\n",
        "# 3. Eliminate sellers without any products\n",
        "sellers_with_products_df = sellers_df.join(products_df, \"seller_id\", \"left_semi\")\n",
        "orphan_sellers_df = sellers_df.join(products_df, \"seller_id\", \"left_anti\")\n",
        "\n",
        "print(\"Orphan Sellers:\")\n",
        "orphan_sellers_df.show()\n",
        "\n",
        "# Update sellers_df to only include sellers with products\n",
        "sellers_df = sellers_with_products_df\n",
        "\n",
        "print(\"DataFrames after eliminating orphan records:\")\n",
        "customers_df.show()\n",
        "products_df.show()\n",
        "sellers_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9Yi_muQIyKT",
        "outputId": "d45e1c8a-7b87-411e-a558-93424259585f"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orphan Customers:\n",
            "+-----------+-------+---+----+--------------------+\n",
            "|customer_id|   name|age|city|           interests|\n",
            "+-----------+-------+---+----+--------------------+\n",
            "|       C004|Pallavi| 27|Pune|[Electronics, Bea...|\n",
            "+-----------+-------+---+----+--------------------+\n",
            "\n",
            "Orphan Products:\n",
            "+----------+----------+--------+---------+-----+\n",
            "|product_id|   product|category|seller_id|price|\n",
            "+----------+----------+--------+---------+-----+\n",
            "|      P005|Face Cream|  Beauty|     S004|  800|\n",
            "+----------+----------+--------+---------+-----+\n",
            "\n",
            "Orphan Sellers:\n",
            "+---------+-----------+--------+----------+\n",
            "|seller_id|seller_name|category|start_date|\n",
            "+---------+-----------+--------+----------+\n",
            "|     S004|BeautyStore|  Beauty|      NULL|\n",
            "+---------+-----------+--------+----------+\n",
            "\n",
            "DataFrames after eliminating orphan records:\n",
            "+-----------+-----+----+---------+--------------------+\n",
            "|customer_id| name| age|     city|           interests|\n",
            "+-----------+-----+----+---------+--------------------+\n",
            "|       C001|Rahul|  29|Bangalore|[Electronics, Fas...|\n",
            "|       C002|Sneha|NULL|    Delhi|           [Fashion]|\n",
            "|       C003| Aman|NULL|   Mumbai|[[Home,  Electron...|\n",
            "|       C005| NULL|  35|  Chennai|                  []|\n",
            "+-----------+-----+----+---------+--------------------+\n",
            "\n",
            "+----------+----------+-----------+---------+-----+\n",
            "|product_id|   product|   category|seller_id|price|\n",
            "+----------+----------+-----------+---------+-----+\n",
            "|      P001|    Laptop|Electronics|     S001|55000|\n",
            "|      P002|Headphones|Electronics|     S001| 2500|\n",
            "|      P003|   T-Shirt|    Fashion|     S002| 1200|\n",
            "|      P004|      Sofa|       Home|     S003|45000|\n",
            "+----------+----------+-----------+---------+-----+\n",
            "\n",
            "+---------+--------------+-----------+----------+\n",
            "|seller_id|   seller_name|   category|start_date|\n",
            "+---------+--------------+-----------+----------+\n",
            "|     S001|     TechWorld|Electronics|2019-06-01|\n",
            "|     S002|    FashionHub|    Fashion|2020-07-01|\n",
            "|     S003|HomeEssentials|       Home|2018-09-15|\n",
            "+---------+--------------+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PART C — ANALYTICS & AGGREGATIONS"
      ],
      "metadata": {
        "id": "sTecIx0dIhhu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Total revenue per category"
      ],
      "metadata": {
        "id": "uB7Jkc9EJDwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "revenue_category_df = orders_products_df.groupBy(\"category\").agg(sum(\"amount\").alias(\"total_revenue\"))\n",
        "revenue_category_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qEj31nOJGPS",
        "outputId": "5ead9296-bb38-4175-eb37-9996b8ccf591"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+\n",
            "|   category|total_revenue|\n",
            "+-----------+-------------+\n",
            "|       Home|      45000.0|\n",
            "|    Fashion|       1200.0|\n",
            "|Electronics|      57500.0|\n",
            "+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Total revenue per seller"
      ],
      "metadata": {
        "id": "PKifVhmCJRQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "revenue_seller_df = orders_products_df.groupBy(\"seller_id\").agg(sum(\"amount\").alias(\"total_revenue\"))\n",
        "revenue_seller_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE1WLHh4JNl7",
        "outputId": "689b97ab-6789-4aca-f9c8-a0cc71326ca5"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------+\n",
            "|seller_id|total_revenue|\n",
            "+---------+-------------+\n",
            "|     S001|      57500.0|\n",
            "|     S002|       1200.0|\n",
            "|     S003|      45000.0|\n",
            "+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Total orders per customer"
      ],
      "metadata": {
        "id": "CE1gZl42JX-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders_customers_df = orders_df.groupBy(\"customer_id\").agg(count(\"order_id\").alias(\"total_orders\"))\n",
        "orders_customers_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjg97ZG8JWx6",
        "outputId": "a469762c-79fe-402f-ee8c-df21a4f77922"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+\n",
            "|customer_id|total_orders|\n",
            "+-----------+------------+\n",
            "|       C003|           1|\n",
            "|       C001|           2|\n",
            "|       C002|           1|\n",
            "|       C005|           1|\n",
            "+-----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Average order value per customer"
      ],
      "metadata": {
        "id": "xf4l_5ocJjPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "average_order_value_df = orders_df.withColumn(\"amount\", col(\"amount\").cast(\"double\")) \\\n",
        "    .groupBy(\"customer_id\") \\\n",
        "    .agg(avg(\"amount\").alias(\"average_order_value\"))\n",
        "average_order_value_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbcBsZhXJiMx",
        "outputId": "94199df9-800b-4d53-c316-1c4656b22eb8"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------------+\n",
            "|customer_id|average_order_value|\n",
            "+-----------+-------------------+\n",
            "|       C003|            45000.0|\n",
            "|       C001|            28750.0|\n",
            "|       C002|                0.0|\n",
            "|       C005|             1200.0|\n",
            "+-----------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Identify sellers with zero delivered orders"
      ],
      "metadata": {
        "id": "ji2UNvSaJxNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zero_delivery_sellers_df = orders_products_df.filter(col(\"status\") == \"Delivered\")\n",
        "zero_delivery_sellers_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkcTOUQmJyCV",
        "outputId": "cce4c351-a964-4b3d-fa25-d992db4a4d9e"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+-----------+----------+---------+------+----------+-----------+---------+-----+\n",
            "|product_id|order_id|customer_id|order_date|   status|amount|   product|   category|seller_id|price|\n",
            "+----------+--------+-----------+----------+---------+------+----------+-----------+---------+-----+\n",
            "|      P001|    O001|       C001|2024-01-05|Delivered| 55000|    Laptop|Electronics|     S001|55000|\n",
            "|      P002|    O005|       C001|2024-01-10|Delivered|  2500|Headphones|Electronics|     S001| 2500|\n",
            "|      P003|    O006|       C005|2024-01-12|Delivered|  1200|   T-Shirt|    Fashion|     S002| 1200|\n",
            "|      P004|    O003|       C003|2024-01-06|Delivered| 45000|      Sofa|       Home|     S003|45000|\n",
            "+----------+--------+-----------+----------+---------+------+----------+-----------+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PART D — WINDOW FUNCTIONS"
      ],
      "metadata": {
        "id": "UKizjxupJ-9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Rank customers by total spend (overall)"
      ],
      "metadata": {
        "id": "XuMUnawyLzIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import sum, col, rank\n",
        "\n",
        "# Calculate total spend per customer\n",
        "total_spend_per_customer_df = orders_products_df.groupBy(\"customer_id\") \\\n",
        "    .agg(sum(col(\"amount\").cast(\"double\")).alias(\"total_spend\"))\n",
        "\n",
        "# Define a window specification to rank customers by total spend\n",
        "window_spec = Window.orderBy(col(\"total_spend\").desc())\n",
        "\n",
        "# Apply the rank function\n",
        "customer_spend_rank_df = total_spend_per_customer_df.withColumn(\"spend_rank\", rank().over(window_spec))\n",
        "\n",
        "customer_spend_rank_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omOOwqiwJ6zm",
        "outputId": "76c9a4e2-f71e-4111-e607-4be7899f1cfb"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+----------+\n",
            "|customer_id|total_spend|spend_rank|\n",
            "+-----------+-----------+----------+\n",
            "|       C001|    57500.0|         1|\n",
            "|       C003|    45000.0|         2|\n",
            "|       C005|     1200.0|         3|\n",
            "|       C002|        0.0|         4|\n",
            "+-----------+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Rank sellers by revenue within each category"
      ],
      "metadata": {
        "id": "-Vt2PUr1MEUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import sum, col, rank\n",
        "\n",
        "revenue_per_seller_category_df = orders_products_df.groupBy(\"category\", \"seller_id\") \\\n",
        "    .agg(sum(col(\"amount\").cast(\"double\")).alias(\"total_revenue\"))\n",
        "\n",
        "window_spec_category = Window.partitionBy(\"category\").orderBy(col(\"total_revenue\").desc())\n",
        "\n",
        "seller_category_rank_df = revenue_per_seller_category_df.withColumn(\"category_rank\", rank().over(window_spec_category))\n",
        "\n",
        "seller_category_rank_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gCcTY5zMG6Q",
        "outputId": "ce822966-2e5c-42a2-e9f8-725e79fd01b7"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+-------------+-------------+\n",
            "|   category|seller_id|total_revenue|category_rank|\n",
            "+-----------+---------+-------------+-------------+\n",
            "|Electronics|     S001|      57500.0|            1|\n",
            "|    Fashion|     S002|       1200.0|            1|\n",
            "|       Home|     S003|      45000.0|            1|\n",
            "+-----------+---------+-------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Calculate running revenue per day"
      ],
      "metadata": {
        "id": "S2DVquylMXy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import sum, col, asc\n",
        "\n",
        "daily_revenue_df = orders_products_df.withColumn(\"amount\", col(\"amount\").cast(\"double\")) \\\n",
        "    .groupBy(\"order_date\") \\\n",
        "    .agg(sum(\"amount\").alias(\"daily_revenue\"))\n",
        "\n",
        "\n",
        "window_spec_daily = Window.orderBy(asc(\"order_date\"))\n",
        "\n",
        "running_revenue_df = daily_revenue_df.withColumn(\"running_revenue\", sum(\"daily_revenue\").over(window_spec_daily))\n",
        "\n",
        "running_revenue_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaewIRZuMeFh",
        "outputId": "0d367e5e-1231-4f61-9b83-ba3871c4cfc4"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+---------------+\n",
            "|order_date|daily_revenue|running_revenue|\n",
            "+----------+-------------+---------------+\n",
            "|2024-01-05|      55000.0|        55000.0|\n",
            "|2024-01-06|      45000.0|       100000.0|\n",
            "|2024-01-10|       2500.0|       102500.0|\n",
            "|2024-01-12|       1200.0|       103700.0|\n",
            "+----------+-------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Identify top 2 products per category by revenue"
      ],
      "metadata": {
        "id": "tQDLH2StMv2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import sum, col, rank\n",
        "\n",
        "product_revenue_per_category_df = orders_products_df.groupBy(\"category\", \"product_id\", \"product\") \\\n",
        "    .agg(sum(col(\"amount\").cast(\"double\")).alias(\"product_revenue\"))\n",
        "\n",
        "window_spec_product_rank = Window.partitionBy(\"category\").orderBy(col(\"product_revenue\").desc())\n",
        "\n",
        "top_2_products_per_category_df = product_revenue_per_category_df.withColumn(\"rank\", rank().over(window_spec_product_rank)) \\\n",
        "    .filter(col(\"rank\") <= 2)\n",
        "\n",
        "top_2_products_per_category_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYiCQPLhMvHb",
        "outputId": "3a0e9a95-f913-4d63-cdd9-67566fa33dc1"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+----------+---------------+----+\n",
            "|   category|product_id|   product|product_revenue|rank|\n",
            "+-----------+----------+----------+---------------+----+\n",
            "|Electronics|      P001|    Laptop|        55000.0|   1|\n",
            "|Electronics|      P002|Headphones|         2500.0|   2|\n",
            "|    Fashion|      P003|   T-Shirt|         1200.0|   1|\n",
            "|       Home|      P004|      Sofa|        45000.0|   1|\n",
            "+-----------+----------+----------+---------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PART E — UDF (ONLY IF REQUIRED)"
      ],
      "metadata": {
        "id": "tYAQRrCyNGBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Classify customers into spending tiers:\n",
        "High\n",
        "Medium\n",
        "Low\n",
        "\n",
        "Rules:\n",
        "Prefer built-in functions\n",
        "Use UDF only if unavoidable\n",
        "Justify your choice"
      ],
      "metadata": {
        "id": "4yd_5kGHNBiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "customer_spending_tiers_df = total_spend_per_customer_df.withColumn(\n",
        "    \"spending_tier\",\n",
        "    when(col(\"total_spend\") > 10000, \"High\")\n",
        "    .when((col(\"total_spend\") > 1000) & (col(\"total_spend\") <= 10000), \"Medium\")\n",
        "    .otherwise(\"Low\")\n",
        ")\n",
        "\n",
        "print(\"Customers classified into spending tiers:\")\n",
        "customer_spending_tiers_df.show()\n",
        "\n",
        "# Justification for not using a UDF:\n",
        "# PySpark's `when().otherwise()` provides native, optimized functionality for conditional logic.\n",
        "# It is executed within the Spark engine, benefiting from Catalyst Optimizer and Tungsten execution engine,\n",
        "# leading to significantly better performance compared to Python UDFs. UDFs involve serialization/deserialization\n",
        "# overhead and context switching between JVM and Python, which can be very slow for large datasets.\n",
        "# Since `when().otherwise()` perfectly handles the tier classification logic, a UDF is unnecessary and less efficient."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBOiC7BsNA5j",
        "outputId": "28b4adfd-4774-4c1f-bfad-0370f2144962"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customers classified into spending tiers:\n",
            "+-----------+-----------+-------------+\n",
            "|customer_id|total_spend|spending_tier|\n",
            "+-----------+-----------+-------------+\n",
            "|       C003|    45000.0|         High|\n",
            "|       C005|     1200.0|       Medium|\n",
            "|       C001|    57500.0|         High|\n",
            "|       C002|        0.0|          Low|\n",
            "+-----------+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PART F — SORTING & ORDERING"
      ],
      "metadata": {
        "id": "ePCxtznzNbGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Sort categories by total revenue (descending)"
      ],
      "metadata": {
        "id": "Hgyow4eSNgQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import desc\n",
        "\n",
        "sorted_categories_by_revenue_df = revenue_category_df.orderBy(desc(\"total_revenue\"))\n",
        "sorted_categories_by_revenue_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zec_ReWxNimi",
        "outputId": "861cc2be-5f50-42a3-f49b-780c94b022ab"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+\n",
            "|   category|total_revenue|\n",
            "+-----------+-------------+\n",
            "|Electronics|      57500.0|\n",
            "|       Home|      45000.0|\n",
            "|    Fashion|       1200.0|\n",
            "+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Sort sellers by revenue within category"
      ],
      "metadata": {
        "id": "Af60o9XrN8RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "sorted_sellers_by_category_revenue_df = seller_category_rank_df.orderBy(col(\"category\").asc(), col(\"total_revenue\").desc())\n",
        "\n",
        "print(\"Sellers sorted by revenue within each category:\")\n",
        "sorted_sellers_by_category_revenue_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLWP2Je2N1vz",
        "outputId": "5da9b6b6-9294-4e39-93a1-939c84c2b300"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sellers sorted by revenue within each category:\n",
            "+-----------+---------+-------------+-------------+\n",
            "|   category|seller_id|total_revenue|category_rank|\n",
            "+-----------+---------+-------------+-------------+\n",
            "|Electronics|     S001|      57500.0|            1|\n",
            "|    Fashion|     S002|       1200.0|            1|\n",
            "|       Home|     S003|      45000.0|            1|\n",
            "+-----------+---------+-------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Explain why sorting caused a shuffle"
      ],
      "metadata": {
        "id": "0n7kepGPOKwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sorting a DataFrame in Spark often triggers a 'shuffle' operation.\n",
        "# A shuffle is the process of redistributing data across partitions (and potentially across machines in a cluster).\n",
        "# This is necessary because to perform a global sort (or even a sort within groups if data is not pre-partitioned\n",
        "# or pre-sorted), all data relevant to a specific sort key range might need to be collected on the same partition.\n",
        "# For example, when sorting categories by total revenue, Spark needs to know the total revenue for all categories\n",
        "# to correctly order them. If different parts of a category's data reside on different partitions,\n",
        "# Spark must move this data to ensure a consistent global order. This involves serializing data,\n",
        "# sending it over the network, and deserializing it on the receiving end, which is a resource-intensive operation."
      ],
      "metadata": {
        "id": "Ovl0a2ntOToV"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PART G — SET OPERATIONS"
      ],
      "metadata": {
        "id": "ZAnhus8GOe4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create two DataFrames:\n",
        "\n",
        "Customers who placed orders\n",
        "Customers who were active (search/view)"
      ],
      "metadata": {
        "id": "0uSM9QiiOh-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ordered_customers_df = orders_df.select(\"customer_id\").distinct()\n",
        "active_customers_df = activity_df.filter(col(\"actions\").isNotNull()).select(\"customer_id\").distinct()"
      ],
      "metadata": {
        "id": "OKX59YX-Oobm"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Find customers who were active but never ordered"
      ],
      "metadata": {
        "id": "DiR9c4jmOoBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "active_customers_df.subtract(ordered_customers_df).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IHq4jNNO-de",
        "outputId": "e0a4133f-2fc0-4d2b-98e2-df0aa6feab80"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|customer_id|\n",
            "+-----------+\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Find customers who ordered and were active"
      ],
      "metadata": {
        "id": "-J87ePFuPF-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "active_customers_df.intersect(ordered_customers_df).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdAzxZKVPDLW",
        "outputId": "ae2babd6-92b3-49bd-c1a2-148bccd49f70"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|customer_id|\n",
            "+-----------+\n",
            "|       C003|\n",
            "|       C005|\n",
            "|       C001|\n",
            "|       C002|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Explain why set operations differ from joins"
      ],
      "metadata": {
        "id": "gwboIqrePX_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Differences between Set Operations and Joins:\n",
        "#\n",
        "# Set Operations (UNION, INTERSECT, EXCEPT/SUBTRACT):\n",
        "# - Operate on the *rows* of DataFrames.\n",
        "# - Require the DataFrames to have a compatible schema (same number of columns, same column names, and compatible data types).\n",
        "# - Combine or compare rows based on their *entire content*.\n",
        "# - The result has the same schema as the input DataFrames.\n",
        "#\n",
        "# Join Operations (INNER, LEFT, RIGHT, FULL, ANTI, SEMI):\n",
        "# - Combine *columns* from two DataFrames.\n",
        "# - Combine data based on a *common key* or a specified condition.\n",
        "# - Typically result in a wider DataFrame (more columns) by merging information from both DataFrames.\n",
        "# - The schema of the result is a combination of the schemas of the input DataFrames (excluding duplicate join keys if specified).\n",
        "\n",
        "print(\"\\n--- Set Operations (Operating on rows) ---\")\n",
        "print(\"Customers active but never ordered (using subtract):\")\n",
        "active_customers_df.subtract(ordered_customers_df).show()\n",
        "\n",
        "print(\"Customers who ordered AND were active (using intersect):\")\n",
        "active_customers_df.intersect(ordered_customers_df).show()\n",
        "\n",
        "print(\"\\n--- Join Operations (Operating on columns based on keys) ---\")\n",
        "print(\"Inner Join: Combining customer and order details for matching customer_ids:\")\n",
        "orders_df.join(customers_df, \"customer_id\", \"inner\").show()\n",
        "\n",
        "print(\"Left Anti Join: Customers from 'customers_df' who are NOT in 'orders_df' (different from subtract, key-based):\")\n",
        "customers_df.join(orders_df, \"customer_id\", \"left_anti\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKf8i0KdPWjZ",
        "outputId": "3495ef97-4088-4377-fcbe-2c3fe7f0a59c"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Set Operations (Operating on rows) ---\n",
            "Customers active but never ordered (using subtract):\n",
            "+-----------+\n",
            "|customer_id|\n",
            "+-----------+\n",
            "+-----------+\n",
            "\n",
            "Customers who ordered AND were active (using intersect):\n",
            "+-----------+\n",
            "|customer_id|\n",
            "+-----------+\n",
            "|       C003|\n",
            "|       C005|\n",
            "|       C001|\n",
            "|       C002|\n",
            "+-----------+\n",
            "\n",
            "\n",
            "--- Join Operations (Operating on columns based on keys) ---\n",
            "Inner Join: Combining customer and order details for matching customer_ids:\n",
            "+-----------+--------+----------+----------+---------+------+-----+----+---------+--------------------+\n",
            "|customer_id|order_id|product_id|order_date|   status|amount| name| age|     city|           interests|\n",
            "+-----------+--------+----------+----------+---------+------+-----+----+---------+--------------------+\n",
            "|       C003|    O003|      P004|2024-01-06|Delivered| 45000| Aman|NULL|   Mumbai|[[Home,  Electron...|\n",
            "|       C001|    O001|      P001|2024-01-05|Delivered| 55000|Rahul|  29|Bangalore|[Electronics, Fas...|\n",
            "|       C002|    O002|      P003|2024-01-05|Cancelled|     0|Sneha|NULL|    Delhi|           [Fashion]|\n",
            "|       C005|    O006|      P003|2024-01-12|Delivered|  1200| NULL|  35|  Chennai|                  []|\n",
            "|       C001|    O005|      P002|2024-01-10|Delivered|  2500|Rahul|  29|Bangalore|[Electronics, Fas...|\n",
            "+-----------+--------+----------+----------+---------+------+-----+----+---------+--------------------+\n",
            "\n",
            "Left Anti Join: Customers from 'customers_df' who are NOT in 'orders_df' (different from subtract, key-based):\n",
            "+-----------+----+---+----+---------+\n",
            "|customer_id|name|age|city|interests|\n",
            "+-----------+----+---+----+---------+\n",
            "+-----------+----+---+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART H — DAG & PERFORMANCE ANALYSIS"
      ],
      "metadata": {
        "id": "pGi3Ox8ZPdH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Run explain(True) for:\n",
        "Product → Seller join\n",
        "Window ranking\n",
        "Sorting"
      ],
      "metadata": {
        "id": "CHNDhZA4PiZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "products_seller_df.explain(True)\n",
        "customer_spend_rank_df.explain(True)\n",
        "sorted_categories_by_revenue_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY-TP-bpPoi1",
        "outputId": "0e9a7618-096a-4bf7-c725-749296e354b2"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [seller_id])\n",
            ":- Project [product_id#175, product#176, category#177, seller_id#178, cast(price#245 as int) AS price#246]\n",
            ":  +- Project [product_id#175, product#176, category#177, seller_id#178, start_date#179 AS price#245]\n",
            ":     +- LogicalRDD [product_id#175, product#176, category#177, seller_id#178, start_date#179], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- Project [seller_id#171, seller_name#172, category#173, coalesce(to_date(try_to_timestamp(start_date#174, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(start_date#174, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(start_date#174, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS start_date#397]\n",
            "      +- LogicalRDD [seller_id#171, seller_name#172, category#173, start_date#174], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "seller_id: string, product_id: string, product: string, category: string, price: int, seller_name: string, category: string, start_date: date\n",
            "Project [seller_id#178, product_id#175, product#176, category#177, price#246, seller_name#172, category#173, start_date#397]\n",
            "+- Join Inner, (seller_id#178 = seller_id#171)\n",
            "   :- Project [product_id#175, product#176, category#177, seller_id#178, cast(price#245 as int) AS price#246]\n",
            "   :  +- Project [product_id#175, product#176, category#177, seller_id#178, start_date#179 AS price#245]\n",
            "   :     +- LogicalRDD [product_id#175, product#176, category#177, seller_id#178, start_date#179], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- Project [seller_id#171, seller_name#172, category#173, coalesce(to_date(try_to_timestamp(start_date#174, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(start_date#174, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(start_date#174, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS start_date#397]\n",
            "         +- LogicalRDD [seller_id#171, seller_name#172, category#173, start_date#174], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [seller_id#178, product_id#175, product#176, category#177, price#246, seller_name#172, category#173, start_date#397]\n",
            "+- Join Inner, (seller_id#178 = seller_id#171), rightHint=(strategy=broadcast)\n",
            "   :- Project [product_id#175, product#176, category#177, seller_id#178, cast(start_date#179 as int) AS price#246]\n",
            "   :  +- Filter isnotnull(seller_id#178)\n",
            "   :     +- LogicalRDD [product_id#175, product#176, category#177, seller_id#178, start_date#179], false\n",
            "   +- Project [seller_id#171, seller_name#172, category#173, coalesce(cast(gettimestamp(start_date#174, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(start_date#174, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(start_date#174, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS start_date#397]\n",
            "      +- Filter isnotnull(seller_id#171)\n",
            "         +- LogicalRDD [seller_id#171, seller_name#172, category#173, start_date#174], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=true\n",
            "+- == Final Plan ==\n",
            "   ResultQueryStage 1\n",
            "   +- *(2) Project [seller_id#178, product_id#175, product#176, category#177, price#246, seller_name#172, category#173, start_date#397]\n",
            "      +- *(2) BroadcastHashJoin [seller_id#178], [seller_id#171], Inner, BuildRight, false\n",
            "         :- *(2) Project [product_id#175, product#176, category#177, seller_id#178, cast(start_date#179 as int) AS price#246]\n",
            "         :  +- *(2) Filter isnotnull(seller_id#178)\n",
            "         :     +- *(2) Scan ExistingRDD[product_id#175,product#176,category#177,seller_id#178,start_date#179]\n",
            "         +- BroadcastQueryStage 0\n",
            "            +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=7566]\n",
            "               +- *(1) Project [seller_id#171, seller_name#172, category#173, coalesce(cast(gettimestamp(start_date#174, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(start_date#174, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(start_date#174, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS start_date#397]\n",
            "                  +- *(1) Filter isnotnull(seller_id#171)\n",
            "                     +- *(1) Scan ExistingRDD[seller_id#171,seller_name#172,category#173,start_date#174]\n",
            "+- == Initial Plan ==\n",
            "   Project [seller_id#178, product_id#175, product#176, category#177, price#246, seller_name#172, category#173, start_date#397]\n",
            "   +- BroadcastHashJoin [seller_id#178], [seller_id#171], Inner, BuildRight, false\n",
            "      :- Project [product_id#175, product#176, category#177, seller_id#178, cast(start_date#179 as int) AS price#246]\n",
            "      :  +- Filter isnotnull(seller_id#178)\n",
            "      :     +- Scan ExistingRDD[product_id#175,product#176,category#177,seller_id#178,start_date#179]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=7555]\n",
            "         +- Project [seller_id#171, seller_name#172, category#173, coalesce(cast(gettimestamp(start_date#174, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(start_date#174, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(start_date#174, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS start_date#397]\n",
            "            +- Filter isnotnull(seller_id#171)\n",
            "               +- Scan ExistingRDD[seller_id#171,seller_name#172,category#173,start_date#174]\n",
            "\n",
            "== Parsed Logical Plan ==\n",
            "'Project [unresolvedstarwithcolumns(spend_rank, 'rank() windowspecdefinition('total_spend DESC NULLS LAST, unspecifiedframe$()), None)]\n",
            "+- Aggregate [customer_id#181], [customer_id#181, sum(cast(amount#185 as double)) AS total_spend#694]\n",
            "   +- Project [product_id#182, order_id#180, customer_id#181, order_date#263, status#184, amount#185, product#176, category#177, seller_id#178, price#246]\n",
            "      +- Join Inner, (product_id#182 = product_id#175)\n",
            "         :- Filter isnotnull(order_date#263)\n",
            "         :  +- Filter isnotnull(order_date#263)\n",
            "         :     +- Filter isnotnull(order_date#263)\n",
            "         :        +- Project [order_id#180, customer_id#181, product_id#182, coalesce(to_date(try_to_timestamp(order_date#183, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#183, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#183, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS order_date#263, status#184, amount#185]\n",
            "         :           +- LogicalRDD [order_id#180, customer_id#181, product_id#182, order_date#183, status#184, amount#185], false\n",
            "         +- Project [product_id#175, product#176, category#177, seller_id#178, cast(price#245 as int) AS price#246]\n",
            "            +- Project [product_id#175, product#176, category#177, seller_id#178, start_date#179 AS price#245]\n",
            "               +- LogicalRDD [product_id#175, product#176, category#177, seller_id#178, start_date#179], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "customer_id: string, total_spend: double, spend_rank: int\n",
            "Project [customer_id#181, total_spend#694, spend_rank#706]\n",
            "+- Project [customer_id#181, total_spend#694, spend_rank#706, spend_rank#706]\n",
            "   +- Window [rank(total_spend#694) windowspecdefinition(total_spend#694 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS spend_rank#706], [total_spend#694 DESC NULLS LAST]\n",
            "      +- Project [customer_id#181, total_spend#694]\n",
            "         +- Aggregate [customer_id#181], [customer_id#181, sum(cast(amount#185 as double)) AS total_spend#694]\n",
            "            +- Project [product_id#182, order_id#180, customer_id#181, order_date#263, status#184, amount#185, product#176, category#177, seller_id#178, price#246]\n",
            "               +- Join Inner, (product_id#182 = product_id#175)\n",
            "                  :- Filter isnotnull(order_date#263)\n",
            "                  :  +- Filter isnotnull(order_date#263)\n",
            "                  :     +- Filter isnotnull(order_date#263)\n",
            "                  :        +- Project [order_id#180, customer_id#181, product_id#182, coalesce(to_date(try_to_timestamp(order_date#183, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#183, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#183, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS order_date#263, status#184, amount#185]\n",
            "                  :           +- LogicalRDD [order_id#180, customer_id#181, product_id#182, order_date#183, status#184, amount#185], false\n",
            "                  +- Project [product_id#175, product#176, category#177, seller_id#178, cast(price#245 as int) AS price#246]\n",
            "                     +- Project [product_id#175, product#176, category#177, seller_id#178, start_date#179 AS price#245]\n",
            "                        +- LogicalRDD [product_id#175, product#176, category#177, seller_id#178, start_date#179], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Window [rank(total_spend#694) windowspecdefinition(total_spend#694 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS spend_rank#706], [total_spend#694 DESC NULLS LAST]\n",
            "+- Aggregate [customer_id#181], [customer_id#181, sum(cast(amount#185 as double)) AS total_spend#694]\n",
            "   +- Project [customer_id#181, amount#185]\n",
            "      +- Join Inner, (product_id#182 = product_id#175)\n",
            "         :- Project [customer_id#181, product_id#182, amount#185]\n",
            "         :  +- Filter (isnotnull(coalesce(cast(gettimestamp(order_date#183, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#183, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#183, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date))) AND isnotnull(product_id#182))\n",
            "         :     +- LogicalRDD [order_id#180, customer_id#181, product_id#182, order_date#183, status#184, amount#185], false\n",
            "         +- Project [product_id#175]\n",
            "            +- Filter isnotnull(product_id#175)\n",
            "               +- LogicalRDD [product_id#175, product#176, category#177, seller_id#178, start_date#179], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Window [rank(total_spend#694) windowspecdefinition(total_spend#694 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS spend_rank#706], [total_spend#694 DESC NULLS LAST]\n",
            "   +- Sort [total_spend#694 DESC NULLS LAST], false, 0\n",
            "      +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=7648]\n",
            "         +- HashAggregate(keys=[customer_id#181], functions=[sum(cast(amount#185 as double))], output=[customer_id#181, total_spend#694])\n",
            "            +- Exchange hashpartitioning(customer_id#181, 200), ENSURE_REQUIREMENTS, [plan_id=7645]\n",
            "               +- HashAggregate(keys=[customer_id#181], functions=[partial_sum(cast(amount#185 as double))], output=[customer_id#181, sum#719])\n",
            "                  +- Project [customer_id#181, amount#185]\n",
            "                     +- SortMergeJoin [product_id#182], [product_id#175], Inner\n",
            "                        :- Sort [product_id#182 ASC NULLS FIRST], false, 0\n",
            "                        :  +- Exchange hashpartitioning(product_id#182, 200), ENSURE_REQUIREMENTS, [plan_id=7637]\n",
            "                        :     +- Project [customer_id#181, product_id#182, amount#185]\n",
            "                        :        +- Filter (isnotnull(coalesce(cast(gettimestamp(order_date#183, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#183, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#183, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date))) AND isnotnull(product_id#182))\n",
            "                        :           +- Scan ExistingRDD[order_id#180,customer_id#181,product_id#182,order_date#183,status#184,amount#185]\n",
            "                        +- Sort [product_id#175 ASC NULLS FIRST], false, 0\n",
            "                           +- Exchange hashpartitioning(product_id#175, 200), ENSURE_REQUIREMENTS, [plan_id=7638]\n",
            "                              +- Project [product_id#175]\n",
            "                                 +- Filter isnotnull(product_id#175)\n",
            "                                    +- Scan ExistingRDD[product_id#175,product#176,category#177,seller_id#178,start_date#179]\n",
            "\n",
            "== Parsed Logical Plan ==\n",
            "'Sort ['total_revenue DESC NULLS LAST], true\n",
            "+- Aggregate [category#177], [category#177, sum(cast(amount#185 as double)) AS total_revenue#575]\n",
            "   +- Project [product_id#182, order_id#180, customer_id#181, order_date#263, status#184, amount#185, product#176, category#177, seller_id#178, price#246]\n",
            "      +- Join Inner, (product_id#182 = product_id#175)\n",
            "         :- Filter isnotnull(order_date#263)\n",
            "         :  +- Filter isnotnull(order_date#263)\n",
            "         :     +- Filter isnotnull(order_date#263)\n",
            "         :        +- Project [order_id#180, customer_id#181, product_id#182, coalesce(to_date(try_to_timestamp(order_date#183, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#183, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#183, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS order_date#263, status#184, amount#185]\n",
            "         :           +- LogicalRDD [order_id#180, customer_id#181, product_id#182, order_date#183, status#184, amount#185], false\n",
            "         +- Project [product_id#175, product#176, category#177, seller_id#178, cast(price#245 as int) AS price#246]\n",
            "            +- Project [product_id#175, product#176, category#177, seller_id#178, start_date#179 AS price#245]\n",
            "               +- LogicalRDD [product_id#175, product#176, category#177, seller_id#178, start_date#179], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "category: string, total_revenue: double\n",
            "Sort [total_revenue#575 DESC NULLS LAST], true\n",
            "+- Aggregate [category#177], [category#177, sum(cast(amount#185 as double)) AS total_revenue#575]\n",
            "   +- Project [product_id#182, order_id#180, customer_id#181, order_date#263, status#184, amount#185, product#176, category#177, seller_id#178, price#246]\n",
            "      +- Join Inner, (product_id#182 = product_id#175)\n",
            "         :- Filter isnotnull(order_date#263)\n",
            "         :  +- Filter isnotnull(order_date#263)\n",
            "         :     +- Filter isnotnull(order_date#263)\n",
            "         :        +- Project [order_id#180, customer_id#181, product_id#182, coalesce(to_date(try_to_timestamp(order_date#183, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#183, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#183, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS order_date#263, status#184, amount#185]\n",
            "         :           +- LogicalRDD [order_id#180, customer_id#181, product_id#182, order_date#183, status#184, amount#185], false\n",
            "         +- Project [product_id#175, product#176, category#177, seller_id#178, cast(price#245 as int) AS price#246]\n",
            "            +- Project [product_id#175, product#176, category#177, seller_id#178, start_date#179 AS price#245]\n",
            "               +- LogicalRDD [product_id#175, product#176, category#177, seller_id#178, start_date#179], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Sort [total_revenue#575 DESC NULLS LAST], true\n",
            "+- Aggregate [category#177], [category#177, sum(cast(amount#185 as double)) AS total_revenue#575]\n",
            "   +- Project [amount#185, category#177]\n",
            "      +- Join Inner, (product_id#182 = product_id#175)\n",
            "         :- Project [product_id#182, amount#185]\n",
            "         :  +- Filter (isnotnull(coalesce(cast(gettimestamp(order_date#183, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#183, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#183, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date))) AND isnotnull(product_id#182))\n",
            "         :     +- LogicalRDD [order_id#180, customer_id#181, product_id#182, order_date#183, status#184, amount#185], false\n",
            "         +- Project [product_id#175, category#177]\n",
            "            +- Filter isnotnull(product_id#175)\n",
            "               +- LogicalRDD [product_id#175, product#176, category#177, seller_id#178, start_date#179], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Sort [total_revenue#575 DESC NULLS LAST], true, 0\n",
            "   +- Exchange rangepartitioning(total_revenue#575 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=7706]\n",
            "      +- HashAggregate(keys=[category#177], functions=[sum(cast(amount#185 as double))], output=[category#177, total_revenue#575])\n",
            "         +- Exchange hashpartitioning(category#177, 200), ENSURE_REQUIREMENTS, [plan_id=7703]\n",
            "            +- HashAggregate(keys=[category#177], functions=[partial_sum(cast(amount#185 as double))], output=[category#177, sum#590])\n",
            "               +- Project [amount#185, category#177]\n",
            "                  +- SortMergeJoin [product_id#182], [product_id#175], Inner\n",
            "                     :- Sort [product_id#182 ASC NULLS FIRST], false, 0\n",
            "                     :  +- Exchange hashpartitioning(product_id#182, 200), ENSURE_REQUIREMENTS, [plan_id=7695]\n",
            "                     :     +- Project [product_id#182, amount#185]\n",
            "                     :        +- Filter (isnotnull(coalesce(cast(gettimestamp(order_date#183, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#183, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#183, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date))) AND isnotnull(product_id#182))\n",
            "                     :           +- Scan ExistingRDD[order_id#180,customer_id#181,product_id#182,order_date#183,status#184,amount#185]\n",
            "                     +- Sort [product_id#175 ASC NULLS FIRST], false, 0\n",
            "                        +- Exchange hashpartitioning(product_id#175, 200), ENSURE_REQUIREMENTS, [plan_id=7696]\n",
            "                           +- Project [product_id#175, category#177]\n",
            "                              +- Filter isnotnull(product_id#175)\n",
            "                                 +- Scan ExistingRDD[product_id#175,product#176,category#177,seller_id#178,start_date#179]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Identify:\n",
        "Shuffles\n",
        "Broadcast joins\n",
        "Sort stages"
      ],
      "metadata": {
        "id": "bnCMbkQfP0SG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Shuffles: GroupBy, sort\n",
        "#Broadcast joins: Seller join\n",
        "#Sort stages: Window+OrderBy"
      ],
      "metadata": {
        "id": "fC2M5WcpPvrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Suggest one performance improvement"
      ],
      "metadata": {
        "id": "kOtR90U-QAgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance Improvement Suggestion:\n",
        "# Cache the 'orders_products_df' DataFrame.\n",
        "# This DataFrame is the result of a join and is used multiple times in subsequent calculations\n",
        "# (e.g., total revenue per category/seller, running revenue, top products).\n",
        "# Caching it will prevent Spark from recomputing this DataFrame every time it's accessed.\n",
        "orders_products_df.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpimqrNPQS8F",
        "outputId": "467de99f-3374-41bf-a162-dab65a5820cd"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[product_id: string, order_id: string, customer_id: string, order_date: date, status: string, amount: string, product: string, category: string, seller_id: string, price: int]"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RsDyWcweQZcu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}