{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcHSSFqKenTpbTiTNhRPVy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codingniket/Python-Training/blob/main/19_12_2025/Excericise_1_19_12_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "REAL-TIME CASE STUDY"
      ],
      "metadata": {
        "id": "oxJjZegW6bKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lit, when,regexp_replace, split, trim, array_compact, transform\n",
        "spark = SparkSession.builder.appName(\"Excercise1\").getOrCreate()"
      ],
      "metadata": {
        "id": "x2t2_pyc6whi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YL2rGhRy5nxg"
      },
      "outputs": [],
      "source": [
        "user_data = [\n",
        "(\"U001\",\"Amit\",\"28\",\"Hyderabad\",\"AI,ML,Cloud\"),\n",
        "(\"U002\",\"Neha\",\"Thirty\",\"Delhi\",\"Testing\"),\n",
        "(\"U003\",\"Ravi\",None,\"Bangalore\",[\"Data\",\"Spark\"]),\n",
        "(\"U004\",\"Pooja\",\"29\",\"Mumbai\",\"AI|ML\"),\n",
        "(\"U005\",\"\", \"31\",\"Chennai\",None)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import (StructType, StructField, StringType,LongType,IntegerType,ArrayType,MapType)"
      ],
      "metadata": {
        "id": "1ujOvzlB7dmI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), nullable=False),\n",
        "    StructField(\"name\", StringType(), nullable=True),\n",
        "    StructField(\"age\", StringType(), nullable=True),\n",
        "    StructField(\"city\", StringType(), nullable=True),\n",
        "    StructField(\"skills\", StringType(), nullable=True)\n",
        "])\n",
        "\n",
        "df_data = spark.createDataFrame(user_data, user_schema)\n",
        "df_data.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhQppC__7Q0m",
        "outputId": "5d85e25c-7ed4-48f4-addb-7facbb6940a3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+------+---------+-------------+\n",
            "|user_id|name |age   |city     |skills       |\n",
            "+-------+-----+------+---------+-------------+\n",
            "|U001   |Amit |28    |Hyderabad|AI,ML,Cloud  |\n",
            "|U002   |Neha |Thirty|Delhi    |Testing      |\n",
            "|U003   |Ravi |NULL  |Bangalore|[Data, Spark]|\n",
            "|U004   |Pooja|29    |Mumbai   |AI|ML        |\n",
            "|U005   |     |31    |Chennai  |NULL         |\n",
            "+-------+-----+------+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df = df_data.withColumn(\"age\", when(col(\"age\") == \"\", None)\n",
        "    .when(col(\"age\").rlike(r\"^\\d+$\"),\n",
        "          col(\"age\").cast(IntegerType()))\n",
        "    .otherwise(None))\n",
        "\n",
        "clean_data = clean_df.withColumn(\n",
        "    \"skills\",\n",
        "    (when(\n",
        "        col(\"skills\").isNull(),\n",
        "        None\n",
        "    ).otherwise(\n",
        "        array_compact(\n",
        "            transform(\n",
        "                split(\n",
        "                    regexp_replace(col(\"skills\"), \"[']\", \"\"),\",\"),lambda x: trim(x)\n",
        "            )\n",
        "        )\n",
        "    )).cast(ArrayType(StringType()))\n",
        ")\n",
        "\n",
        "clean_data.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvKtWwk_71Yi",
        "outputId": "f35e81bb-58c3-4038-8094-7293848e48da"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+----+---------+-------------+\n",
            "|user_id|name |age |city     |skills       |\n",
            "+-------+-----+----+---------+-------------+\n",
            "|U001   |Amit |28  |Hyderabad|AI,ML,Cloud  |\n",
            "|U002   |Neha |NULL|Delhi    |Testing      |\n",
            "|U003   |Ravi |NULL|Bangalore|[Data, Spark]|\n",
            "|U004   |Pooja|29  |Mumbai   |AI|ML        |\n",
            "|U005   |     |31  |Chennai  |NULL         |\n",
            "+-------+-----+----+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Skills Column needs to be fixed"
      ],
      "metadata": {
        "id": "qn2GfG3P8Ldo"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "courses_data = [\n",
        "(\"C001\",\"PySpark Mastery\",\"Data Engineering\",\"Advanced\",\"₹9999\"),\n",
        "(\"C002\",\"AI for Testers\",\"QA\",\"Beginner\",\"8999\"),\n",
        "(\"C003\",\"ML Foundations\",\"AI\",\"Intermediate\",None),\n",
        "(\"C004\",\"Data Engineering Bootcamp\",\"Data\",\"Advanced\",\"₹14999\")\n",
        "]"
      ],
      "metadata": {
        "id": "Hah_5Qn99ur5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "course_schema = StructType([\n",
        "    StructField(\"course_id\", StringType(), nullable=False),\n",
        "    StructField(\"course_name\", StringType(), nullable=True),\n",
        "    StructField(\"skills\", StringType(), nullable=True),\n",
        "     StructField(\"level\", StringType(), nullable=True),\n",
        "    StructField(\"amount\", StringType(), nullable=True),\n",
        "])\n",
        "\n",
        "df_course = spark.createDataFrame(courses_data, course_schema)\n",
        "df_course.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tODJPsJ69w2s",
        "outputId": "b49a184f-e08b-44cb-d1fd-509ec9098d9e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------------+----------------+------------+------+\n",
            "|course_id|course_name              |skills          |level       |amount|\n",
            "+---------+-------------------------+----------------+------------+------+\n",
            "|C001     |PySpark Mastery          |Data Engineering|Advanced    |₹9999 |\n",
            "|C002     |AI for Testers           |QA              |Beginner    |8999  |\n",
            "|C003     |ML Foundations           |AI              |Intermediate|NULL  |\n",
            "|C004     |Data Engineering Bootcamp|Data            |Advanced    |₹14999|\n",
            "+---------+-------------------------+----------------+------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "course_clean_data = df_course.withColumn(\n",
        "    \"amount\",\n",
        "    when(col(\"amount\").isNull() , 0 ).otherwise(regexp_replace(col(\"amount\"), \"₹\", \"\"))\n",
        "    .cast('int')\n",
        ")\n",
        "course_clean_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCN1a6Xt-dV_",
        "outputId": "6f5da8aa-8fa8-4dfc-9581-8b4db9fc2b81"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+----------------+------------+------+\n",
            "|course_id|         course_name|          skills|       level|amount|\n",
            "+---------+--------------------+----------------+------------+------+\n",
            "|     C001|     PySpark Mastery|Data Engineering|    Advanced|  9999|\n",
            "|     C002|      AI for Testers|              QA|    Beginner|  8999|\n",
            "|     C003|      ML Foundations|              AI|Intermediate|     0|\n",
            "|     C004|Data Engineering ...|            Data|    Advanced| 14999|\n",
            "+---------+--------------------+----------------+------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_enrollment_data = [\n",
        "(\"U001\",\"C001\",\"2024-01-05\"),\n",
        "(\"U002\",\"C002\",\"05/01/2024\"),\n",
        "(\"U003\",\"C001\",\"2024/01/06\"),\n",
        "(\"U004\",\"C003\",\"invalid_date\"),\n",
        "(\"U001\",\"C004\",\"2024-01-10\"),\n",
        "(\"U005\",\"C002\",\"2024-01-12\")\n",
        "]"
      ],
      "metadata": {
        "id": "MJaXP6A6-rfn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enrollment_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), nullable=False),\n",
        "    StructField(\"course_id\", StringType(), nullable=False),\n",
        "    StructField(\"enrollment_date\", StringType(), nullable=True),\n",
        "])\n",
        "\n",
        "df_enrollment = spark.createDataFrame(user_enrollment_data, enrollment_schema)\n",
        "df_enrollment.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUOlyZb5_Ed1",
        "outputId": "45ec3ae4-3810-4d0f-e6e3-0e3bd5b3091b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+---------------+\n",
            "|user_id|course_id|enrollment_date|\n",
            "+-------+---------+---------------+\n",
            "|U001   |C001     |2024-01-05     |\n",
            "|U002   |C002     |05/01/2024     |\n",
            "|U003   |C001     |2024/01/06     |\n",
            "|U004   |C003     |invalid_date   |\n",
            "|U001   |C004     |2024-01-10     |\n",
            "|U005   |C002     |2024-01-12     |\n",
            "+-------+---------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import coalesce, try_to_timestamp,array\n",
        "from pyspark.sql.types import DateType\n",
        "\n",
        "df_enrollment_clean = df_enrollment.withColumn(\n",
        "    \"enrollment_date\",\n",
        "    coalesce(\n",
        "        try_to_timestamp(col(\"enrollment_date\"), lit(\"yyyy-MM-dd\")).cast(DateType()),\n",
        "        try_to_timestamp(col(\"enrollment_date\"), lit(\"dd/MM/yyyy\")).cast(DateType()),\n",
        "        try_to_timestamp(col(\"enrollment_date\"), lit(\"yyyy/MM/dd\")).cast(DateType())\n",
        "    )\n",
        ")\n",
        "\n",
        "df_enrollment_clean.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRBB_IAG_QHr",
        "outputId": "2adb47b9-b34d-4be7-ff17-13d685de6e90"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+---------------+\n",
            "|user_id|course_id|enrollment_date|\n",
            "+-------+---------+---------------+\n",
            "|U001   |C001     |2024-01-05     |\n",
            "|U002   |C002     |2024-01-05     |\n",
            "|U003   |C001     |2024-01-06     |\n",
            "|U004   |C003     |NULL           |\n",
            "|U001   |C004     |2024-01-10     |\n",
            "|U005   |C002     |2024-01-12     |\n",
            "+-------+---------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_activity_log = [\n",
        "(\"U001\",\"login,watch,logout\",\"{'device':'mobile'}\",120),\n",
        "(\"U002\",[\"login\",\"watch\"],\"device=laptop\",90),\n",
        "(\"U003\",\"login|logout\",None,30),\n",
        "(\"U004\",None,\"{'device':'tablet'}\",60),\n",
        "(\"U005\",\"login\",\"{'device':'mobile'}\",15)\n",
        "]"
      ],
      "metadata": {
        "id": "EVNi3uyc_2Qi"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_activity_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), nullable=False),\n",
        "    StructField(\"activity_log\", StringType(), nullable=True),\n",
        "    StructField(\"device_info\", StringType(), nullable=True),\n",
        "    StructField(\"time\",IntegerType(), nullable=True)\n",
        "])\n",
        "\n",
        "df_activity = spark.createDataFrame(user_activity_log, user_activity_schema)\n",
        "df_activity.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cAw870EAekR",
        "outputId": "b4428cd8-ed92-4ca7-f789-7f02ba7c5ac7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+-------------------+----+\n",
            "|user_id|activity_log      |device_info        |time|\n",
            "+-------+------------------+-------------------+----+\n",
            "|U001   |login,watch,logout|{'device':'mobile'}|120 |\n",
            "|U002   |[login, watch]    |device=laptop      |90  |\n",
            "|U003   |login|logout      |NULL               |30  |\n",
            "|U004   |NULL              |{'device':'tablet'}|60  |\n",
            "|U005   |login             |{'device':'mobile'}|15  |\n",
            "+-------+------------------+-------------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_activity_clean = df_activity.withColumn(\n",
        "    \"activity_log_clean\",\n",
        "    when(col(\"activity_log\").isNull(), array())  # Empty array for null\n",
        "    .when(col(\"activity_log\").contains(\"['\"),\n",
        "          # Handle Python list format: ['login', 'watch']\n",
        "          split(\n",
        "              regexp_replace(\n",
        "                  regexp_replace(col(\"activity_log\"), r\"\\[|\\]|'\", \"\"),\n",
        "                  \" \", \"\"\n",
        "              ),\n",
        "              \",\"\n",
        "          )\n",
        "    )\n",
        "    .when(col(\"activity_log\").contains(\"|\"),\n",
        "          # Handle pipe delimiter\n",
        "          split(col(\"activity_log\"), r\"\\|\")\n",
        "    )\n",
        "    .otherwise(\n",
        "          # Handle comma delimiter (default)\n",
        "          split(col(\"activity_log\"), \",\")\n",
        "    )\n",
        ").drop(\"activity_log\")\n",
        "\n",
        "df_activity_clean.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "YAMUzaRrBJRp",
        "outputId": "35038ab9-abbf-4dde-afa2-41c7b77473e3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "{\"ts\": \"2025-12-19 05:22:28.440\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `[]'` cannot be resolved. Did you mean one of the following? [`time`, `user_id`, `device_info`, `activity_log`]. SQLSTATE: 42703\", \"context\": {\"file\": \"jdk.internal.reflect.GeneratedMethodAccessor59.invoke(Unknown Source)\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o261.withColumn.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `[]'` cannot be resolved. Did you mean one of the following? [`time`, `user_id`, `device_info`, `activity_log`]. SQLSTATE: 42703;\\n'Project [user_id#130, activity_log#131, device_info#132, time#133, CASE WHEN isnull(activity_log#131) THEN array() WHEN Contains(activity_log#131, [') THEN 'split('regexp_replace('trim('[]', activity_log#131),  , ), ,, -1) WHEN Contains(activity_log#131, |) THEN split(activity_log#131, \\\\|, -1) ELSE split(activity_log#131, ,, -1) END AS activity_log_clean#187]\\n+- LogicalRDD [user_id#130, activity_log#131, device_info#132, time#133], false\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\\n\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\\n\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\\n\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2263)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumns(Dataset.scala:1283)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumns(Dataset.scala:232)\\n\\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2187)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumn(Dataset.scala:1819)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\\n\\t\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\\n\\t\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\\n\\t\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\\n\\t\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\\n\\t\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\\n\\t\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 23 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `[]'` cannot be resolved. Did you mean one of the following? [`time`, `user_id`, `device_info`, `activity_log`]. SQLSTATE: 42703;\n'Project [user_id#130, activity_log#131, device_info#132, time#133, CASE WHEN isnull(activity_log#131) THEN array() WHEN Contains(activity_log#131, [') THEN 'split('regexp_replace('trim('[]', activity_log#131),  , ), ,, -1) WHEN Contains(activity_log#131, |) THEN split(activity_log#131, \\|, -1) ELSE split(activity_log#131, ,, -1) END AS activity_log_clean#187]\n+- LogicalRDD [user_id#130, activity_log#131, device_info#132, time#133], false\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3674975880.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m df_activity_clean = df_activity.withColumn(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"activity_log_clean\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"activity_log\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Empty array for null\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     .when(col(\"activity_log\").contains(\"['\"),\n\u001b[1;32m      5\u001b[0m           \u001b[0;31m# Handle Python list format: ['login', 'watch']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1621\u001b[0m                 \u001b[0mmessageParameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m             )\n\u001b[0;32m-> 1623\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1625\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mParentDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `[]'` cannot be resolved. Did you mean one of the following? [`time`, `user_id`, `device_info`, `activity_log`]. SQLSTATE: 42703;\n'Project [user_id#130, activity_log#131, device_info#132, time#133, CASE WHEN isnull(activity_log#131) THEN array() WHEN Contains(activity_log#131, [') THEN 'split('regexp_replace('trim('[]', activity_log#131),  , ), ,, -1) WHEN Contains(activity_log#131, |) THEN split(activity_log#131, \\|, -1) ELSE split(activity_log#131, ,, -1) END AS activity_log_clean#187]\n+- LogicalRDD [user_id#130, activity_log#131, device_info#132, time#133], false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_activity_"
      ],
      "metadata": {
        "id": "tmT83muaCsxf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}