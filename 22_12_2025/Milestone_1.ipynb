{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrfHrQujuYmjJ4LDYkArc/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codingniket/Python-Training/blob/main/22_12_2025/Milestone_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASErbb6ts1qw"
      },
      "outputs": [],
      "source": [
        "raw_drivers = [\n",
        "(\"D001\",\"Ramesh\",\"35\",\"Hyderabad\",\"Car,Bike\"),\n",
        "(\"D002\",\"Suresh\",\"Forty\",\"Bangalore\",\"Auto\"),\n",
        "(\"D003\",\"Anita\",None,\"Mumbai\",[\"Car\"]),\n",
        "(\"D004\",\"Kiran\",\"29\",\"Delhi\",\"Car|Bike\"),\n",
        "(\"D005\",\"\", \"42\",\"Chennai\",None)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lit, when,regexp_replace, split, trim, array_compact, transform, get_json_object, lower\n",
        "spark = SparkSession.builder.appName(\"MileStone1\").getOrCreate()\n",
        "from pyspark.sql.types import (StructType, StructField, StringType,LongType,IntegerType,ArrayType,MapType)"
      ],
      "metadata": {
        "id": "n7GrBZiItR20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "driver_schema = StructType([\n",
        "    StructField(\"driverid\", StringType(), nullable=False),\n",
        "    StructField(\"name\", StringType(), nullable=True),\n",
        "    StructField(\"age\", StringType(), nullable=True),\n",
        "    StructField(\"city\", StringType(), nullable=True),\n",
        "    StructField(\"vechile\", StringType(), nullable=True)\n",
        "])\n",
        "df = spark.createDataFrame(raw_drivers,driver_schema)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymtMrX6hxErG",
        "outputId": "23619833-63af-41eb-b848-6b1683e84a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+-----+---------+--------+\n",
            "|driverid|  name|  age|     city| vechile|\n",
            "+--------+------+-----+---------+--------+\n",
            "|    D001|Ramesh|   35|Hyderabad|Car,Bike|\n",
            "|    D002|Suresh|Forty|Bangalore|    Auto|\n",
            "|    D003| Anita| NULL|   Mumbai|   [Car]|\n",
            "|    D004| Kiran|   29|    Delhi|Car|Bike|\n",
            "|    D005|      |   42|  Chennai|    NULL|\n",
            "+--------+------+-----+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fixing Given Issue Below"
      ],
      "metadata": {
        "id": "xZKeWHh_xths"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Known Issues\n",
        "Age in mixed formats\n",
        "\n",
        "Vehicle types in string / array / multiple delimiters\n",
        "\n",
        "Missing names\n",
        "\n",
        "Null value"
      ],
      "metadata": {
        "id": "d-Vt2sZFxv78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_age = df.withColumn(\"age\", when(col(\"age\") == \"\", None)\n",
        "    .when(col(\"age\").rlike(r\"^\\d+$\"),\n",
        "          col(\"age\").cast(IntegerType()))\n",
        "    .otherwise(None))\n",
        "\n",
        "clean_name_city_vechile = clean_age.withColumn(\"name\", when(col(\"name\") == \"\", None)\n",
        "    .otherwise(col(\"name\"))) \\\n",
        ".withColumn(\"city\",trim(col(\"city\")))\\\n",
        ".withColumn(\n",
        "    \"vechile\",\n",
        "    (when(\n",
        "        col(\"vechile\").isNull(),\n",
        "        None\n",
        "    ).otherwise(\n",
        "        array_compact(\n",
        "            transform(\n",
        "                split(\n",
        "                    regexp_replace(\n",
        "                        col(\"vechile\"),\n",
        "                        r\"\\[|\\]|'|\\|\", \",\"),\n",
        "                    \",\"),\n",
        "                lambda x: when(trim(x) != lit(\"\"), trim(x)).otherwise(lit(None))\n",
        "            )\n",
        "        )\n",
        "    )).cast(ArrayType(StringType()))\n",
        ")\n",
        "\n",
        "clean_name_city_vechile.show()\n",
        "\n",
        "driver_df = clean_name_city_vechile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSH9j2Msw8is",
        "outputId": "06c914e0-ee9c-4a8e-d9ec-d5e73dd0e013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+----+---------+-----------+\n",
            "|driverid|  name| age|     city|    vechile|\n",
            "+--------+------+----+---------+-----------+\n",
            "|    D001|Ramesh|  35|Hyderabad|[Car, Bike]|\n",
            "|    D002|Suresh|NULL|Bangalore|     [Auto]|\n",
            "|    D003| Anita|NULL|   Mumbai|      [Car]|\n",
            "|    D004| Kiran|  29|    Delhi|[Car, Bike]|\n",
            "|    D005|  NULL|  42|  Chennai|       NULL|\n",
            "+--------+------+----+---------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_cities = [\n",
        "(\"Hyderabad\",\"South\"),\n",
        "(\"Bangalore\",\"South\"),\n",
        "(\"Mumbai\",\"West\"),\n",
        "(\"Delhi\",\"North\"),\n",
        "(\"Chennai\",\"South\")\n",
        "]"
      ],
      "metadata": {
        "id": "vOmeLYy-yWa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "city_schema = StructType([\n",
        "    StructField(\"city\", StringType(), nullable=True),\n",
        "    StructField(\"region\", StringType(), nullable=True)\n",
        "])\n",
        "city_df = spark.createDataFrame(raw_cities,city_schema)\n",
        "city_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOvesvwSzeSF",
        "outputId": "f6da7388-a752-4430-cd0d-09e911bfd76d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+\n",
            "|     city|region|\n",
            "+---------+------+\n",
            "|Hyderabad| South|\n",
            "|Bangalore| South|\n",
            "|   Mumbai|  West|\n",
            "|    Delhi| North|\n",
            "|  Chennai| South|\n",
            "+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes\n",
        "\n",
        "Small reference dataset\n",
        "\n",
        "Intended for broadcast join"
      ],
      "metadata": {
        "id": "6gZQX4XI0Dnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast"
      ],
      "metadata": {
        "id": "QHgxJCb_0XqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "driver_join  = driver_df.join(broadcast(city_df), \"city\", \"inner\")\n",
        "driver_join.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sonBQ6bz4x0",
        "outputId": "d2f477ce-9f1b-47b6-b773-a7bce22065b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+------+----+-----------+------+\n",
            "|     city|driverid|  name| age|    vechile|region|\n",
            "+---------+--------+------+----+-----------+------+\n",
            "|Hyderabad|    D001|Ramesh|  35|[Car, Bike]| South|\n",
            "|Bangalore|    D002|Suresh|NULL|     [Auto]| South|\n",
            "|   Mumbai|    D003| Anita|NULL|      [Car]|  West|\n",
            "|    Delhi|    D004| Kiran|  29|[Car, Bike]| North|\n",
            "|  Chennai|    D005|  NULL|  42|       NULL| South|\n",
            "+---------+--------+------+----+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_trips = [\n",
        "(\"T001\",\"D001\",\"Hyderabad\",\"2024-01-05\",\"Completed\",\"450\"),\n",
        "(\"T002\",\"D002\",\"Bangalore\",\"05/01/2024\",\"Cancelled\",\"0\"),\n",
        "(\"T003\",\"D003\",\"Mumbai\",\"2024/01/06\",\"Completed\",\"620\"),\n",
        "(\"T004\",\"D004\",\"Delhi\",\"invalid_date\",\"Completed\",\"540\"),\n",
        "(\"T005\",\"D001\",\"Hyderabad\",\"2024-01-10\",\"Completed\",\"700\"),\n",
        "(\"T006\",\"D005\",\"Chennai\",\"2024-01-12\",\"Completed\",\"350\")\n",
        "]"
      ],
      "metadata": {
        "id": "pHV7sDvK0lsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trips_schema = StructType([\n",
        "    StructField(\"userid\", StringType(), nullable=False),\n",
        "    StructField(\"driverid\", StringType(), nullable=False),\n",
        "    StructField(\"city\", StringType(), nullable=True),\n",
        "    StructField(\"date\", StringType(), nullable=True),\n",
        "    StructField(\"status\", StringType(), nullable=True),\n",
        "    StructField(\"amount\", StringType(), nullable=True),\n",
        "])\n",
        "trips_df = spark.createDataFrame(raw_trips,trips_schema)\n",
        "trips_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LsiSpMP0uy8",
        "outputId": "72870747-afa4-4226-cc3c-852bdd6ad704"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------+------------+---------+------+\n",
            "|userid|driverid|     city|        date|   status|amount|\n",
            "+------+--------+---------+------------+---------+------+\n",
            "|  T001|    D001|Hyderabad|  2024-01-05|Completed|   450|\n",
            "|  T002|    D002|Bangalore|  05/01/2024|Cancelled|     0|\n",
            "|  T003|    D003|   Mumbai|  2024/01/06|Completed|   620|\n",
            "|  T004|    D004|    Delhi|invalid_date|Completed|   540|\n",
            "|  T005|    D001|Hyderabad|  2024-01-10|Completed|   700|\n",
            "|  T006|    D005|  Chennai|  2024-01-12|Completed|   350|\n",
            "+------+--------+---------+------------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, to_date, coalesce, split, lit, array_remove, try_to_timestamp"
      ],
      "metadata": {
        "id": "lqBlF9Cf1Zmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_date_amount = trips_df.withColumn(\"amount\", col(\"amount\").cast(IntegerType()))\\\n",
        ".withColumn(\n",
        "    \"date\",\n",
        "    coalesce(\n",
        "        to_date(try_to_timestamp(col(\"date\"), lit(\"yyyy-MM-dd\"))),\n",
        "        to_date(try_to_timestamp(col(\"date\"), lit(\"dd/MM/yyyy\"))),\n",
        "        to_date(try_to_timestamp(col(\"date\"), lit(\"yyyy/MM/dd\")))\n",
        "    )\n",
        ")\n",
        "\n",
        "clean_date_amount = clean_date_amount.filter(col(\"amount\") > 0)\n",
        "\n",
        "clean_date_amount.show()\n",
        "tripsdf=clean_date_amount"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLOOrDH51DIR",
        "outputId": "6534c7b7-bfdd-4d17-daf3-ab57c231414f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------+----------+---------+------+\n",
            "|userid|driverid|     city|      date|   status|amount|\n",
            "+------+--------+---------+----------+---------+------+\n",
            "|  T001|    D001|Hyderabad|2024-01-05|Completed|   450|\n",
            "|  T003|    D003|   Mumbai|2024-01-06|Completed|   620|\n",
            "|  T004|    D004|    Delhi|      NULL|Completed|   540|\n",
            "|  T005|    D001|Hyderabad|2024-01-10|Completed|   700|\n",
            "|  T006|    D005|  Chennai|2024-01-12|Completed|   350|\n",
            "+------+--------+---------+----------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nH5cE_Fk1scJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_activity = [\n",
        "(\"D001\",\"login,accept_trip,logout\",\"{'device':'mobile'}\",180),\n",
        "(\"D002\",[\"login\",\"logout\"],\"device=laptop\",60),\n",
        "(\"D003\",\"login|accept_trip\",None,120),\n",
        "(\"D004\",None,\"{'device':'tablet'}\",90),\n",
        "(\"D005\",\"login\",\"{'device':'mobile'}\",30)\n",
        "]"
      ],
      "metadata": {
        "id": "zuF572HI2xHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activity_schema = StructType([\n",
        "    StructField(\"userid\", StringType(), nullable=False),\n",
        "    StructField(\"actions\", StringType(), nullable=True),\n",
        "    StructField(\"device\", StringType(), nullable=True),\n",
        "    StructField(\"amount\", IntegerType(), nullable=True),\n",
        "])\n",
        "activity_df = spark.createDataFrame(raw_activity,activity_schema)\n",
        "activity_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rQUyG9Q1bTp",
        "outputId": "7cc6dfdd-31c3-46e5-bd2a-615002ca2531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+-------------------+------+\n",
            "|userid|             actions|             device|amount|\n",
            "+------+--------------------+-------------------+------+\n",
            "|  D001|login,accept_trip...|{'device':'mobile'}|   180|\n",
            "|  D002|     [login, logout]|      device=laptop|    60|\n",
            "|  D003|   login|accept_trip|               NULL|   120|\n",
            "|  D004|                NULL|{'device':'tablet'}|    90|\n",
            "|  D005|               login|{'device':'mobile'}|    30|\n",
            "+------+--------------------+-------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Known Issues\n",
        "Actions in multiple formats\n",
        "Metadata as JSON-like strings\n",
        "Missing actions"
      ],
      "metadata": {
        "id": "gJjoKUMf4KNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_activity_clean = activity_df.withColumn(\n",
        "    \"actions\",\n",
        "    (when(\n",
        "        col(\"actions\").isNull(),\n",
        "        None\n",
        "    ).otherwise(\n",
        "        array_compact(\n",
        "            transform(\n",
        "                split(\n",
        "                    regexp_replace(\n",
        "                        col(\"actions\"),\n",
        "                        r\"\\[|\\]|'|\\|\", \",\"),\n",
        "                    \",\"),\n",
        "                lambda x: when(trim(x) != lit(\"\"), trim(x)).otherwise(lit(None))\n",
        "            )\n",
        "        )\n",
        "    )).cast(ArrayType(StringType()))\n",
        ").withColumn(\n",
        "    \"device\",\n",
        "    when(col(\"device\").isNull(), None)\n",
        "    .when(col(\"device\").like(\"{'device':%}\"), get_json_object(col(\"device\"), \"$.device\"))\n",
        "    .when(col(\"device\").like(\"device=%\"), split(col(\"device\"), \"=\").getItem(1))\n",
        "    .otherwise(None)\n",
        ")\n",
        "\n",
        "df_activity_clean.show(truncate=False)\n",
        "df_activity_clean.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIMlaV643E7y",
        "outputId": "f8c774cd-7ea0-4899-9230-ecab826ae049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------------------------+------+------+\n",
            "|userid|actions                     |device|amount|\n",
            "+------+----------------------------+------+------+\n",
            "|D001  |[login, accept_trip, logout]|mobile|180   |\n",
            "|D002  |[login, logout]             |laptop|60    |\n",
            "|D003  |[login, accept_trip]        |NULL  |120   |\n",
            "|D004  |NULL                        |tablet|90    |\n",
            "|D005  |[login]                     |mobile|30    |\n",
            "+------+----------------------------+------+------+\n",
            "\n",
            "root\n",
            " |-- userid: string (nullable = false)\n",
            " |-- actions: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- device: string (nullable = true)\n",
            " |-- amount: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All clean data"
      ],
      "metadata": {
        "id": "0jCLbnH34hX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART A — DATA CLEANING & STRUCTURING\n",
        "\n",
        ". Design explicit schemas for all\n",
        " datasets\n",
        ". Normalize:\n",
        "\n",
        "Age\n",
        "\n",
        "Fare\n",
        "\n",
        "Dates\n",
        "\n",
        ". Convert vehicle types and actions into arrays\n",
        "\n",
        ". Handle missing and invalid records gracefully\n",
        "\n",
        ". Produce clean DataFrames:\n",
        "\n",
        "drivers_df\n",
        "\n",
        "cities_df\n",
        "\n",
        "trips_df\n",
        "\n",
        "activity_df"
      ],
      "metadata": {
        "id": "cXim-JGi4qfO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K3OvcDVJ4qQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_activity_clean.show()\n",
        "tripsdf.show()\n",
        "driver_join.show()\n",
        "driver_df.show()\n",
        "city_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KYxNuNv3oDK",
        "outputId": "2c83dd28-725b-4c58-c05b-da596cabd166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+------+------+\n",
            "|userid|             actions|device|amount|\n",
            "+------+--------------------+------+------+\n",
            "|  D001|[login, accept_tr...|mobile|   180|\n",
            "|  D002|     [login, logout]|laptop|    60|\n",
            "|  D003|[login, accept_trip]|  NULL|   120|\n",
            "|  D004|                NULL|tablet|    90|\n",
            "|  D005|             [login]|mobile|    30|\n",
            "+------+--------------------+------+------+\n",
            "\n",
            "+------+--------+---------+----------+---------+------+\n",
            "|userid|driverid|     city|      date|   status|amount|\n",
            "+------+--------+---------+----------+---------+------+\n",
            "|  T001|    D001|Hyderabad|2024-01-05|Completed|   450|\n",
            "|  T003|    D003|   Mumbai|2024-01-06|Completed|   620|\n",
            "|  T004|    D004|    Delhi|      NULL|Completed|   540|\n",
            "|  T005|    D001|Hyderabad|2024-01-10|Completed|   700|\n",
            "|  T006|    D005|  Chennai|2024-01-12|Completed|   350|\n",
            "+------+--------+---------+----------+---------+------+\n",
            "\n",
            "+---------+--------+------+----+-----------+------+\n",
            "|     city|driverid|  name| age|    vechile|region|\n",
            "+---------+--------+------+----+-----------+------+\n",
            "|Hyderabad|    D001|Ramesh|  35|[Car, Bike]| South|\n",
            "|Bangalore|    D002|Suresh|NULL|     [Auto]| South|\n",
            "|   Mumbai|    D003| Anita|NULL|      [Car]|  West|\n",
            "|    Delhi|    D004| Kiran|  29|[Car, Bike]| North|\n",
            "|  Chennai|    D005|  NULL|  42|       NULL| South|\n",
            "+---------+--------+------+----+-----------+------+\n",
            "\n",
            "+--------+------+----+---------+-----------+\n",
            "|driverid|  name| age|     city|    vechile|\n",
            "+--------+------+----+---------+-----------+\n",
            "|    D001|Ramesh|  35|Hyderabad|[Car, Bike]|\n",
            "|    D002|Suresh|NULL|Bangalore|     [Auto]|\n",
            "|    D003| Anita|NULL|   Mumbai|      [Car]|\n",
            "|    D004| Kiran|  29|    Delhi|[Car, Bike]|\n",
            "|    D005|  NULL|  42|  Chennai|       NULL|\n",
            "+--------+------+----+---------+-----------+\n",
            "\n",
            "+---------+------+\n",
            "|     city|region|\n",
            "+---------+------+\n",
            "|Hyderabad| South|\n",
            "|Bangalore| South|\n",
            "|   Mumbai|  West|\n",
            "|    Delhi| North|\n",
            "|  Chennai| South|\n",
            "+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART B — DATA INTEGRATION (JOINS)\n",
        "\n",
        ". Join trips with drivers\n",
        "\n",
        ". Join trips with cities\n",
        "\n",
        ". Decide which dataset should be\n",
        "broadcast\n",
        "\n",
        ". Prove your decision using explain(True)\n",
        "\n",
        ". Remove orphan trips (drivers not in master"
      ],
      "metadata": {
        "id": "OqcI2OuY40Fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "city_df.show()\n",
        "\n",
        "trips_city_join  = tripsdf.join(broadcast(city_df), \"city\", \"inner\")\n",
        "trips_city_join.show()\n",
        "\n",
        "trips_city_join.explain(True)\n",
        "\n",
        "ophan = trips_city_join.filter(~trips_city_join[\"date\"].isNull())\n",
        "ophan.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZa4-xaG4iT1",
        "outputId": "c9090bf2-816f-4424-d5ec-751ba901fd6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+\n",
            "|     city|region|\n",
            "+---------+------+\n",
            "|Hyderabad| South|\n",
            "|Bangalore| South|\n",
            "|   Mumbai|  West|\n",
            "|    Delhi| North|\n",
            "|  Chennai| South|\n",
            "+---------+------+\n",
            "\n",
            "+---------+------+--------+----------+---------+------+------+\n",
            "|     city|userid|driverid|      date|   status|amount|region|\n",
            "+---------+------+--------+----------+---------+------+------+\n",
            "|Hyderabad|  T001|    D001|2024-01-05|Completed|   450| South|\n",
            "|   Mumbai|  T003|    D003|2024-01-06|Completed|   620|  West|\n",
            "|    Delhi|  T004|    D004|      NULL|Completed|   540| North|\n",
            "|Hyderabad|  T005|    D001|2024-01-10|Completed|   700| South|\n",
            "|  Chennai|  T006|    D005|2024-01-12|Completed|   350| South|\n",
            "+---------+------+--------+----------+---------+------+------+\n",
            "\n",
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [city])\n",
            ":- Filter (amount#366 > 0)\n",
            ":  +- Project [userid#246, driverid#247, city#248, coalesce(to_date(try_to_timestamp(date#249, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(date#249, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(date#249, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS date#367, status#250, amount#366]\n",
            ":     +- Project [userid#246, driverid#247, city#248, date#249, status#250, cast(amount#251 as int) AS amount#366]\n",
            ":        +- LogicalRDD [userid#246, driverid#247, city#248, date#249, status#250, amount#251], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- LogicalRDD [city#195, region#196], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, userid: string, driverid: string, date: date, status: string, amount: int, region: string\n",
            "Project [city#248, userid#246, driverid#247, date#367, status#250, amount#366, region#196]\n",
            "+- Join Inner, (city#248 = city#195)\n",
            "   :- Filter (amount#366 > 0)\n",
            "   :  +- Project [userid#246, driverid#247, city#248, coalesce(to_date(try_to_timestamp(date#249, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(date#249, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(date#249, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS date#367, status#250, amount#366]\n",
            "   :     +- Project [userid#246, driverid#247, city#248, date#249, status#250, cast(amount#251 as int) AS amount#366]\n",
            "   :        +- LogicalRDD [userid#246, driverid#247, city#248, date#249, status#250, amount#251], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- LogicalRDD [city#195, region#196], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#248, userid#246, driverid#247, date#367, status#250, amount#366, region#196]\n",
            "+- Join Inner, (city#248 = city#195), rightHint=(strategy=broadcast)\n",
            "   :- Project [userid#246, driverid#247, city#248, coalesce(cast(gettimestamp(date#249, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(date#249, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(date#249, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS date#367, status#250, cast(amount#251 as int) AS amount#366]\n",
            "   :  +- Filter ((isnotnull(amount#251) AND (cast(amount#251 as int) > 0)) AND isnotnull(city#248))\n",
            "   :     +- LogicalRDD [userid#246, driverid#247, city#248, date#249, status#250, amount#251], false\n",
            "   +- Filter isnotnull(city#195)\n",
            "      +- LogicalRDD [city#195, region#196], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#248, userid#246, driverid#247, date#367, status#250, amount#366, region#196]\n",
            "   +- BroadcastHashJoin [city#248], [city#195], Inner, BuildRight, false\n",
            "      :- Project [userid#246, driverid#247, city#248, coalesce(cast(gettimestamp(date#249, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(date#249, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(date#249, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS date#367, status#250, cast(amount#251 as int) AS amount#366]\n",
            "      :  +- Filter ((isnotnull(amount#251) AND (cast(amount#251 as int) > 0)) AND isnotnull(city#248))\n",
            "      :     +- Scan ExistingRDD[userid#246,driverid#247,city#248,date#249,status#250,amount#251]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=934]\n",
            "         +- Filter isnotnull(city#195)\n",
            "            +- Scan ExistingRDD[city#195,region#196]\n",
            "\n",
            "+---------+------+--------+----------+---------+------+------+\n",
            "|     city|userid|driverid|      date|   status|amount|region|\n",
            "+---------+------+--------+----------+---------+------+------+\n",
            "|Hyderabad|  T001|    D001|2024-01-05|Completed|   450| South|\n",
            "|   Mumbai|  T003|    D003|2024-01-06|Completed|   620|  West|\n",
            "|Hyderabad|  T005|    D001|2024-01-10|Completed|   700| South|\n",
            "|  Chennai|  T006|    D005|2024-01-12|Completed|   350| South|\n",
            "+---------+------+--------+----------+---------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART C — ANALYTICS & AGGREGATIONS\n",
        ". Total trips per city\n",
        ". Total revenue per city\n",
        ". Average fare per driver\n",
        ". Total completed trips per driver\n",
        ". Identify drivers with no completed trip"
      ],
      "metadata": {
        "id": "UIi7Qsq46YST"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qgpVzMd15I-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ycuPaTrj5U-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART D — WINDOW FUNCTIONS\n",
        ". Rank drivers by total revenue (overall)\n",
        ". Rank drivers by revenue within each city\n",
        ". Calculate running revenue per city by date\n",
        ". Compare GroupBy vs Window for one metric"
      ],
      "metadata": {
        "id": "EwpcNjjB74MG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trips_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VH_MX7IX8ShO",
        "outputId": "8a65a401-7a7a-49f4-ff0f-4049de8b7eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------+------------+---------+------+\n",
            "|userid|driverid|     city|        date|   status|amount|\n",
            "+------+--------+---------+------------+---------+------+\n",
            "|  T001|    D001|Hyderabad|  2024-01-05|Completed|   450|\n",
            "|  T002|    D002|Bangalore|  05/01/2024|Cancelled|     0|\n",
            "|  T003|    D003|   Mumbai|  2024/01/06|Completed|   620|\n",
            "|  T004|    D004|    Delhi|invalid_date|Completed|   540|\n",
            "|  T005|    D001|Hyderabad|  2024-01-10|Completed|   700|\n",
            "|  T006|    D005|  Chennai|  2024-01-12|Completed|   350|\n",
            "+------+--------+---------+------------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "driver_revenue = trips_df.groupBy(\"driverid\") \\\n",
        "    .agg(F.sum(\"amount\").alias(\"total_revenue\")) \\\n",
        "    .orderBy(F.desc(\"total_revenue\"))\n",
        "\n",
        "driver_revenue.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlYX0tne75JH",
        "outputId": "5feb1898-7b5c-4cb5-c8a5-f59455414d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------------+\n",
            "|driverid|total_revenue|\n",
            "+--------+-------------+\n",
            "|    D001|       1150.0|\n",
            "|    D003|        620.0|\n",
            "|    D004|        540.0|\n",
            "|    D005|        350.0|\n",
            "|    D002|          0.0|\n",
            "+--------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window"
      ],
      "metadata": {
        "id": "Oug-Lbhb8iqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "city_driver_rank = trips_df.groupBy(\"city\", \"driverid\") \\\n",
        "    .agg(F.sum(\"amount\").alias(\"city_revenue\")) \\\n",
        "    .withColumn(\"rank\", F.rank().over(Window.partitionBy(\"city\").orderBy(F.desc(\"city_revenue\"))))\n",
        "city_driver_rank.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THk0umdU76Us",
        "outputId": "70464c14-7de8-40d9-b336-e76311eb5504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+------------+----+\n",
            "|     city|driverid|city_revenue|rank|\n",
            "+---------+--------+------------+----+\n",
            "|Bangalore|    D002|         0.0|   1|\n",
            "|  Chennai|    D005|       350.0|   1|\n",
            "|    Delhi|    D004|       540.0|   1|\n",
            "|Hyderabad|    D001|      1150.0|   1|\n",
            "|   Mumbai|    D003|       620.0|   1|\n",
            "+---------+--------+------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "city_date_window = Window.partitionBy(\"city\").orderBy(\"date\") \\\n",
        "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "running_revenue = trips_df.groupBy(\"city\", \"date\") \\\n",
        "    .agg(F.sum(\"amount\").alias(\"daily_revenue\")) \\\n",
        "    .withColumn(\"running_revenue\", F.sum(\"daily_revenue\").over(city_date_window))"
      ],
      "metadata": {
        "id": "VlXMLWXk8kHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ". Compare GroupBy vs Window for one metric\n",
        "GroupBy: Aggregates data into fewer rows (e.g., total revenue per driver).\n",
        "Window: Keeps original granularity but adds computed columns (e.g., rank, cumulative sum).\n",
        "Use Case:\n",
        "GroupBy → summary reports.\n",
        "Window → analytics like ranking, running totals without collapsing rows"
      ],
      "metadata": {
        "id": "Q1BfWmA98-96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART E — UDF (ONLY IF REQUIRED)\n",
        ". Classify drivers into performance levels:\n",
        "High\n",
        "Medium\n",
        "Rules:\n",
        "Low\n",
        "Prefer built-in functions\n",
        "Use UDF only if unavoidable\n",
        "Justify your choic"
      ],
      "metadata": {
        "id": "YEMO9tMx9WkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import udf, col\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Define classification based on revenue\n",
        "def classify_revenue(revenue):\n",
        "    if revenue >= 1000:\n",
        "        return \"High\"\n",
        "    elif revenue >= 500:\n",
        "        return \"Medium\"\n",
        "    else:\n",
        "        return \"Low\"\n",
        "\n",
        "# Register UDF\n",
        "classify_revenue_udf = udf(classify_revenue, StringType())\n",
        "\n",
        "# Apply UDF on city_revenue column\n",
        "city_driver_rank.withColumn(\"revenue_grade\", classify_revenue_udf(col(\"city_revenue\"))).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "DFLOfixe894m",
        "outputId": "203a67bb-98a5-4cbe-fc51-ad0211f8362a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "{\"ts\": \"2025-12-22 12:10:02.819\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `city_revenue` cannot be resolved. Did you mean one of the following? [`city`, `driverid`, `age`, `name`, `vechile`]. SQLSTATE: 42703\", \"context\": {\"file\": \"line 17 in cell [60]\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o49.withColumn.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `city_revenue` cannot be resolved. Did you mean one of the following? [`city`, `driverid`, `age`, `name`, `vechile`]. SQLSTATE: 42703;\\n'Project [driverid#0, name#1, age#2, city#3, vechile#4, classify_revenue('city_revenue)#761 AS revenue_grade#762]\\n+- LogicalRDD [driverid#0, name#1, age#2, city#3, vechile#4], false\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2263)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumns(Dataset.scala:1283)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumns(Dataset.scala:232)\\n\\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2187)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumn(Dataset.scala:1819)\\n\\tat jdk.internal.reflect.GeneratedMethodAccessor41.invoke(Unknown Source)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 22 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `city_revenue` cannot be resolved. Did you mean one of the following? [`city`, `driverid`, `age`, `name`, `vechile`]. SQLSTATE: 42703;\n'Project [driverid#0, name#1, age#2, city#3, vechile#4, classify_revenue('city_revenue)#761 AS revenue_grade#762]\n+- LogicalRDD [driverid#0, name#1, age#2, city#3, vechile#4], false\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2211091712.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Apply UDF on city_revenue column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"revenue_grade\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassify_revenue_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"city_revenue\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1621\u001b[0m                 \u001b[0mmessageParameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m             )\n\u001b[0;32m-> 1623\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1625\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mParentDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `city_revenue` cannot be resolved. Did you mean one of the following? [`city`, `driverid`, `age`, `name`, `vechile`]. SQLSTATE: 42703;\n'Project [driverid#0, name#1, age#2, city#3, vechile#4, classify_revenue('city_revenue)#761 AS revenue_grade#762]\n+- LogicalRDD [driverid#0, name#1, age#2, city#3, vechile#4], false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IiDtDGrI8_S7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}