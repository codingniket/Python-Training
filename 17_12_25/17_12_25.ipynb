{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOL+zqgDCG3gC4FYLjYGT8X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codingniket/Python-Training/blob/main/17_12_25/17_12_25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Uw_WwJo_a6ds"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"SQL\").getOrCreate()\n",
        "from pyspark.sql.functions import col,substring"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sales_data = [\n",
        "    (\"S001\",\"North\",\"Laptop\",\"2024-01-05\",75000),\n",
        "    (\"S002\",\"South\",\"Mobile\",\"2024-01-06\",30000),\n",
        "    (\"S003\",\"East\",\"Laptop\",\"2024-01-07\",72000),\n",
        "    (\"S004\",\"West\",\"Tablet\",\"2024-01-08\",25000),\n",
        "    (\"S005\",\"North\",\"Mobile\",\"2024-01-10\",28000),\n",
        "    (\"S006\",\"South\",\"Laptop\",\"2024-01-11\",76000),\n",
        "    (\"S007\",\"East\",\"Tablet\",\"2024-01-12\",24000),\n",
        "    (\"S008\",\"West\",\"Mobile\",\"2024-01-13\",32000),\n",
        "    (\"S009\",\"North\",\"Tablet\",\"2024-01-14\",26000),\n",
        "    (\"S010\",\"South\",\"Mobile\",\"2024-01-15\",31000),\n",
        "    (\"S011\",\"East\",\"Laptop\",\"2024-01-16\",74000),\n",
        "    (\"S012\",\"West\",\"Laptop\",\"2024-01-17\",78000)\n",
        "]\n",
        "\n",
        "sales_cols = [\"sales_id\",\"region\",\"product\",\"sale_date\",\"amount\"]\n",
        "\n",
        "df_sales = spark.createDataFrame(sales_data,sales_cols)\n",
        "df_sales.show()\n",
        "df_sales.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jsi23SZIcmIX",
        "outputId": "11b1bd0c-0b63-4bd7-aa6c-530866b42f4c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+-------+----------+------+\n",
            "|sales_id|region|product| sale_date|amount|\n",
            "+--------+------+-------+----------+------+\n",
            "|    S001| North| Laptop|2024-01-05| 75000|\n",
            "|    S002| South| Mobile|2024-01-06| 30000|\n",
            "|    S003|  East| Laptop|2024-01-07| 72000|\n",
            "|    S004|  West| Tablet|2024-01-08| 25000|\n",
            "|    S005| North| Mobile|2024-01-10| 28000|\n",
            "|    S006| South| Laptop|2024-01-11| 76000|\n",
            "|    S007|  East| Tablet|2024-01-12| 24000|\n",
            "|    S008|  West| Mobile|2024-01-13| 32000|\n",
            "|    S009| North| Tablet|2024-01-14| 26000|\n",
            "|    S010| South| Mobile|2024-01-15| 31000|\n",
            "|    S011|  East| Laptop|2024-01-16| 74000|\n",
            "|    S012|  West| Laptop|2024-01-17| 78000|\n",
            "+--------+------+-------+----------+------+\n",
            "\n",
            "root\n",
            " |-- sales_id: string (nullable = true)\n",
            " |-- region: string (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- sale_date: string (nullable = true)\n",
            " |-- amount: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sales.select(\"sales_id\",\"amount\",\"product\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y7mhvawd7iA",
        "outputId": "3540f457-5972-4c54-915e-094f1bfed47b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+-------+\n",
            "|sales_id|amount|product|\n",
            "+--------+------+-------+\n",
            "|    S001| 75000| Laptop|\n",
            "|    S002| 30000| Mobile|\n",
            "|    S003| 72000| Laptop|\n",
            "|    S004| 25000| Tablet|\n",
            "|    S005| 28000| Mobile|\n",
            "|    S006| 76000| Laptop|\n",
            "|    S007| 24000| Tablet|\n",
            "|    S008| 32000| Mobile|\n",
            "|    S009| 26000| Tablet|\n",
            "|    S010| 31000| Mobile|\n",
            "|    S011| 74000| Laptop|\n",
            "|    S012| 78000| Laptop|\n",
            "+--------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sales.select(col(\"sales_id\"),col(\"amount\").alias(\"sales\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Or-n67e8eFl-",
        "outputId": "6e8825af-533f-4bad-b189-2af34cecf9ec"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+\n",
            "|sales_id|sales|\n",
            "+--------+-----+\n",
            "|    S001|75000|\n",
            "|    S002|30000|\n",
            "|    S003|72000|\n",
            "|    S004|25000|\n",
            "|    S005|28000|\n",
            "|    S006|76000|\n",
            "|    S007|24000|\n",
            "|    S008|32000|\n",
            "|    S009|26000|\n",
            "|    S010|31000|\n",
            "|    S011|74000|\n",
            "|    S012|78000|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SET 1**"
      ],
      "metadata": {
        "id": "9Z5vDgOVe4Jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_data = [\n",
        "(\"T001\",\"North\",\"Delhi\",\"Store-01\",\"Laptop\",\"2024-01-01\",75000),\n",
        "(\"T002\",\"North\",\"Delhi\",\"Store-01\",\"Mobile\",\"2024-01-02\",32000),\n",
        "(\"T003\",\"North\",\"Chandigarh\",\"Store-02\",\"Tablet\",\"2024-01-03\",26000),\n",
        "(\"T004\",\"South\",\"Bangalore\",\"Store-03\",\"Laptop\",\"2024-01-01\",78000),\n",
        "(\"T005\",\"South\",\"Chennai\",\"Store-04\",\"Mobile\",\"2024-01-02\",30000),\n",
        "(\"T006\",\"South\",\"Bangalore\",\"Store-03\",\"Tablet\",\"2024-01-03\",24000),\n",
        "(\"T007\",\"East\",\"Kolkata\",\"Store-05\",\"Laptop\",\"2024-01-01\",72000),\n",
        "(\"T008\",\"East\",\"Kolkata\",\"Store-05\",\"Mobile\",\"2024-01-02\",28000),\n",
        "(\"T009\",\"East\",\"Patna\",\"Store-06\",\"Tablet\",\"2024-01-03\",23000),\n",
        "(\"T010\",\"West\",\"Mumbai\",\"Store-07\",\"Laptop\",\"2024-01-01\",80000),\n",
        "(\"T011\",\"West\",\"Mumbai\",\"Store-07\",\"Mobile\",\"2024-01-02\",35000),\n",
        "(\"T012\",\"West\",\"Pune\",\"Store-08\",\"Tablet\",\"2024-01-03\",27000),\n",
        "(\"T013\",\"North\",\"Delhi\",\"Store-01\",\"Laptop\",\"2024-01-04\",76000),\n",
        "(\"T014\",\"South\",\"Chennai\",\"Store-04\",\"Laptop\",\"2024-01-04\",79000),\n",
        "(\"T015\",\"East\",\"Patna\",\"Store-06\",\"Mobile\",\"2024-01-04\",29000),\n",
        "(\"T016\",\"West\",\"Pune\",\"Store-08\",\"Laptop\",\"2024-01-04\",77000),\n",
        "(\"T017\",\"North\",\"Chandigarh\",\"Store-02\",\"Mobile\",\"2024-01-05\",31000),\n",
        "(\"T018\",\"South\",\"Bangalore\",\"Store-03\",\"Mobile\",\"2024-01-05\",34000),\n",
        "(\"T019\",\"East\",\"Kolkata\",\"Store-05\",\"Tablet\",\"2024-01-05\",25000),\n",
        "(\"T020\",\"West\",\"Mumbai\",\"Store-07\",\"Tablet\",\"2024-01-05\",29000),\n",
        "(\"T021\",\"North\",\"Delhi\",\"Store-01\",\"Tablet\",\"2024-01-06\",28000),\n",
        "(\"T022\",\"South\",\"Chennai\",\"Store-04\",\"Tablet\",\"2024-01-06\",26000),\n",
        "(\"T023\",\"East\",\"Patna\",\"Store-06\",\"Laptop\",\"2024-01-06\",74000),\n",
        "(\"T024\",\"West\",\"Pune\",\"Store-08\",\"Mobile\",\"2024-01-06\",33000)\n",
        "]\n",
        "columns = [\n",
        "\"txn_id\",\"region\",\"city\",\"store_id\",\"product\",\"sale_date\",\"amount\"\n",
        "]\n",
        "df_sales = spark.createDataFrame(sales_data, columns)\n",
        "df_sales.show(5)\n",
        "df_sales.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qmQ1P6neQO2",
        "outputId": "b15c6697-7545-4b46-af1c-03767a2b41dc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+----------+--------+-------+----------+------+\n",
            "|txn_id|region|      city|store_id|product| sale_date|amount|\n",
            "+------+------+----------+--------+-------+----------+------+\n",
            "|  T001| North|     Delhi|Store-01| Laptop|2024-01-01| 75000|\n",
            "|  T002| North|     Delhi|Store-01| Mobile|2024-01-02| 32000|\n",
            "|  T003| North|Chandigarh|Store-02| Tablet|2024-01-03| 26000|\n",
            "|  T004| South| Bangalore|Store-03| Laptop|2024-01-01| 78000|\n",
            "|  T005| South|   Chennai|Store-04| Mobile|2024-01-02| 30000|\n",
            "+------+------+----------+--------+-------+----------+------+\n",
            "only showing top 5 rows\n",
            "root\n",
            " |-- txn_id: string (nullable = true)\n",
            " |-- region: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- store_id: string (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- sale_date: string (nullable = true)\n",
            " |-- amount: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Select only txn_id , region , product , and amount"
      ],
      "metadata": {
        "id": "UNvwnOnLfUjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mini_data = df_sales.select(\"txn_id\",\"region\",\"product\",\"amount\")\n",
        "mini_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRY2AeT8fOBf",
        "outputId": "11d5e3a7-37df-4e66-da14-acfd132f9e53"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+------+\n",
            "|txn_id|region|product|amount|\n",
            "+------+------+-------+------+\n",
            "|  T001| North| Laptop| 75000|\n",
            "|  T002| North| Mobile| 32000|\n",
            "|  T003| North| Tablet| 26000|\n",
            "|  T004| South| Laptop| 78000|\n",
            "|  T005| South| Mobile| 30000|\n",
            "|  T006| South| Tablet| 24000|\n",
            "|  T007|  East| Laptop| 72000|\n",
            "|  T008|  East| Mobile| 28000|\n",
            "|  T009|  East| Tablet| 23000|\n",
            "|  T010|  West| Laptop| 80000|\n",
            "|  T011|  West| Mobile| 35000|\n",
            "|  T012|  West| Tablet| 27000|\n",
            "|  T013| North| Laptop| 76000|\n",
            "|  T014| South| Laptop| 79000|\n",
            "|  T015|  East| Mobile| 29000|\n",
            "|  T016|  West| Laptop| 77000|\n",
            "|  T017| North| Mobile| 31000|\n",
            "|  T018| South| Mobile| 34000|\n",
            "|  T019|  East| Tablet| 25000|\n",
            "|  T020|  West| Tablet| 29000|\n",
            "+------+------+-------+------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Rename amount to revenue"
      ],
      "metadata": {
        "id": "nl806lAzfntv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mini_data = df_sales.select(\"txn_id\",\"region\",\"product\",col(\"amount\").alias(\"revenue\"))\n",
        "mini_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBoX5ktSfc4U",
        "outputId": "a1e94443-24e4-4037-8c1d-1d6c89a2f580"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+-------+\n",
            "|txn_id|region|product|revenue|\n",
            "+------+------+-------+-------+\n",
            "|  T001| North| Laptop|  75000|\n",
            "|  T002| North| Mobile|  32000|\n",
            "|  T003| North| Tablet|  26000|\n",
            "|  T004| South| Laptop|  78000|\n",
            "|  T005| South| Mobile|  30000|\n",
            "|  T006| South| Tablet|  24000|\n",
            "|  T007|  East| Laptop|  72000|\n",
            "|  T008|  East| Mobile|  28000|\n",
            "|  T009|  East| Tablet|  23000|\n",
            "|  T010|  West| Laptop|  80000|\n",
            "|  T011|  West| Mobile|  35000|\n",
            "|  T012|  West| Tablet|  27000|\n",
            "|  T013| North| Laptop|  76000|\n",
            "|  T014| South| Laptop|  79000|\n",
            "|  T015|  East| Mobile|  29000|\n",
            "|  T016|  West| Laptop|  77000|\n",
            "|  T017| North| Mobile|  31000|\n",
            "|  T018| South| Mobile|  34000|\n",
            "|  T019|  East| Tablet|  25000|\n",
            "|  T020|  West| Tablet|  29000|\n",
            "+------+------+-------+-------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create a derived column amount_in_thousands"
      ],
      "metadata": {
        "id": "yzLDNTCNfxyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "df = df_sales.withColumn('amount_in_thousands', (col('amount') / 1000).cast('int'))\n",
        "\n",
        "mini_data = df.select(\"txn_id\",\"region\",\"product\",col(\"amount_in_thousands\").alias(\"revenue (in thousand)\"))\n",
        "mini_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUlnIxh0fwdM",
        "outputId": "a03c1edd-0203-4e27-c5a1-5144eaa0a9da"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+---------------------+\n",
            "|txn_id|region|product|revenue (in thousand)|\n",
            "+------+------+-------+---------------------+\n",
            "|  T001| North| Laptop|                   75|\n",
            "|  T002| North| Mobile|                   32|\n",
            "|  T003| North| Tablet|                   26|\n",
            "|  T004| South| Laptop|                   78|\n",
            "|  T005| South| Mobile|                   30|\n",
            "|  T006| South| Tablet|                   24|\n",
            "|  T007|  East| Laptop|                   72|\n",
            "|  T008|  East| Mobile|                   28|\n",
            "|  T009|  East| Tablet|                   23|\n",
            "|  T010|  West| Laptop|                   80|\n",
            "|  T011|  West| Mobile|                   35|\n",
            "|  T012|  West| Tablet|                   27|\n",
            "|  T013| North| Laptop|                   76|\n",
            "|  T014| South| Laptop|                   79|\n",
            "|  T015|  East| Mobile|                   29|\n",
            "|  T016|  West| Laptop|                   77|\n",
            "|  T017| North| Mobile|                   31|\n",
            "|  T018| South| Mobile|                   34|\n",
            "|  T019|  East| Tablet|                   25|\n",
            "|  T020|  West| Tablet|                   29|\n",
            "+------+------+-------+---------------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Select distinct combinations of region and product"
      ],
      "metadata": {
        "id": "6py3hVw9g_qP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mini_data.groupBy(\"region\",\"product\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88pDuFMNghhr",
        "outputId": "322eb3d7-918a-4cd5-e66e-eb19865cee78"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+\n",
            "|region|product|count|\n",
            "+------+-------+-----+\n",
            "| North| Laptop|    2|\n",
            "| North| Tablet|    2|\n",
            "|  East| Tablet|    2|\n",
            "|  East| Laptop|    2|\n",
            "| South| Tablet|    2|\n",
            "| North| Mobile|    2|\n",
            "|  West| Tablet|    2|\n",
            "|  East| Mobile|    2|\n",
            "| South| Mobile|    2|\n",
            "| South| Laptop|    2|\n",
            "|  West| Mobile|    2|\n",
            "|  West| Laptop|    2|\n",
            "+------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Select all columns but exclude store_id"
      ],
      "metadata": {
        "id": "z6BIHP08hrhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dropping = mini_data.drop(col(\"store_id\"))\n",
        "dropping.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4XbdqV6hHm0",
        "outputId": "11ca665c-fcc2-4218-8d7f-12421fedd6e4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+---------------------+\n",
            "|txn_id|region|product|revenue (in thousand)|\n",
            "+------+------+-------+---------------------+\n",
            "|  T001| North| Laptop|                   75|\n",
            "|  T002| North| Mobile|                   32|\n",
            "|  T003| North| Tablet|                   26|\n",
            "|  T004| South| Laptop|                   78|\n",
            "|  T005| South| Mobile|                   30|\n",
            "|  T006| South| Tablet|                   24|\n",
            "|  T007|  East| Laptop|                   72|\n",
            "|  T008|  East| Mobile|                   28|\n",
            "|  T009|  East| Tablet|                   23|\n",
            "|  T010|  West| Laptop|                   80|\n",
            "|  T011|  West| Mobile|                   35|\n",
            "|  T012|  West| Tablet|                   27|\n",
            "|  T013| North| Laptop|                   76|\n",
            "|  T014| South| Laptop|                   79|\n",
            "|  T015|  East| Mobile|                   29|\n",
            "|  T016|  West| Laptop|                   77|\n",
            "|  T017| North| Mobile|                   31|\n",
            "|  T018| South| Mobile|                   34|\n",
            "|  T019|  East| Tablet|                   25|\n",
            "|  T020|  West| Tablet|                   29|\n",
            "+------+------+-------+---------------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Create a new column sale_year extracted from sale_date\n"
      ],
      "metadata": {
        "id": "SvH4GjBjizmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_sales.withColumn('year', substring(col(\"sale_date\"),0,4))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQwPEz30i21x",
        "outputId": "16caa40c-b799-48ff-e917-c96768c05b9b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+----------+--------+-------+----------+------+----+\n",
            "|txn_id|region|      city|store_id|product| sale_date|amount|year|\n",
            "+------+------+----------+--------+-------+----------+------+----+\n",
            "|  T001| North|     Delhi|Store-01| Laptop|2024-01-01| 75000|2024|\n",
            "|  T002| North|     Delhi|Store-01| Mobile|2024-01-02| 32000|2024|\n",
            "|  T003| North|Chandigarh|Store-02| Tablet|2024-01-03| 26000|2024|\n",
            "|  T004| South| Bangalore|Store-03| Laptop|2024-01-01| 78000|2024|\n",
            "|  T005| South|   Chennai|Store-04| Mobile|2024-01-02| 30000|2024|\n",
            "|  T006| South| Bangalore|Store-03| Tablet|2024-01-03| 24000|2024|\n",
            "|  T007|  East|   Kolkata|Store-05| Laptop|2024-01-01| 72000|2024|\n",
            "|  T008|  East|   Kolkata|Store-05| Mobile|2024-01-02| 28000|2024|\n",
            "|  T009|  East|     Patna|Store-06| Tablet|2024-01-03| 23000|2024|\n",
            "|  T010|  West|    Mumbai|Store-07| Laptop|2024-01-01| 80000|2024|\n",
            "|  T011|  West|    Mumbai|Store-07| Mobile|2024-01-02| 35000|2024|\n",
            "|  T012|  West|      Pune|Store-08| Tablet|2024-01-03| 27000|2024|\n",
            "|  T013| North|     Delhi|Store-01| Laptop|2024-01-04| 76000|2024|\n",
            "|  T014| South|   Chennai|Store-04| Laptop|2024-01-04| 79000|2024|\n",
            "|  T015|  East|     Patna|Store-06| Mobile|2024-01-04| 29000|2024|\n",
            "|  T016|  West|      Pune|Store-08| Laptop|2024-01-04| 77000|2024|\n",
            "|  T017| North|Chandigarh|Store-02| Mobile|2024-01-05| 31000|2024|\n",
            "|  T018| South| Bangalore|Store-03| Mobile|2024-01-05| 34000|2024|\n",
            "|  T019|  East|   Kolkata|Store-05| Tablet|2024-01-05| 25000|2024|\n",
            "|  T020|  West|    Mumbai|Store-07| Tablet|2024-01-05| 29000|2024|\n",
            "+------+------+----------+--------+-------+----------+------+----+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Reorder columns in a business-friendly format"
      ],
      "metadata": {
        "id": "thYISF5Pi1OA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_reordered = df.select(\"txn_id\", \"sale_date\", \"year\", \"region\", \"city\", \"store_id\", \"product\", \"amount\")\n",
        "df_reordered.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSKKBsZ9hk4n",
        "outputId": "3a869a5a-d2a8-49ae-c850-1080bf243ace"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+----+------+----------+--------+-------+------+\n",
            "|txn_id| sale_date|year|region|      city|store_id|product|amount|\n",
            "+------+----------+----+------+----------+--------+-------+------+\n",
            "|  T001|2024-01-01|2024| North|     Delhi|Store-01| Laptop| 75000|\n",
            "|  T002|2024-01-02|2024| North|     Delhi|Store-01| Mobile| 32000|\n",
            "|  T003|2024-01-03|2024| North|Chandigarh|Store-02| Tablet| 26000|\n",
            "|  T004|2024-01-01|2024| South| Bangalore|Store-03| Laptop| 78000|\n",
            "|  T005|2024-01-02|2024| South|   Chennai|Store-04| Mobile| 30000|\n",
            "|  T006|2024-01-03|2024| South| Bangalore|Store-03| Tablet| 24000|\n",
            "|  T007|2024-01-01|2024|  East|   Kolkata|Store-05| Laptop| 72000|\n",
            "|  T008|2024-01-02|2024|  East|   Kolkata|Store-05| Mobile| 28000|\n",
            "|  T009|2024-01-03|2024|  East|     Patna|Store-06| Tablet| 23000|\n",
            "|  T010|2024-01-01|2024|  West|    Mumbai|Store-07| Laptop| 80000|\n",
            "|  T011|2024-01-02|2024|  West|    Mumbai|Store-07| Mobile| 35000|\n",
            "|  T012|2024-01-03|2024|  West|      Pune|Store-08| Tablet| 27000|\n",
            "|  T013|2024-01-04|2024| North|     Delhi|Store-01| Laptop| 76000|\n",
            "|  T014|2024-01-04|2024| South|   Chennai|Store-04| Laptop| 79000|\n",
            "|  T015|2024-01-04|2024|  East|     Patna|Store-06| Mobile| 29000|\n",
            "|  T016|2024-01-04|2024|  West|      Pune|Store-08| Laptop| 77000|\n",
            "|  T017|2024-01-05|2024| North|Chandigarh|Store-02| Mobile| 31000|\n",
            "|  T018|2024-01-05|2024| South| Bangalore|Store-03| Mobile| 34000|\n",
            "|  T019|2024-01-05|2024|  East|   Kolkata|Store-05| Tablet| 25000|\n",
            "|  T020|2024-01-05|2024|  West|    Mumbai|Store-07| Tablet| 29000|\n",
            "+------+----------+----+------+----------+--------+-------+------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Filter transactions where amount > 50000"
      ],
      "metadata": {
        "id": "ssJ_Qs5NlfkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_reordered.filter(df_reordered.amount > 50000).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_2-xYvclz07",
        "outputId": "80430d4c-d5fb-4fd1-cde4-3a5abb4f0257"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+----+------+---------+--------+-------+------+\n",
            "|txn_id| sale_date|year|region|     city|store_id|product|amount|\n",
            "+------+----------+----+------+---------+--------+-------+------+\n",
            "|  T001|2024-01-01|2024| North|    Delhi|Store-01| Laptop| 75000|\n",
            "|  T004|2024-01-01|2024| South|Bangalore|Store-03| Laptop| 78000|\n",
            "|  T007|2024-01-01|2024|  East|  Kolkata|Store-05| Laptop| 72000|\n",
            "|  T010|2024-01-01|2024|  West|   Mumbai|Store-07| Laptop| 80000|\n",
            "|  T013|2024-01-04|2024| North|    Delhi|Store-01| Laptop| 76000|\n",
            "|  T014|2024-01-04|2024| South|  Chennai|Store-04| Laptop| 79000|\n",
            "|  T016|2024-01-04|2024|  West|     Pune|Store-08| Laptop| 77000|\n",
            "|  T023|2024-01-06|2024|  East|    Patna|Store-06| Laptop| 74000|\n",
            "+------+----------+----+------+---------+--------+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Filter only Laptop sales"
      ],
      "metadata": {
        "id": "GMSwFY0blkT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_reordered.filter(df_reordered.product == \"Laptop\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvRImZ5Wl_mU",
        "outputId": "e37348ad-e3b6-4dab-9e4b-f2e027bad2ae"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+----+------+---------+--------+-------+------+\n",
            "|txn_id| sale_date|year|region|     city|store_id|product|amount|\n",
            "+------+----------+----+------+---------+--------+-------+------+\n",
            "|  T001|2024-01-01|2024| North|    Delhi|Store-01| Laptop| 75000|\n",
            "|  T004|2024-01-01|2024| South|Bangalore|Store-03| Laptop| 78000|\n",
            "|  T007|2024-01-01|2024|  East|  Kolkata|Store-05| Laptop| 72000|\n",
            "|  T010|2024-01-01|2024|  West|   Mumbai|Store-07| Laptop| 80000|\n",
            "|  T013|2024-01-04|2024| North|    Delhi|Store-01| Laptop| 76000|\n",
            "|  T014|2024-01-04|2024| South|  Chennai|Store-04| Laptop| 79000|\n",
            "|  T016|2024-01-04|2024|  West|     Pune|Store-08| Laptop| 77000|\n",
            "|  T023|2024-01-06|2024|  East|    Patna|Store-06| Laptop| 74000|\n",
            "+------+----------+----+------+---------+--------+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Filter sales from North and South regions\n"
      ],
      "metadata": {
        "id": "rft662chlnuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_reordered.filter((df_reordered.region == \"North\") | (df_reordered.region == \"South\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTsZB1wcmHek",
        "outputId": "eb42cf99-ef25-4e1d-ea9f-2c21e27c23f4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+----+------+----------+--------+-------+------+\n",
            "|txn_id| sale_date|year|region|      city|store_id|product|amount|\n",
            "+------+----------+----+------+----------+--------+-------+------+\n",
            "|  T001|2024-01-01|2024| North|     Delhi|Store-01| Laptop| 75000|\n",
            "|  T002|2024-01-02|2024| North|     Delhi|Store-01| Mobile| 32000|\n",
            "|  T003|2024-01-03|2024| North|Chandigarh|Store-02| Tablet| 26000|\n",
            "|  T004|2024-01-01|2024| South| Bangalore|Store-03| Laptop| 78000|\n",
            "|  T005|2024-01-02|2024| South|   Chennai|Store-04| Mobile| 30000|\n",
            "|  T006|2024-01-03|2024| South| Bangalore|Store-03| Tablet| 24000|\n",
            "|  T013|2024-01-04|2024| North|     Delhi|Store-01| Laptop| 76000|\n",
            "|  T014|2024-01-04|2024| South|   Chennai|Store-04| Laptop| 79000|\n",
            "|  T017|2024-01-05|2024| North|Chandigarh|Store-02| Mobile| 31000|\n",
            "|  T018|2024-01-05|2024| South| Bangalore|Store-03| Mobile| 34000|\n",
            "|  T021|2024-01-06|2024| North|     Delhi|Store-01| Tablet| 28000|\n",
            "|  T022|2024-01-06|2024| South|   Chennai|Store-04| Tablet| 26000|\n",
            "+------+----------+----+------+----------+--------+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Filter sales between 25000 and 75000\n"
      ],
      "metadata": {
        "id": "HYUjTq2XlryD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_reordered.filter((df_reordered.amount > 25000) & (df_reordered.amount < 75000)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKEgzORgmXse",
        "outputId": "e3e8b85b-dd5a-4a53-b422-b3d744d5e129"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+----+------+----------+--------+-------+------+\n",
            "|txn_id| sale_date|year|region|      city|store_id|product|amount|\n",
            "+------+----------+----+------+----------+--------+-------+------+\n",
            "|  T002|2024-01-02|2024| North|     Delhi|Store-01| Mobile| 32000|\n",
            "|  T003|2024-01-03|2024| North|Chandigarh|Store-02| Tablet| 26000|\n",
            "|  T005|2024-01-02|2024| South|   Chennai|Store-04| Mobile| 30000|\n",
            "|  T007|2024-01-01|2024|  East|   Kolkata|Store-05| Laptop| 72000|\n",
            "|  T008|2024-01-02|2024|  East|   Kolkata|Store-05| Mobile| 28000|\n",
            "|  T011|2024-01-02|2024|  West|    Mumbai|Store-07| Mobile| 35000|\n",
            "|  T012|2024-01-03|2024|  West|      Pune|Store-08| Tablet| 27000|\n",
            "|  T015|2024-01-04|2024|  East|     Patna|Store-06| Mobile| 29000|\n",
            "|  T017|2024-01-05|2024| North|Chandigarh|Store-02| Mobile| 31000|\n",
            "|  T018|2024-01-05|2024| South| Bangalore|Store-03| Mobile| 34000|\n",
            "|  T020|2024-01-05|2024|  West|    Mumbai|Store-07| Tablet| 29000|\n",
            "|  T021|2024-01-06|2024| North|     Delhi|Store-01| Tablet| 28000|\n",
            "|  T022|2024-01-06|2024| South|   Chennai|Store-04| Tablet| 26000|\n",
            "|  T023|2024-01-06|2024|  East|     Patna|Store-06| Laptop| 74000|\n",
            "|  T024|2024-01-06|2024|  West|      Pune|Store-08| Mobile| 33000|\n",
            "+------+----------+----+------+----------+--------+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Filter transactions from Delhi stores only\n"
      ],
      "metadata": {
        "id": "Q6rsUlZKltab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_reordered.filter(df_reordered.city == \"Delhi\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b30dbed5-8d57-48a5-f350-db385ec520fb",
        "id": "pmFSskL8mwLV"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+----+------+-----+--------+-------+------+\n",
            "|txn_id| sale_date|year|region| city|store_id|product|amount|\n",
            "+------+----------+----+------+-----+--------+-------+------+\n",
            "|  T001|2024-01-01|2024| North|Delhi|Store-01| Laptop| 75000|\n",
            "|  T002|2024-01-02|2024| North|Delhi|Store-01| Mobile| 32000|\n",
            "|  T013|2024-01-04|2024| North|Delhi|Store-01| Laptop| 76000|\n",
            "|  T021|2024-01-06|2024| North|Delhi|Store-01| Tablet| 28000|\n",
            "+------+----------+----+------+-----+--------+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Apply multiple filters using both filter and where"
      ],
      "metadata": {
        "id": "mefvgTYmluny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_reordered.filter(df_reordered.product == \"Laptop\").where(df_reordered.region == \"North\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVf0nKmXmZTX",
        "outputId": "59197376-bd23-4689-d77a-84754fe26ce9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+----+------+-----+--------+-------+------+\n",
            "|txn_id| sale_date|year|region| city|store_id|product|amount|\n",
            "+------+----------+----+------+-----+--------+-------+------+\n",
            "|  T001|2024-01-01|2024| North|Delhi|Store-01| Laptop| 75000|\n",
            "|  T013|2024-01-04|2024| North|Delhi|Store-01| Laptop| 76000|\n",
            "+------+----------+----+------+-----+--------+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Change the order of filters and compare explain(True)\n",
        "\n"
      ],
      "metadata": {
        "id": "7pabdDTBlv2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Filter Product then Region:\")\n",
        "df_reordered.filter(df_reordered.product == \"Laptop\").filter(df_reordered.region == \"North\").explain(True)\n",
        "\n",
        "print(\"\\nFilter Region then Product:\")\n",
        "df_reordered.filter(df_reordered.region == \"North\").filter(df_reordered.product == \"Laptop\").explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPEHEFSXmZoW",
        "outputId": "dd151837-97f7-4980-b8a0-0704664a340b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filter Product then Region:\n",
            "== Parsed Logical Plan ==\n",
            "'Filter '`=`(region#40, North)\n",
            "+- Filter (product#43 = Laptop)\n",
            "   +- Project [txn_id#39, sale_date#44, year#201, region#40, city#41, store_id#42, product#43, amount#45L]\n",
            "      +- Project [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L, substring(sale_date#44, 0, 4) AS year#201]\n",
            "         +- LogicalRDD [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "txn_id: string, sale_date: string, year: string, region: string, city: string, store_id: string, product: string, amount: bigint\n",
            "Filter (region#40 = North)\n",
            "+- Filter (product#43 = Laptop)\n",
            "   +- Project [txn_id#39, sale_date#44, year#201, region#40, city#41, store_id#42, product#43, amount#45L]\n",
            "      +- Project [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L, substring(sale_date#44, 0, 4) AS year#201]\n",
            "         +- LogicalRDD [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [txn_id#39, sale_date#44, substring(sale_date#44, 0, 4) AS year#201, region#40, city#41, store_id#42, product#43, amount#45L]\n",
            "+- Filter ((isnotnull(product#43) AND isnotnull(region#40)) AND ((product#43 = Laptop) AND (region#40 = North)))\n",
            "   +- LogicalRDD [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L], false\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [txn_id#39, sale_date#44, substring(sale_date#44, 0, 4) AS year#201, region#40, city#41, store_id#42, product#43, amount#45L]\n",
            "+- *(1) Filter ((isnotnull(product#43) AND isnotnull(region#40)) AND ((product#43 = Laptop) AND (region#40 = North)))\n",
            "   +- *(1) Scan ExistingRDD[txn_id#39,region#40,city#41,store_id#42,product#43,sale_date#44,amount#45L]\n",
            "\n",
            "\n",
            "Filter Region then Product:\n",
            "== Parsed Logical Plan ==\n",
            "'Filter '`=`(product#43, Laptop)\n",
            "+- Filter (region#40 = North)\n",
            "   +- Project [txn_id#39, sale_date#44, year#201, region#40, city#41, store_id#42, product#43, amount#45L]\n",
            "      +- Project [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L, substring(sale_date#44, 0, 4) AS year#201]\n",
            "         +- LogicalRDD [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "txn_id: string, sale_date: string, year: string, region: string, city: string, store_id: string, product: string, amount: bigint\n",
            "Filter (product#43 = Laptop)\n",
            "+- Filter (region#40 = North)\n",
            "   +- Project [txn_id#39, sale_date#44, year#201, region#40, city#41, store_id#42, product#43, amount#45L]\n",
            "      +- Project [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L, substring(sale_date#44, 0, 4) AS year#201]\n",
            "         +- LogicalRDD [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [txn_id#39, sale_date#44, substring(sale_date#44, 0, 4) AS year#201, region#40, city#41, store_id#42, product#43, amount#45L]\n",
            "+- Filter ((isnotnull(region#40) AND isnotnull(product#43)) AND ((region#40 = North) AND (product#43 = Laptop)))\n",
            "   +- LogicalRDD [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L], false\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [txn_id#39, sale_date#44, substring(sale_date#44, 0, 4) AS year#201, region#40, city#41, store_id#42, product#43, amount#45L]\n",
            "+- *(1) Filter ((isnotnull(region#40) AND isnotnull(product#43)) AND ((region#40 = North) AND (product#43 = Laptop)))\n",
            "   +- *(1) Scan ExistingRDD[txn_id#39,region#40,city#41,store_id#42,product#43,sale_date#44,amount#45L]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Identify which filters Spark pushes down"
      ],
      "metadata": {
        "id": "Vh7gbwmtlxWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# As observed in the 'Optimized Logical Plan' and 'Physical Plan' sections of the previous explain(True) output,\n",
        "# Spark pushed down both filter conditions together.\n",
        "# The combined filter shown was: `((isnotnull(product#X) AND isnotnull(region#Y)) AND ((product#X = Laptop) AND (region#Y = North)))`.\n",
        "# This indicates that Spark's Catalyst Optimizer combined the two separate filter calls into a single, efficient filter operation\n",
        "# that is applied directly when reading the data from the RDD (Scan ExistingRDD)."
      ],
      "metadata": {
        "id": "vIw4NjOKlmqP"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Total sales amount per region\n"
      ],
      "metadata": {
        "id": "LBQy7L3ptlQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum\n",
        "\n",
        "df_sales.groupBy(\"region\").agg(sum(\"amount\").alias(\"total_sales_amount\")).show()"
      ],
      "metadata": {
        "id": "NjRrPNMPvYw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19fefee1"
      },
      "source": [
        "### Total sales amount per region using `reduceByKey`\n",
        "\n",
        "To use `reduceByKey`, we first convert the DataFrame to an RDD and then transform the data into key-value pairs (region, amount) before applying the reduction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "396777ee",
        "outputId": "a0e2bed0-726b-4a7a-b619-8f81acaf89ca"
      },
      "source": [
        "sales_rdd = df_sales.rdd.map(lambda row: (row['region'], row['amount']))\n",
        "\n",
        "total_sales_by_region_rdd = sales_rdd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "total_sales_by_region_rdd.collect()\n",
        "\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('North', 268000), ('West', 281000), ('South', 271000), ('East', 251000)]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum\n",
        "\n",
        "df_sales.groupBy(\"region\").agg(sum(\"amount\").alias(\"total_sales_amount\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XIc2UhCt_Fw",
        "outputId": "0c83e6dc-2885-4a3b-b474-f889aa288f93"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------------+\n",
            "|region|total_sales_amount|\n",
            "+------+------------------+\n",
            "| South|            271000|\n",
            "|  East|            251000|\n",
            "|  West|            281000|\n",
            "| North|            268000|\n",
            "+------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Average sales amount per product\n"
      ],
      "metadata": {
        "id": "m2ri-EtetnjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum\n",
        "\n",
        "df_sales.groupBy(\"product\").agg(sum(\"amount\").alias(\"total_sales_per_product\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egfVZdsFxtoC",
        "outputId": "4b1b9b1b-da51-4af1-d987-7fe7e46083e4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------------+\n",
            "|product|total_sales_per_product|\n",
            "+-------+-----------------------+\n",
            "| Laptop|                 611000|\n",
            "| Mobile|                 252000|\n",
            "| Tablet|                 208000|\n",
            "+-------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Maximum sale per city\n"
      ],
      "metadata": {
        "id": "j5xYWXCBtpfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "highest = sales_rdd.reduce(\n",
        "    lambda x,y: x if x[1] > y[1] else y\n",
        ")\n",
        "highest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80cdACydyeI-",
        "outputId": "49e04052-c928-4736-b795-dccf63961b61"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Mumbai', 80000)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Minimum sale per store\n"
      ],
      "metadata": {
        "id": "KQSAhJFPtrsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_rdd = df_sales.rdd.map(lambda row: (row['city'], row['amount']))\n",
        "lowest = sales_rdd.reduce(\n",
        "    lambda x,y: x if x[1] < y[1] else y\n",
        ")\n",
        "lowest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GI0mZbqQziOE",
        "outputId": "29c238ce-ab31-4e87-81d2-dfa4b8a0b245"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Patna', 23000)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Count of transactions per region\n"
      ],
      "metadata": {
        "id": "WDDme_dNtucC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sales.groupBy(\"region\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOFxsqbd0hxb",
        "outputId": "fb82882f-8335-4811-b7ef-d0578f8b4d71"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|region|count|\n",
            "+------+-----+\n",
            "| South|    6|\n",
            "|  East|    6|\n",
            "|  West|    6|\n",
            "| North|    6|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Total revenue per store\n"
      ],
      "metadata": {
        "id": "LX4QYGOWtwEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sales.groupBy(\"city\").sum().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjAhG3Qi0uw0",
        "outputId": "efb1a139-d981-467f-b277-e7b57108d742"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|      city|sum(amount)|\n",
            "+----------+-----------+\n",
            "| Bangalore|     136000|\n",
            "|     Patna|     126000|\n",
            "|   Chennai|     135000|\n",
            "|    Mumbai|     144000|\n",
            "|   Kolkata|     125000|\n",
            "|      Pune|     137000|\n",
            "|     Delhi|     211000|\n",
            "|Chandigarh|      57000|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Region-wise product sales count\n",
        "\n"
      ],
      "metadata": {
        "id": "DM6JYTOCty_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_rdd = df_sales.rdd.map(lambda row: (row['product'],row['region'], row['amount']))\n",
        "df_sales.groupBy(\"product\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcRk3dIl0vGU",
        "outputId": "a89c6653-fb5b-4db2-f44c-c858fa9d9792"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|product|count|\n",
            "+-------+-----+\n",
            "| Laptop|    8|\n",
            "| Mobile|    8|\n",
            "| Tablet|    8|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Average transaction value per city"
      ],
      "metadata": {
        "id": "n1GSy40Pt2Qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg, round\n",
        "\n",
        "df_sales.groupBy(\"city\").agg(round(avg(\"amount\"), 2)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5o_HoJc0vw0",
        "outputId": "7f1af3f2-3bdc-4d12-96df-29ceb784e9f7"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------------------+\n",
            "|      city|round(avg(amount), 2)|\n",
            "+----------+---------------------+\n",
            "| Bangalore|             45333.33|\n",
            "|     Patna|              42000.0|\n",
            "|   Chennai|              45000.0|\n",
            "|    Mumbai|              48000.0|\n",
            "|   Kolkata|             41666.67|\n",
            "|      Pune|             45666.67|\n",
            "|     Delhi|              52750.0|\n",
            "|Chandigarh|              28500.0|\n",
            "+----------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Identify regions with total sales above a threshold[link text]"
      ],
      "metadata": {
        "id": "yjCwwl4jt39y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum\n",
        "\n",
        "sales_threshold = 270000\n",
        "\n",
        "df_sales.groupBy(\"region\").agg(sum(\"amount\").alias(\"total_sales\")) \\\n",
        "    .filter(col(\"total_sales\") > sales_threshold).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9B9h1it0wbr",
        "outputId": "ef1ea8e1-b33c-4ee5-e312-0b2091498088"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+\n",
            "|region|total_sales|\n",
            "+------+-----------+\n",
            "| South|     271000|\n",
            "|  West|     281000|\n",
            "+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Use explain(True) and identify shuffle stages\n"
      ],
      "metadata": {
        "id": "FC6JWW3et8FO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum\n",
        "\n",
        "df_sales.groupBy(\"region\").agg(sum(\"amount\").alias(\"total_sales_amount\")).explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lt0s2LYVtbIT",
        "outputId": "a7eff829-6904-4a57-8922-a14ca119265b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Aggregate ['region], ['region, 'sum('amount) AS total_sales_amount#635]\n",
            "+- LogicalRDD [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "region: string, total_sales_amount: bigint\n",
            "Aggregate [region#40], [region#40, sum(amount#45L) AS total_sales_amount#635L]\n",
            "+- LogicalRDD [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Aggregate [region#40], [region#40, sum(amount#45L) AS total_sales_amount#635L]\n",
            "+- Project [region#40, amount#45L]\n",
            "   +- LogicalRDD [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[region#40], functions=[sum(amount#45L)], output=[region#40, total_sales_amount#635L])\n",
            "   +- Exchange hashpartitioning(region#40, 200), ENSURE_REQUIREMENTS, [plan_id=852]\n",
            "      +- HashAggregate(keys=[region#40], functions=[partial_sum(amount#45L)], output=[region#40, sum#645L])\n",
            "         +- Project [region#40, amount#45L]\n",
            "            +- Scan ExistingRDD[txn_id#39,region#40,city#41,store_id#42,product#43,sale_date#44,amount#45L]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Region + Product wise total sales\n"
      ],
      "metadata": {
        "id": "StN5_kTq6iSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sales.groupBy(\"region\",\"product\").agg(sum(\"amount\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDQahuDP6yDg",
        "outputId": "1737d302-8347-49ef-d04f-1aa5e3fd7126"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----------+\n",
            "|region|product|sum(amount)|\n",
            "+------+-------+-----------+\n",
            "| North| Laptop|     151000|\n",
            "| North| Tablet|      54000|\n",
            "|  East| Tablet|      48000|\n",
            "|  East| Laptop|     146000|\n",
            "| South| Tablet|      50000|\n",
            "| North| Mobile|      63000|\n",
            "|  West| Tablet|      56000|\n",
            "|  East| Mobile|      57000|\n",
            "| South| Mobile|      64000|\n",
            "| South| Laptop|     157000|\n",
            "|  West| Mobile|      68000|\n",
            "|  West| Laptop|     157000|\n",
            "+------+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. City + Store wise average sales\n"
      ],
      "metadata": {
        "id": "2TXMD-cS6kAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sales.groupBy(\"city\",\"store_id\").agg(avg(\"amount\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctftkSfY67tv",
        "outputId": "a8395de8-1899-4ecf-dd61-7ec95b6bb4f5"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+------------------+\n",
            "|      city|store_id|       avg(amount)|\n",
            "+----------+--------+------------------+\n",
            "| Bangalore|Store-03|45333.333333333336|\n",
            "|     Patna|Store-06|           42000.0|\n",
            "|   Chennai|Store-04|           45000.0|\n",
            "|      Pune|Store-08|45666.666666666664|\n",
            "|Chandigarh|Store-02|           28500.0|\n",
            "|   Kolkata|Store-05|41666.666666666664|\n",
            "|    Mumbai|Store-07|           48000.0|\n",
            "|     Delhi|Store-01|           52750.0|\n",
            "+----------+--------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Region + City wise transaction count\n"
      ],
      "metadata": {
        "id": "UosqYeV86lvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sales.groupBy(\"region\",\"city\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2QtID8H7HqI",
        "outputId": "91c4cccd-bd48-4124-f25d-d0bfd37c967e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+-----+\n",
            "|region|      city|count|\n",
            "+------+----------+-----+\n",
            "|  West|    Mumbai|    3|\n",
            "| South| Bangalore|    3|\n",
            "| North|     Delhi|    4|\n",
            "| North|Chandigarh|    2|\n",
            "| South|   Chennai|    3|\n",
            "|  West|      Pune|    3|\n",
            "|  East|   Kolkata|    3|\n",
            "|  East|     Patna|    3|\n",
            "+------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Product + Store wise max sale\n"
      ],
      "metadata": {
        "id": "6SdzTfs96uy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sales.groupBy(\"region\",\"city\").max().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efu6SQCB7MPS",
        "outputId": "448c48a1-6b29-4908-9677-4c0d31db4d84"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+-----------+\n",
            "|region|      city|max(amount)|\n",
            "+------+----------+-----------+\n",
            "|  West|    Mumbai|      80000|\n",
            "| South| Bangalore|      78000|\n",
            "| North|     Delhi|      76000|\n",
            "| North|Chandigarh|      31000|\n",
            "| South|   Chennai|      79000|\n",
            "|  West|      Pune|      77000|\n",
            "|  East|   Kolkata|      72000|\n",
            "|  East|     Patna|      74000|\n",
            "+------+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Identify top-selling product per region using aggregation only"
      ],
      "metadata": {
        "id": "2nWvfgyb6ww3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as f\n",
        "df_c=df_sales.groupBy(\"region\",\"product\").agg(f.sum(\"amount\").alias(\"max_total\"))\n",
        "df_c.show()\n",
        "\n",
        "df_top = (\n",
        "    df_c.groupBy(\"region\").agg(f.max(f.struct(\"max_total\", \"product\"))\n",
        "))\n",
        "df_top.show()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3liqQ-Lt7REH",
        "outputId": "f3214e64-7c04-499d-dde6-81568f0be2ed"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+---------+\n",
            "|region|product|max_total|\n",
            "+------+-------+---------+\n",
            "| North| Laptop|   151000|\n",
            "| North| Tablet|    54000|\n",
            "|  East| Tablet|    48000|\n",
            "|  East| Laptop|   146000|\n",
            "| South| Tablet|    50000|\n",
            "| North| Mobile|    63000|\n",
            "|  West| Tablet|    56000|\n",
            "|  East| Mobile|    57000|\n",
            "| South| Mobile|    64000|\n",
            "| South| Laptop|   157000|\n",
            "|  West| Mobile|    68000|\n",
            "|  West| Laptop|   157000|\n",
            "+------+-------+---------+\n",
            "\n",
            "+------+-------------------------------+\n",
            "|region|max(struct(max_total, product))|\n",
            "+------+-------------------------------+\n",
            "|  East|               {146000, Laptop}|\n",
            "| North|               {151000, Laptop}|\n",
            "| South|               {157000, Laptop}|\n",
            "|  West|               {157000, Laptop}|\n",
            "+------+-------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import sum, rank, row_number, dense_rank"
      ],
      "metadata": {
        "id": "6K49E0sfAmBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import sum, rank, row_number, dense_rank"
      ],
      "metadata": {
        "id": "wI839ZDOAlCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import sum, rank, row_number, dense_rank"
      ],
      "metadata": {
        "id": "EgcL59Ve6rOU"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Compute running total of sales per region ordered by date\n"
      ],
      "metadata": {
        "id": "ccKTa7Nt7pGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import sum, col\n",
        "\n",
        "window_spec = Window.partitionBy(\"region\").orderBy(\"sale_date\")\n",
        "df_sales.withColumn(\"running_total_sales\", sum(\"amount\").over(window_spec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcYYWTs_72Em",
        "outputId": "4a655ba4-c3ec-41dd-d9bc-e02418844283"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+----------+--------+-------+----------+------+-------------------+\n",
            "|txn_id|region|      city|store_id|product| sale_date|amount|running_total_sales|\n",
            "+------+------+----------+--------+-------+----------+------+-------------------+\n",
            "|  T007|  East|   Kolkata|Store-05| Laptop|2024-01-01| 72000|              72000|\n",
            "|  T008|  East|   Kolkata|Store-05| Mobile|2024-01-02| 28000|             100000|\n",
            "|  T009|  East|     Patna|Store-06| Tablet|2024-01-03| 23000|             123000|\n",
            "|  T015|  East|     Patna|Store-06| Mobile|2024-01-04| 29000|             152000|\n",
            "|  T019|  East|   Kolkata|Store-05| Tablet|2024-01-05| 25000|             177000|\n",
            "|  T023|  East|     Patna|Store-06| Laptop|2024-01-06| 74000|             251000|\n",
            "|  T001| North|     Delhi|Store-01| Laptop|2024-01-01| 75000|              75000|\n",
            "|  T002| North|     Delhi|Store-01| Mobile|2024-01-02| 32000|             107000|\n",
            "|  T003| North|Chandigarh|Store-02| Tablet|2024-01-03| 26000|             133000|\n",
            "|  T013| North|     Delhi|Store-01| Laptop|2024-01-04| 76000|             209000|\n",
            "|  T017| North|Chandigarh|Store-02| Mobile|2024-01-05| 31000|             240000|\n",
            "|  T021| North|     Delhi|Store-01| Tablet|2024-01-06| 28000|             268000|\n",
            "|  T004| South| Bangalore|Store-03| Laptop|2024-01-01| 78000|              78000|\n",
            "|  T005| South|   Chennai|Store-04| Mobile|2024-01-02| 30000|             108000|\n",
            "|  T006| South| Bangalore|Store-03| Tablet|2024-01-03| 24000|             132000|\n",
            "|  T014| South|   Chennai|Store-04| Laptop|2024-01-04| 79000|             211000|\n",
            "|  T018| South| Bangalore|Store-03| Mobile|2024-01-05| 34000|             245000|\n",
            "|  T022| South|   Chennai|Store-04| Tablet|2024-01-06| 26000|             271000|\n",
            "|  T010|  West|    Mumbai|Store-07| Laptop|2024-01-01| 80000|              80000|\n",
            "|  T011|  West|    Mumbai|Store-07| Mobile|2024-01-02| 35000|             115000|\n",
            "+------+------+----------+--------+-------+----------+------+-------------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Rank transactions by amount within each region\n"
      ],
      "metadata": {
        "id": "fHBiBnNJ7qyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rank\n",
        "window_spec = Window.partitionBy(\"region\").orderBy(\"amount\")\n",
        "ranked_df = df.withColumn(\"Rank\", rank().over(window_spec))\n",
        "ranked_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca3b7a49-c140-43ba-d5e7-ab6918ac3c39",
        "id": "gvH5rmrlBLZA"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+----------+--------+-------+----------+------+----+----+\n",
            "|txn_id|region|      city|store_id|product| sale_date|amount|year|Rank|\n",
            "+------+------+----------+--------+-------+----------+------+----+----+\n",
            "|  T009|  East|     Patna|Store-06| Tablet|2024-01-03| 23000|2024|   1|\n",
            "|  T019|  East|   Kolkata|Store-05| Tablet|2024-01-05| 25000|2024|   2|\n",
            "|  T008|  East|   Kolkata|Store-05| Mobile|2024-01-02| 28000|2024|   3|\n",
            "|  T015|  East|     Patna|Store-06| Mobile|2024-01-04| 29000|2024|   4|\n",
            "|  T007|  East|   Kolkata|Store-05| Laptop|2024-01-01| 72000|2024|   5|\n",
            "|  T023|  East|     Patna|Store-06| Laptop|2024-01-06| 74000|2024|   6|\n",
            "|  T003| North|Chandigarh|Store-02| Tablet|2024-01-03| 26000|2024|   1|\n",
            "|  T021| North|     Delhi|Store-01| Tablet|2024-01-06| 28000|2024|   2|\n",
            "|  T017| North|Chandigarh|Store-02| Mobile|2024-01-05| 31000|2024|   3|\n",
            "|  T002| North|     Delhi|Store-01| Mobile|2024-01-02| 32000|2024|   4|\n",
            "|  T001| North|     Delhi|Store-01| Laptop|2024-01-01| 75000|2024|   5|\n",
            "|  T013| North|     Delhi|Store-01| Laptop|2024-01-04| 76000|2024|   6|\n",
            "|  T006| South| Bangalore|Store-03| Tablet|2024-01-03| 24000|2024|   1|\n",
            "|  T022| South|   Chennai|Store-04| Tablet|2024-01-06| 26000|2024|   2|\n",
            "|  T005| South|   Chennai|Store-04| Mobile|2024-01-02| 30000|2024|   3|\n",
            "|  T018| South| Bangalore|Store-03| Mobile|2024-01-05| 34000|2024|   4|\n",
            "|  T004| South| Bangalore|Store-03| Laptop|2024-01-01| 78000|2024|   5|\n",
            "|  T014| South|   Chennai|Store-04| Laptop|2024-01-04| 79000|2024|   6|\n",
            "|  T012|  West|      Pune|Store-08| Tablet|2024-01-03| 27000|2024|   1|\n",
            "|  T020|  West|    Mumbai|Store-07| Tablet|2024-01-05| 29000|2024|   2|\n",
            "+------+------+----------+--------+-------+----------+------+----+----+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Assign row numbers per store ordered by sale amount\n"
      ],
      "metadata": {
        "id": "4ykUNz7c7sO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_spec = Window.partitionBy(\"store_id\").orderBy(\"amount\")\n",
        "ranked_df = df.withColumn(\"Rank\", rank().over(window_spec))\n",
        "ranked_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwwpuGGU9Etw",
        "outputId": "687615f6-4656-4ca9-957b-d70fbbf6d1fe"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+----------+--------+-------+----------+------+----+----+\n",
            "|txn_id|region|      city|store_id|product| sale_date|amount|year|Rank|\n",
            "+------+------+----------+--------+-------+----------+------+----+----+\n",
            "|  T021| North|     Delhi|Store-01| Tablet|2024-01-06| 28000|2024|   1|\n",
            "|  T002| North|     Delhi|Store-01| Mobile|2024-01-02| 32000|2024|   2|\n",
            "|  T001| North|     Delhi|Store-01| Laptop|2024-01-01| 75000|2024|   3|\n",
            "|  T013| North|     Delhi|Store-01| Laptop|2024-01-04| 76000|2024|   4|\n",
            "|  T003| North|Chandigarh|Store-02| Tablet|2024-01-03| 26000|2024|   1|\n",
            "|  T017| North|Chandigarh|Store-02| Mobile|2024-01-05| 31000|2024|   2|\n",
            "|  T006| South| Bangalore|Store-03| Tablet|2024-01-03| 24000|2024|   1|\n",
            "|  T018| South| Bangalore|Store-03| Mobile|2024-01-05| 34000|2024|   2|\n",
            "|  T004| South| Bangalore|Store-03| Laptop|2024-01-01| 78000|2024|   3|\n",
            "|  T022| South|   Chennai|Store-04| Tablet|2024-01-06| 26000|2024|   1|\n",
            "|  T005| South|   Chennai|Store-04| Mobile|2024-01-02| 30000|2024|   2|\n",
            "|  T014| South|   Chennai|Store-04| Laptop|2024-01-04| 79000|2024|   3|\n",
            "|  T019|  East|   Kolkata|Store-05| Tablet|2024-01-05| 25000|2024|   1|\n",
            "|  T008|  East|   Kolkata|Store-05| Mobile|2024-01-02| 28000|2024|   2|\n",
            "|  T007|  East|   Kolkata|Store-05| Laptop|2024-01-01| 72000|2024|   3|\n",
            "|  T009|  East|     Patna|Store-06| Tablet|2024-01-03| 23000|2024|   1|\n",
            "|  T015|  East|     Patna|Store-06| Mobile|2024-01-04| 29000|2024|   2|\n",
            "|  T023|  East|     Patna|Store-06| Laptop|2024-01-06| 74000|2024|   3|\n",
            "|  T020|  West|    Mumbai|Store-07| Tablet|2024-01-05| 29000|2024|   1|\n",
            "|  T011|  West|    Mumbai|Store-07| Mobile|2024-01-02| 35000|2024|   2|\n",
            "+------+------+----------+--------+-------+----------+------+----+----+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Use dense rank to rank products per region\n"
      ],
      "metadata": {
        "id": "kMkbIe7F7uHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import dense_rank\n",
        "window_spec = Window.partitionBy(\"region\").orderBy(col(\"product\").desc())\n",
        "\n",
        "# Apply dense_rank\n",
        "ranked_df = df.withColumn(\"dense_rank\", dense_rank().over(window_spec))\n",
        "ranked_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIdGrODa9Jkk",
        "outputId": "16bbf174-2811-41d0-cc97-c01e10eca296"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+----------+--------+-------+----------+------+----+----------+\n",
            "|txn_id|region|      city|store_id|product| sale_date|amount|year|dense_rank|\n",
            "+------+------+----------+--------+-------+----------+------+----+----------+\n",
            "|  T009|  East|     Patna|Store-06| Tablet|2024-01-03| 23000|2024|         1|\n",
            "|  T019|  East|   Kolkata|Store-05| Tablet|2024-01-05| 25000|2024|         1|\n",
            "|  T008|  East|   Kolkata|Store-05| Mobile|2024-01-02| 28000|2024|         2|\n",
            "|  T015|  East|     Patna|Store-06| Mobile|2024-01-04| 29000|2024|         2|\n",
            "|  T007|  East|   Kolkata|Store-05| Laptop|2024-01-01| 72000|2024|         3|\n",
            "|  T023|  East|     Patna|Store-06| Laptop|2024-01-06| 74000|2024|         3|\n",
            "|  T003| North|Chandigarh|Store-02| Tablet|2024-01-03| 26000|2024|         1|\n",
            "|  T021| North|     Delhi|Store-01| Tablet|2024-01-06| 28000|2024|         1|\n",
            "|  T002| North|     Delhi|Store-01| Mobile|2024-01-02| 32000|2024|         2|\n",
            "|  T017| North|Chandigarh|Store-02| Mobile|2024-01-05| 31000|2024|         2|\n",
            "|  T001| North|     Delhi|Store-01| Laptop|2024-01-01| 75000|2024|         3|\n",
            "|  T013| North|     Delhi|Store-01| Laptop|2024-01-04| 76000|2024|         3|\n",
            "|  T006| South| Bangalore|Store-03| Tablet|2024-01-03| 24000|2024|         1|\n",
            "|  T022| South|   Chennai|Store-04| Tablet|2024-01-06| 26000|2024|         1|\n",
            "|  T005| South|   Chennai|Store-04| Mobile|2024-01-02| 30000|2024|         2|\n",
            "|  T018| South| Bangalore|Store-03| Mobile|2024-01-05| 34000|2024|         2|\n",
            "|  T004| South| Bangalore|Store-03| Laptop|2024-01-01| 78000|2024|         3|\n",
            "|  T014| South|   Chennai|Store-04| Laptop|2024-01-04| 79000|2024|         3|\n",
            "|  T012|  West|      Pune|Store-08| Tablet|2024-01-03| 27000|2024|         1|\n",
            "|  T020|  West|    Mumbai|Store-07| Tablet|2024-01-05| 29000|2024|         1|\n",
            "+------+------+----------+--------+-------+----------+------+----+----------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Identify top 2 highest sales per region using window functions\n"
      ],
      "metadata": {
        "id": "jK0H93pb7vic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number, col\n",
        "\n",
        "window_spec = Window.partitionBy(\"region\").orderBy(col(\"amount\").desc())\n",
        "ranked_df = df.withColumn(\"row_num\", row_number().over(window_spec))\n",
        "ranked_df.filter(col(\"row_num\") <= 2).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca2BUxI-9KHd",
        "outputId": "5b22665a-3deb-4a1a-9b65-4c2d46bc087c"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+---------+--------+-------+----------+------+----+-------+\n",
            "|txn_id|region|     city|store_id|product| sale_date|amount|year|row_num|\n",
            "+------+------+---------+--------+-------+----------+------+----+-------+\n",
            "|  T023|  East|    Patna|Store-06| Laptop|2024-01-06| 74000|2024|      1|\n",
            "|  T007|  East|  Kolkata|Store-05| Laptop|2024-01-01| 72000|2024|      2|\n",
            "|  T013| North|    Delhi|Store-01| Laptop|2024-01-04| 76000|2024|      1|\n",
            "|  T001| North|    Delhi|Store-01| Laptop|2024-01-01| 75000|2024|      2|\n",
            "|  T014| South|  Chennai|Store-04| Laptop|2024-01-04| 79000|2024|      1|\n",
            "|  T004| South|Bangalore|Store-03| Laptop|2024-01-01| 78000|2024|      2|\n",
            "|  T010|  West|   Mumbai|Store-07| Laptop|2024-01-01| 80000|2024|      1|\n",
            "|  T016|  West|     Pune|Store-08| Laptop|2024-01-04| 77000|2024|      2|\n",
            "+------+------+---------+--------+-------+----------+------+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Compare rank vs dense_rank output\n"
      ],
      "metadata": {
        "id": "KybckKw57w9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_spec_amount = Window.partitionBy(\"region\").orderBy(\"amount\")\n",
        "\n",
        "# Calculate rank\n",
        "df_with_rank = df.withColumn(\"rank_output\", rank().over(window_spec_amount))\n",
        "\n",
        "# Calculate dense_rank\n",
        "df_with_dense_rank = df_with_rank.withColumn(\"dense_rank_output\", dense_rank().over(window_spec_amount))\n",
        "\n",
        "# Show the comparison for relevant columns\n",
        "df_with_dense_rank.select(\"region\", \"product\", \"amount\", \"rank_output\", \"dense_rank_output\").orderBy(\"region\", \"amount\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsTIw4129K2M",
        "outputId": "135abc35-62d8-4bd9-f620-94df81d652c5"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+------+-----------+-----------------+\n",
            "|region|product|amount|rank_output|dense_rank_output|\n",
            "+------+-------+------+-----------+-----------------+\n",
            "|  East| Tablet| 23000|          1|                1|\n",
            "|  East| Tablet| 25000|          2|                2|\n",
            "|  East| Mobile| 28000|          3|                3|\n",
            "|  East| Mobile| 29000|          4|                4|\n",
            "|  East| Laptop| 72000|          5|                5|\n",
            "|  East| Laptop| 74000|          6|                6|\n",
            "| North| Tablet| 26000|          1|                1|\n",
            "| North| Tablet| 28000|          2|                2|\n",
            "| North| Mobile| 31000|          3|                3|\n",
            "| North| Mobile| 32000|          4|                4|\n",
            "| North| Laptop| 75000|          5|                5|\n",
            "| North| Laptop| 76000|          6|                6|\n",
            "| South| Tablet| 24000|          1|                1|\n",
            "| South| Tablet| 26000|          2|                2|\n",
            "| South| Mobile| 30000|          3|                3|\n",
            "| South| Mobile| 34000|          4|                4|\n",
            "| South| Laptop| 78000|          5|                5|\n",
            "| South| Laptop| 79000|          6|                6|\n",
            "|  West| Tablet| 27000|          1|                1|\n",
            "|  West| Tablet| 29000|          2|                2|\n",
            "+------+-------+------+-----------+-----------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Calculate cumulative sales per store\n"
      ],
      "metadata": {
        "id": "BCGLR0f-7ywN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_spec = Window.partitionBy(\"store_id\")\n",
        "ranked_df = df.withColumn(\"sum\", sum(\"amount\").over(window_spec))\n",
        "ranked_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbb82348-b55d-47a0-f5d6-03be89bb31f8",
        "id": "QSjyVz-0EYae"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+----------+--------+-------+----------+------+----+------+\n",
            "|txn_id|region|      city|store_id|product| sale_date|amount|year|   sum|\n",
            "+------+------+----------+--------+-------+----------+------+----+------+\n",
            "|  T001| North|     Delhi|Store-01| Laptop|2024-01-01| 75000|2024|211000|\n",
            "|  T002| North|     Delhi|Store-01| Mobile|2024-01-02| 32000|2024|211000|\n",
            "|  T013| North|     Delhi|Store-01| Laptop|2024-01-04| 76000|2024|211000|\n",
            "|  T021| North|     Delhi|Store-01| Tablet|2024-01-06| 28000|2024|211000|\n",
            "|  T003| North|Chandigarh|Store-02| Tablet|2024-01-03| 26000|2024| 57000|\n",
            "|  T017| North|Chandigarh|Store-02| Mobile|2024-01-05| 31000|2024| 57000|\n",
            "|  T004| South| Bangalore|Store-03| Laptop|2024-01-01| 78000|2024|136000|\n",
            "|  T006| South| Bangalore|Store-03| Tablet|2024-01-03| 24000|2024|136000|\n",
            "|  T018| South| Bangalore|Store-03| Mobile|2024-01-05| 34000|2024|136000|\n",
            "|  T005| South|   Chennai|Store-04| Mobile|2024-01-02| 30000|2024|135000|\n",
            "|  T014| South|   Chennai|Store-04| Laptop|2024-01-04| 79000|2024|135000|\n",
            "|  T022| South|   Chennai|Store-04| Tablet|2024-01-06| 26000|2024|135000|\n",
            "|  T007|  East|   Kolkata|Store-05| Laptop|2024-01-01| 72000|2024|125000|\n",
            "|  T008|  East|   Kolkata|Store-05| Mobile|2024-01-02| 28000|2024|125000|\n",
            "|  T019|  East|   Kolkata|Store-05| Tablet|2024-01-05| 25000|2024|125000|\n",
            "|  T009|  East|     Patna|Store-06| Tablet|2024-01-03| 23000|2024|126000|\n",
            "|  T015|  East|     Patna|Store-06| Mobile|2024-01-04| 29000|2024|126000|\n",
            "|  T023|  East|     Patna|Store-06| Laptop|2024-01-06| 74000|2024|126000|\n",
            "|  T010|  West|    Mumbai|Store-07| Laptop|2024-01-01| 80000|2024|144000|\n",
            "|  T011|  West|    Mumbai|Store-07| Mobile|2024-01-02| 35000|2024|144000|\n",
            "+------+------+----------+--------+-------+----------+------+----+------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Identify first and last transaction per city using windows"
      ],
      "metadata": {
        "id": "nf5rzDLv70rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number, col\n",
        "\n",
        "# Window specification for the first transaction (ordered by sale_date ascending)\n",
        "window_first = Window.partitionBy(\"city\").orderBy(\"sale_date\")\n",
        "\n",
        "# Add row number for first transaction and filter\n",
        "df_with_first_rn = df_sales.withColumn(\"row_num_asc\", row_number().over(window_first))\n",
        "first_transactions = df_with_first_rn.filter(col(\"row_num_asc\") == 1).drop(\"row_num_asc\")\n",
        "\n",
        "print(\"First transactions per city:\")\n",
        "first_transactions.show()\n",
        "\n",
        "# Window specification for the last transaction (ordered by sale_date descending)\n",
        "window_last = Window.partitionBy(\"city\").orderBy(col(\"sale_date\").desc())\n",
        "\n",
        "# Add row number for last transaction and filter\n",
        "df_with_last_rn = df_sales.withColumn(\"row_num_desc\", row_number().over(window_last))\n",
        "last_transactions = df_with_last_rn.filter(col(\"row_num_desc\") == 1).drop(\"row_num_desc\")\n",
        "\n",
        "print(\"Last transactions per city:\")\n",
        "last_transactions.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gLmCkBS70eM",
        "outputId": "d8ee3926-99ec-43c7-a18d-215bab4e5e6b"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First transactions per city:\n",
            "+------+------+----------+--------+-------+----------+------+\n",
            "|txn_id|region|      city|store_id|product| sale_date|amount|\n",
            "+------+------+----------+--------+-------+----------+------+\n",
            "|  T004| South| Bangalore|Store-03| Laptop|2024-01-01| 78000|\n",
            "|  T003| North|Chandigarh|Store-02| Tablet|2024-01-03| 26000|\n",
            "|  T005| South|   Chennai|Store-04| Mobile|2024-01-02| 30000|\n",
            "|  T001| North|     Delhi|Store-01| Laptop|2024-01-01| 75000|\n",
            "|  T007|  East|   Kolkata|Store-05| Laptop|2024-01-01| 72000|\n",
            "|  T010|  West|    Mumbai|Store-07| Laptop|2024-01-01| 80000|\n",
            "|  T009|  East|     Patna|Store-06| Tablet|2024-01-03| 23000|\n",
            "|  T012|  West|      Pune|Store-08| Tablet|2024-01-03| 27000|\n",
            "+------+------+----------+--------+-------+----------+------+\n",
            "\n",
            "Last transactions per city:\n",
            "+------+------+----------+--------+-------+----------+------+\n",
            "|txn_id|region|      city|store_id|product| sale_date|amount|\n",
            "+------+------+----------+--------+-------+----------+------+\n",
            "|  T018| South| Bangalore|Store-03| Mobile|2024-01-05| 34000|\n",
            "|  T017| North|Chandigarh|Store-02| Mobile|2024-01-05| 31000|\n",
            "|  T022| South|   Chennai|Store-04| Tablet|2024-01-06| 26000|\n",
            "|  T021| North|     Delhi|Store-01| Tablet|2024-01-06| 28000|\n",
            "|  T019|  East|   Kolkata|Store-05| Tablet|2024-01-05| 25000|\n",
            "|  T020|  West|    Mumbai|Store-07| Tablet|2024-01-05| 29000|\n",
            "|  T023|  East|     Patna|Store-06| Laptop|2024-01-06| 74000|\n",
            "|  T024|  West|      Pune|Store-08| Mobile|2024-01-06| 33000|\n",
            "+------+------+----------+--------+-------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_9MGLlf27icP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cbd9887"
      },
      "source": [
        "# Task\n",
        "The original user task is to:\n",
        "\"Analyze and explain the physical execution plans of various Apache Spark DataFrame operations, specifically `select`, `filter`, `groupBy`, and `window functions`, identifying the presence of shuffles, exchanges, and sorts, and providing an explanation for why window functions introduce sorting.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fb232a3"
      },
      "source": [
        "## Explain Select Operation\n",
        "\n",
        "### Subtask:\n",
        "Run explain(True) for a simple DataFrame select operation to observe its physical plan.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1147f399"
      },
      "source": [
        "**Reasoning**:\n",
        "To observe the physical plan of a simple DataFrame select operation, I will select the specified columns from the `df_sales` DataFrame and then call the `explain(True)` method on the result.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65e3a825",
        "outputId": "8defcb2b-77c2-4e9c-cbd4-07080a9f3cab"
      },
      "source": [
        "df_sales.select(\"region\", \"product\", \"amount\").explain(True)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Project ['region, 'product, 'amount]\n",
            "+- LogicalRDD [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "region: string, product: string, amount: bigint\n",
            "Project [region#40, product#43, amount#45L]\n",
            "+- LogicalRDD [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [region#40, product#43, amount#45L]\n",
            "+- LogicalRDD [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L], false\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [region#40, product#43, amount#45L]\n",
            "+- *(1) Scan ExistingRDD[txn_id#39,region#40,city#41,store_id#42,product#43,sale_date#44,amount#45L]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2a5904c"
      },
      "source": [
        "### Explanation of Select Operation's Physical Plan\n",
        "\n",
        "The `explain(True)` output for the simple `select` operation (`df_sales.select(\"region\", \"product\", \"amount\")`) shows a straightforward execution plan without any complex operations like shuffles, exchanges, or sorts.\n",
        "\n",
        "**Physical Plan Breakdown:**\n",
        "\n",
        "1.  **`*(1) Project [region#40, product#43, amount#45L]`**: This indicates that Spark is performing a projection operation. It's essentially selecting (or picking out) the specified columns (`region`, `product`, `amount`) from the input data. This is a narrow transformation, meaning it doesn't require shuffling data across partitions or nodes.\n",
        "\n",
        "2.  **`+- *(1) Scan ExistingRDD[txn_id#39,region#40,city#41,store_id#42,product#43,sale_date#44,amount#45L]`**: This is the source of the data. Spark is scanning an existing Resilient Distributed Dataset (RDD), which in this case was created from the in-memory `sales_data`. This step involves reading the raw data.\n",
        "\n",
        "**Absence of Shuffles, Exchanges, and Sorts:**\n",
        "\n",
        "*   **Shuffles/Exchanges**: These operations are typically introduced by wide transformations such as `groupByKey`, `reduceByKey`, `join`, or `repartition`. A `select` operation only involves picking columns and doesn't change the partitioning of the data or require data movement between executors. Therefore, no shuffles or exchanges are necessary.\n",
        "*   **Sorts**: Sorting operations are introduced by `orderBy` clauses or certain window functions. Since there is no explicit sorting requested in a simple `select` statement, no sort operations appear in the physical plan.\n",
        "\n",
        "In summary, the physical plan for a `select` operation is very efficient as it only involves reading the data and projecting the required columns, all within the existing partitions, hence avoiding costly data movement operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f3a6baf"
      },
      "source": [
        "## Explain Filter Operation\n",
        "\n",
        "### Subtask:\n",
        "Run `explain(True)` for a DataFrame filter operation to observe its physical plan.\n",
        "\n",
        "#### Instructions\n",
        "1. Filter the `df_reordered` DataFrame to include only sales where the product is \"Laptop\".\n",
        "2. Call the `.explain(True)` method on the resulting DataFrame to display its physical execution plan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efe0c642"
      },
      "source": [
        "**Reasoning**:\n",
        "As per the instructions, I will now filter the `df_reordered` DataFrame for 'Laptop' products and then apply `explain(True)` to display its physical plan.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb603bb4",
        "outputId": "5639cbba-bb25-4249-c29e-5e8d30f2bac1"
      },
      "source": [
        "df_reordered.filter(df_reordered.product == \"Laptop\").explain(True)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Filter '`=`(product#43, Laptop)\n",
            "+- Project [txn_id#39, sale_date#44, year#201, region#40, city#41, store_id#42, product#43, amount#45L]\n",
            "   +- Project [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L, substring(sale_date#44, 0, 4) AS year#201]\n",
            "      +- LogicalRDD [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "txn_id: string, sale_date: string, year: string, region: string, city: string, store_id: string, product: string, amount: bigint\n",
            "Filter (product#43 = Laptop)\n",
            "+- Project [txn_id#39, sale_date#44, year#201, region#40, city#41, store_id#42, product#43, amount#45L]\n",
            "   +- Project [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L, substring(sale_date#44, 0, 4) AS year#201]\n",
            "      +- LogicalRDD [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [txn_id#39, sale_date#44, substring(sale_date#44, 0, 4) AS year#201, region#40, city#41, store_id#42, product#43, amount#45L]\n",
            "+- Filter (isnotnull(product#43) AND (product#43 = Laptop))\n",
            "   +- LogicalRDD [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L], false\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [txn_id#39, sale_date#44, substring(sale_date#44, 0, 4) AS year#201, region#40, city#41, store_id#42, product#43, amount#45L]\n",
            "+- *(1) Filter (isnotnull(product#43) AND (product#43 = Laptop))\n",
            "   +- *(1) Scan ExistingRDD[txn_id#39,region#40,city#41,store_id#42,product#43,sale_date#44,amount#45L]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d126b030"
      },
      "source": [
        "### Explanation of Filter Operation's Physical Plan\n",
        "\n",
        "The `explain(True)` output for the `filter` operation (`df_reordered.filter(df_reordered.product == \"Laptop\")`) reveals that Spark processes filters efficiently, similar to `select`, without introducing costly operations like shuffles, exchanges, or explicit sorts.\n",
        "\n",
        "**Physical Plan Breakdown:**\n",
        "\n",
        "1.  **`*(1) Project [txn_id#39, sale_date#44, substring(sale_date#44, 0, 4) AS year#201, region#40, city#41, store_id#42, product#43, amount#45L]`**: This is a projection step that selects and potentially transforms columns. In this specific case, the `year` column was derived earlier, and this step ensures all relevant columns are retained in the final output.\n",
        "\n",
        "2.  **`+- *(1) Filter (isnotnull(product#43) AND (product#43 = Laptop))`**: This is the core filtering operation. Spark applies the condition `product == \"Laptop\"` to each row. The `isnotnull(product#43)` part is added by Spark's optimizer to handle potential null values gracefully, ensuring that only non-null 'product' values are compared. This operation is a **narrow transformation** because each row can be processed independently to determine if it meets the filter criteria. The data within partitions does not need to be rearranged or moved to other partitions.\n",
        "\n",
        "3.  **`+- *(1) Scan ExistingRDD[txn_id#39,region#40,city#41,store_id#42,product#43,sale_date#44,amount#45L]`**: This is the initial step where Spark reads the data from the underlying RDD (which represents our `df_sales` DataFrame). The filter condition is pushed down as close to the data source as possible (predicate pushdown), allowing Spark to read only the necessary data from the outset.\n",
        "\n",
        "**Absence of Shuffles, Exchanges, and Sorts:**\n",
        "\n",
        "*   **Shuffles/Exchanges**: Filter operations, being narrow transformations, do not require data to be redistributed across the network. Each partition can be filtered independently on the node where it resides. Thus, there are no `Exchange` operations (which signify shuffles).\n",
        "*   **Sorts**: No explicit sorting was requested in the filter operation, and filtering itself does not inherently require data to be sorted. Therefore, no `Sort` operations appear in the physical plan.\n",
        "\n",
        "Spark's Catalyst Optimizer intelligently combines the projection and filter operations and pushes the filter predicate down to the data source, optimizing performance by reducing the amount of data processed and transferred."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "299c9e69"
      },
      "source": [
        "## Explain GroupBy Operation\n",
        "\n",
        "### Subtask:\n",
        "Run `explain(True)` for a DataFrame `groupBy` and `agg` operation to observe its physical plan.\n",
        "\n",
        "#### Instructions\n",
        "1. Group the `df_sales` DataFrame by `region`.\n",
        "2. Aggregate the sum of `amount` and alias it as `total_sales_amount`.\n",
        "3. Call the `.explain(True)` method on the resulting DataFrame to display its physical execution plan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "520f1554"
      },
      "source": [
        "**Reasoning**:\n",
        "As per the instructions in the preceding markdown block, I will now group the `df_sales` DataFrame by `region`, aggregate the sum of `amount`, and then apply `explain(True)` to display its physical plan.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d25a812c",
        "outputId": "834491ad-0d65-4404-9cec-a3c891ccf583"
      },
      "source": [
        "from pyspark.sql.functions import sum\n",
        "\n",
        "df_sales.groupBy(\"region\").agg(sum(\"amount\").alias(\"total_sales_amount\")).explain(True)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Aggregate ['region], ['region, 'sum('amount) AS total_sales_amount#1314]\n",
            "+- LogicalRDD [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "region: string, total_sales_amount: bigint\n",
            "Aggregate [region#40], [region#40, sum(amount#45L) AS total_sales_amount#1314L]\n",
            "+- LogicalRDD [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Aggregate [region#40], [region#40, sum(amount#45L) AS total_sales_amount#1314L]\n",
            "+- Project [region#40, amount#45L]\n",
            "   +- LogicalRDD [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[region#40], functions=[sum(amount#45L)], output=[region#40, total_sales_amount#1314L])\n",
            "   +- Exchange hashpartitioning(region#40, 200), ENSURE_REQUIREMENTS, [plan_id=2474]\n",
            "      +- HashAggregate(keys=[region#40], functions=[partial_sum(amount#45L)], output=[region#40, sum#1324L])\n",
            "         +- Project [region#40, amount#45L]\n",
            "            +- Scan ExistingRDD[txn_id#39,region#40,city#41,store_id#42,product#43,sale_date#44,amount#45L]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "046d6a86"
      },
      "source": [
        "### Explanation of GroupBy Operation's Physical Plan\n",
        "\n",
        "The `explain(True)` output for the `groupBy` and `agg` operation (`df_sales.groupBy(\"region\").agg(sum(\"amount\").alias(\"total_sales_amount\"))`) shows a more complex physical plan compared to `select` and `filter`, primarily due to the introduction of **shuffles** (represented by `Exchange`) and **aggregate** stages.\n",
        "\n",
        "**Physical Plan Breakdown:**\n",
        "\n",
        "1.  **`AdaptiveSparkPlan isFinalPlan=false`**: This indicates that Spark's Adaptive Query Execution (AQE) is enabled, which can dynamically optimize the query plan during execution based on runtime statistics.\n",
        "\n",
        "2.  **`+- HashAggregate(keys=[region#40], functions=[sum(amount#45L)], output=[region#40, total_sales_amount#1314L])`**: This is the final aggregation step. After all the data for each `region` has been brought together, Spark performs the final `sum` aggregation to compute the `total_sales_amount` for each unique region. This is typically the second stage of a two-stage aggregation.\n",
        "\n",
        "3.  **`+- Exchange hashpartitioning(region#40, 200), ENSURE_REQUIREMENTS, [plan_id=2474]`**: This is the crucial step that introduces a **shuffle**. For a `groupBy` operation, all records belonging to the same grouping key (in this case, `region`) must be processed together to compute the aggregate. If data for a given region is spread across different partitions on different executors, Spark needs to redistribute (shuffle) the data so that all data for a specific `region` ends up on the same partition. `hashpartitioning(region#40, 200)` means Spark is using a hash function on the `region` column to determine which of the 200 default partitions each row should go to. `ENSURE_REQUIREMENTS` ensures that the data meets the partitioning requirements for the subsequent aggregation step.\n",
        "\n",
        "4.  **`+- HashAggregate(keys=[region#40], functions=[partial_sum(amount#45L)], output=[region#40, sum#1324L])`**: This is the **partial aggregation** step, performed *before* the shuffle. To optimize performance and reduce the amount of data shuffled across the network, Spark often performs a local (partial) aggregation on each partition first. For example, if a partition has multiple sales for 'North' region, Spark will sum these locally before sending them over the network. This reduces the data volume that needs to be shuffled.\n",
        "\n",
        "5.  **`+- Project [region#40, amount#45L]`**: This projects only the necessary columns (`region` and `amount`) from the input for the aggregation, discarding other columns early on.\n",
        "\n",
        "6.  **`+- Scan ExistingRDD[txn_id#39,region#40,city#41,store_id#42,product#43,sale_date#44,amount#45L]`**: The initial step of reading the data from the source RDD.\n",
        "\n",
        "**Presence of Shuffles (Exchanges):**\n",
        "\n",
        "The `groupBy` operation inherently requires a **shuffle** (represented by the `Exchange` operator) because it's a **wide transformation**. To compute aggregates like `sum` for each region, all rows corresponding to a particular region must be collected and processed together. If these rows are distributed across different partitions or nodes, Spark must redistribute the data across the cluster to ensure that all values for a given key are on the same machine/partition. This data movement across the network is expensive and is a primary factor impacting the performance of `groupBy` operations.\n",
        "\n",
        "**Absence of Sorts (in this specific case):**\n",
        "\n",
        "While some aggregations might implicitly involve sorting (e.g., if you were to use `orderBy` or certain window functions), a simple `groupBy` with `sum` aggregation does not inherently require sorting. Spark uses `HashAggregate` here, which typically involves hashing keys to group them, not sorting. Therefore, no explicit `Sort` operations are present in this physical plan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2244c27f"
      },
      "source": [
        "## Explain Window Functions Operation\n",
        "\n",
        "### Subtask:\n",
        "Run `explain(True)` for a DataFrame operation involving a window function to observe its physical plan, identifying shuffles, exchanges, and sorts, and explaining why sorts are introduced.\n",
        "\n",
        "#### Instructions\n",
        "1.  Define a window specification that partitions by `region` and orders by `sale_date`.\n",
        "2.  Calculate the `running_total_sales` using `sum(\"amount\")` over this window on the `df_sales` DataFrame.\n",
        "3.  Call the `.explain(True)` method on the resulting DataFrame to display its physical execution plan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f24fcf8"
      },
      "source": [
        "**Reasoning**:\n",
        "As per the instructions in the preceding markdown block, I will define a window specification, apply it to calculate running total sales, and then call `explain(True)` to display its physical plan.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd1d7d22",
        "outputId": "c1286333-24a9-4cb9-da30-bae2385aa944"
      },
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import sum, col\n",
        "\n",
        "window_spec = Window.partitionBy(\"region\").orderBy(\"sale_date\")\n",
        "df_sales.withColumn(\"running_total_sales\", sum(\"amount\").over(window_spec)).explain(True)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Project [unresolvedstarwithcolumns(running_total_sales, 'sum('amount) windowspecdefinition('region, 'sale_date ASC NULLS FIRST, unspecifiedframe$()), None)]\n",
            "+- LogicalRDD [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "txn_id: string, region: string, city: string, store_id: string, product: string, sale_date: string, amount: bigint, running_total_sales: bigint\n",
            "Project [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L, running_total_sales#1325L]\n",
            "+- Project [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L, running_total_sales#1325L, running_total_sales#1325L]\n",
            "   +- Window [sum(amount#45L) windowspecdefinition(region#40, sale_date#44 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS running_total_sales#1325L], [region#40], [sale_date#44 ASC NULLS FIRST]\n",
            "      +- Project [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L]\n",
            "         +- LogicalRDD [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Window [sum(amount#45L) windowspecdefinition(region#40, sale_date#44 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS running_total_sales#1325L], [region#40], [sale_date#44 ASC NULLS FIRST]\n",
            "+- LogicalRDD [txn_id#39, region#40, city#41, store_id#42, product#43, sale_date#44, amount#45L], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Window [sum(amount#45L) windowspecdefinition(region#40, sale_date#44 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS running_total_sales#1325L], [region#40], [sale_date#44 ASC NULLS FIRST]\n",
            "   +- Sort [region#40 ASC NULLS FIRST, sale_date#44 ASC NULLS FIRST], false, 0\n",
            "      +- Exchange hashpartitioning(region#40, 200), ENSURE_REQUIREMENTS, [plan_id=2484]\n",
            "         +- Scan ExistingRDD[txn_id#39,region#40,city#41,store_id#42,product#43,sale_date#44,amount#45L]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52128755"
      },
      "source": [
        "### Explanation of Window Function's Physical Plan\n",
        "\n",
        "The `explain(True)` output for the window function operation (`df_sales.withColumn(\"running_total_sales\", sum(\"amount\").over(window_spec))`) reveals a physical plan that includes **shuffles**, **sorts**, and the **Window** operator itself. This complexity arises because window functions require data to be grouped (partitioned) and ordered before calculations can be performed within each window.\n",
        "\n",
        "**Physical Plan Breakdown:**\n",
        "\n",
        "1.  **`AdaptiveSparkPlan isFinalPlan=false`**: Indicates that Adaptive Query Execution (AQE) is enabled for potential runtime optimizations.\n",
        "\n",
        "2.  **`+- Window [sum(amount#45L) windowspecdefinition(region#40, sale_date#44 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS running_total_sales#1325L], [region#40], [sale_date#44 ASC NULLS FIRST]`**: This is the actual window operation. It computes the `sum(amount)` for the `running_total_sales` within each defined window. The window is partitioned by `region` and ordered by `sale_date` (ascending, nulls first), with a frame of `unboundedpreceding` to `currentrow` (which defines the running total).\n",
        "\n",
        "3.  **`+- Sort [region#40 ASC NULLS FIRST, sale_date#44 ASC NULLS FIRST], false, 0`**: This step explicitly introduces a **sort** operation. Data is sorted first by `region` and then by `sale_date` within each `region`. The `false` indicates that this is not a global sort but rather a local sort within partitions after shuffling.\n",
        "\n",
        "4.  **`+- Exchange hashpartitioning(region#40, 200), ENSURE_REQUIREMENTS, [plan_id=2484]`**: This is a **shuffle** operation. For window functions partitioned by a column (here, `region`), Spark needs to ensure that all rows belonging to the same `region` are grouped together on the same partition. `hashpartitioning(region#40, 200)` redistributes the data based on the hash of the `region` column into 200 partitions. This is a wide transformation, incurring network I/O.\n",
        "\n",
        "5.  **`+- Scan ExistingRDD[txn_id#39,region#40,city#41,store_id#42,product#43,sale_date#44,amount#45L]`**: The initial step of reading the data from the source RDD.\n",
        "\n",
        "**Why Window Functions Introduce Sorting:**\n",
        "\n",
        "Window functions operate on a defined"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d01ff5f"
      },
      "source": [
        "### Explanation of Window Function's Physical Plan\n",
        "\n",
        "The `explain(True)` output for the window function operation (`df_sales.withColumn(\"running_total_sales\", sum(\"amount\").over(window_spec))`) reveals a physical plan that includes **shuffles**, **sorts**, and the **Window** operator itself. This complexity arises because window functions require data to be grouped (partitioned) and ordered before calculations can be performed within each window.\n",
        "\n",
        "**Physical Plan Breakdown:**\n",
        "\n",
        "1.  **`AdaptiveSparkPlan isFinalPlan=false`**: Indicates that Adaptive Query Execution (AQE) is enabled for potential runtime optimizations.\n",
        "\n",
        "2.  **`+- Window [sum(amount#45L) windowspecdefinition(region#40, sale_date#44 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS running_total_sales#1325L], [region#40], [sale_date#44 ASC NULLS FIRST]`**: This is the actual window operation. It computes the `sum(amount)` for the `running_total_sales` within each defined window. The window is partitioned by `region` and ordered by `sale_date` (ascending, nulls first), with a frame of `unboundedpreceding` to `currentrow` (which defines the running total).\n",
        "\n",
        "3.  **`+- Sort [region#40 ASC NULLS FIRST, sale_date#44 ASC NULLS FIRST], false, 0`**: This step explicitly introduces a **sort** operation. Data is sorted first by `region` and then by `sale_date` within each `region`. The `false` indicates that this is not a global sort but rather a local sort within partitions after shuffling.\n",
        "\n",
        "4.  **`+- Exchange hashpartitioning(region#40, 200), ENSURE_REQUIREMENTS, [plan_id=2484]`**: This is a **shuffle** operation. For window functions partitioned by a column (here, `region`), Spark needs to ensure that all rows belonging to the same `region` are grouped together on the same partition. `hashpartitioning(region#40, 200)` redistributes the data based on the hash of the `region` column into 200 partitions. This is a wide transformation, incurring network I/O.\n",
        "\n",
        "5.  **`+- Scan ExistingRDD[txn_id#39,region#40,city#41,store_id#42,product#43,sale_date#44,amount#45L]`**: The initial step of reading the data from the source RDD.\n",
        "\n",
        "**Why Window Functions Introduce Sorting:**\n",
        "\n",
        "Window functions operate on a defined **frame of rows** relative to the current row, within each partition. The `ORDER BY` clause within a window specification is fundamental because it dictates the sequence in which these rows are considered. For calculations like a running total (`sum(...) OVER (ORDER BY ...)`), or for ranking functions (`rank()`, `row_number()`), the order of data is critical to produce correct and deterministic results. Spark must physically sort the data within each partition according to the `ORDER BY` clause of the window specification to guarantee that the window frame is correctly established and the aggregate or ranking function operates on the intended set of ordered rows. This ensures that the results (e.g., the sum up to a certain date, or the rank of a transaction) are accurate based on the specified order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa55f697"
      },
      "source": [
        "### Explanation of Window Function's Physical Plan\n",
        "\n",
        "The `explain(True)` output for the window function operation (`df_sales.withColumn(\"running_total_sales\", sum(\"amount\").over(window_spec))`) reveals a physical plan that includes **shuffles**, **sorts**, and the **Window** operator itself. This complexity arises because window functions require data to be grouped (partitioned) and ordered before calculations can be performed within each window.\n",
        "\n",
        "**Physical Plan Breakdown:**\n",
        "\n",
        "1.  **`AdaptiveSparkPlan isFinalPlan=false`**: Indicates that Adaptive Query Execution (AQE) is enabled for potential runtime optimizations.\n",
        "\n",
        "2.  **`+- Window [sum(amount#45L) windowspecdefinition(region#40, sale_date#44 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS running_total_sales#1325L], [region#40], [sale_date#44 ASC NULLS FIRST]`**: This is the actual window operation. It computes the `sum(amount)` for the `running_total_sales` within each defined window. The window is partitioned by `region` and ordered by `sale_date` (ascending, nulls first), with a frame of `unboundedpreceding` to `currentrow` (which defines the running total).\n",
        "\n",
        "3.  **`+- Sort [region#40 ASC NULLS FIRST, sale_date#44 ASC NULLS FIRST], false, 0`**: This step explicitly introduces a **sort** operation. Data is sorted first by `region` and then by `sale_date` within each `region`. The `false` indicates that this is not a global sort but rather a local sort within partitions after shuffling.\n",
        "\n",
        "4.  **`+- Exchange hashpartitioning(region#40, 200), ENSURE_REQUIREMENTS, [plan_id=2484]`**: This is a **shuffle** operation. For window functions partitioned by a column (here, `region`), Spark needs to ensure that all rows belonging to the same `region` are grouped together on the same partition. `hashpartitioning(region#40, 200)` redistributes the data based on the hash of the `region` column into 200 partitions. This is a wide transformation, incurring network I/O.\n",
        "\n",
        "5.  **`+- Scan ExistingRDD[txn_id#39,region#40,city#41,store_id#42,product#43,sale_date#44,amount#45L]`**: The initial step of reading the data from the source RDD.\n",
        "\n",
        "**Why Window Functions Introduce Sorting:**\n",
        "\n",
        "Window functions operate on a defined **frame of rows** relative to the current row, within each partition. The `ORDER BY` clause within a window specification is fundamental because it dictates the sequence in which these rows are considered. For calculations like a running total (`sum(...) OVER (ORDER BY ...)`), or for ranking functions (`rank()`, `row_number()`), the order of data is critical to produce correct and deterministic results. Spark must physically sort the data within each partition according to the `ORDER BY` clause of the window specification to guarantee that the window frame is correctly established and the aggregate or ranking function operates on the intended set of ordered rows. This ensures that the results (e.g., the sum up to a certain date, or the rank of a transaction) are accurate based on the specified order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45ce38ae"
      },
      "source": [
        "## Analyze Explain Outputs and Explain Window Sorting\n",
        "\n",
        "### Subtask:\n",
        "Consolidate the observations from the explain outputs for select, filter, groupBy, and window functions, identify the presence of shuffles, exchanges, and sorts in each, and provide a comprehensive explanation for why window functions introduce sorting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "218f452f"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "1.  **Which DataFrame operations introduce shuffles or exchanges?**\n",
        "    The `groupBy` operation introduces a shuffle (observed as `Exchange hashpartitioning`) because it's a wide transformation requiring data redistribution to group records by key. Similarly, `window functions` also introduce a shuffle (`Exchange hashpartitioning`) when a `partitionBy` clause is specified, to co-locate all rows belonging to the same partition.\n",
        "2.  **Which DataFrame operations introduce sorts?**\n",
        "    The `window functions` operation explicitly introduces a sort (observed as `Sort [region#40 ASC NULLS FIRST, sale_date#44 ASC NULLS FIRST]`) due to the `orderBy` clause within the window specification. Simple `select`, `filter`, and `groupBy` (with `HashAggregate`) operations do not introduce sorts.\n",
        "3.  **Why do window functions introduce sorting?**\n",
        "    Window functions introduce sorting to ensure deterministic and correct calculations within a defined window frame. The `ORDER BY` clause within a window specification dictates the sequence in which rows are processed, which is critical for functions like running totals or ranking. Spark must physically sort the data within each partition according to this clause to establish the correct window frame and perform accurate computations.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   **`select` and `filter` operations are narrow transformations**: They execute efficiently, involving only `Project` and `Filter` operations, respectively, followed by `Scan ExistingRDD`. Neither introduces shuffles, exchanges, or sorts, making them generally inexpensive operations.\n",
        "*   **`groupBy` operations are wide transformations and introduce shuffles**: The physical plan for `groupBy` includes a `HashAggregate` (partial), followed by an `Exchange hashpartitioning` (shuffle), and then a final `HashAggregate`. The shuffle is necessary to bring all data for a specific grouping key together for aggregation.\n",
        "*   **`window functions` are complex wide transformations that introduce both shuffles and sorts**: The physical plan for window functions includes an `Exchange hashpartitioning` (shuffle) for partitioning the data by the window's `partitionBy` clause, followed by an explicit `Sort` operation based on the window's `orderBy` clause. Finally, the `Window` operator performs the calculation.\n",
        "*   The `Sort` operation observed in the window function's plan is directly attributed to the `orderBy(\"sale_date\")` clause, indicating Spark's necessity to order data for accurate window-based calculations.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   **Cost Implications**: Operations introducing `Exchange` (shuffles) and `Sort` are generally more resource-intensive due to network I/O and CPU overhead. When designing Spark applications, minimize such wide transformations where possible.\n",
        "*   **Optimize Window Functions**: For window functions, consider if the `ORDER BY` clause is strictly necessary or if an alternative approach can achieve the desired result with fewer costly operations, especially on very large datasets.\n"
      ]
    }
  ]
}