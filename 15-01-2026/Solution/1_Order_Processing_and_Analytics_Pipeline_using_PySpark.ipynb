{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codingniket/Python-Training/blob/main/15-01-2026/Solution/1_Order_Processing_and_Analytics_Pipeline_using_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA7o7rlIAi3g"
      },
      "source": [
        "1. Load the CSV file without schema inference.\n",
        "2. Print the schema.\n",
        "3. Count total records.\n",
        "4. Show sample rows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QhPE8FN68T24"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
        "from pyspark.sql.functions import trim, col, when, to_date, sum as spark_sum, avg, desc, rank, lit, coalesce, isnull,try_to_timestamp,regexp_extract,initcap\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Project1\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsQt-ppC9Lrp",
        "outputId": "a9d46430-2d83-49f2-911f-ececca24e2ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+\n",
            "|   order_id|customer_id|       city|   category|    product| amount|order_date|   status|\n",
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+\n",
            "|ORD00000000|    C000000| hyderabad |   grocery |       Oil |invalid|01/01/2024|Cancelled|\n",
            "|ORD00000001|    C000001|       Pune|    Grocery|      Sugar|  35430|2024-01-02|Completed|\n",
            "|ORD00000002|    C000002|       Pune|Electronics|     Mobile|  65358|2024-01-03|Completed|\n",
            "|ORD00000003|    C000003|  Bangalore|Electronics|     Laptop|   5558|2024-01-04|Completed|\n",
            "|ORD00000004|    C000004|       Pune|       Home|AirPurifier|  33659|2024-01-05|Completed|\n",
            "|ORD00000005|    C000005|      Delhi|    Fashion|      Jeans|   8521|2024-01-06|Completed|\n",
            "|ORD00000006|    C000006|      Delhi|    Grocery|      Sugar|  42383|2024-01-07|Completed|\n",
            "|ORD00000007|    C000007|       Pune|    Grocery|       Rice|  45362|2024-01-08|Completed|\n",
            "|ORD00000008|    C000008|  Bangalore|    Fashion|      Jeans|  10563|2024-01-09|Completed|\n",
            "|ORD00000009|    C000009|    Kolkata|Electronics|     Laptop|  63715|2024-01-10|Completed|\n",
            "|ORD00000010|    C000010|  Bangalore|    Grocery|      Sugar|  66576|2024-01-11|Completed|\n",
            "|ORD00000011|    C000011|    Kolkata|Electronics|     Tablet|  50318|12/01/2024|Completed|\n",
            "|ORD00000012|    C000012|  Bangalore|    Grocery|      Sugar|  84768|2024-01-13|Completed|\n",
            "|ORD00000013|    C000013|       Pune|    Fashion|     TShirt|  79121|2024/01/14|Completed|\n",
            "|ORD00000014|    C000014|     Mumbai|Electronics|     Tablet|  79469|2024-01-15|Completed|\n",
            "|ORD00000015|    C000015|       Pune|Electronics|     Mobile|  81018|2024-01-16|Completed|\n",
            "|ORD00000016|    C000016|     Mumbai|       Home|      Mixer|  64225|2024-01-17|Completed|\n",
            "|ORD00000017|    C000017| bangalore |    Grocery|        Oil|  69582|2024-01-18|Completed|\n",
            "|ORD00000018|    C000018|    Kolkata|    Fashion|      Jeans|  50424|2024-01-19|Completed|\n",
            "|ORD00000019|    C000019|     Mumbai|Electronics|     Mobile|invalid|2024-01-20|Completed|\n",
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+\n",
            "only showing top 20 rows\n",
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- amount: string (nullable = true)\n",
            " |-- order_date: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DataFrame[summary: string, order_id: string, customer_id: string, city: string, category: string, product: string, amount: string, order_date: string, status: string]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_raw = spark.read \\\n",
        ".option(\"header\", \"true\") \\\n",
        ".option(\"inferSchema\", \"false\") \\\n",
        ".csv(\"orders.csv\")\n",
        "\n",
        "df_raw.show()\n",
        "df_raw.printSchema()\n",
        "df_raw.count()\n",
        "df_raw.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ly-_jsS9ccq",
        "outputId": "d76f7f63-2171-4bd2-f828-661d56b96428"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "300000"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_raw.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ph8hBrDG9xAS",
        "outputId": "ec21381a-179a-4c46-bc62-78cdb559f24e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_raw.isEmpty()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E62SRvdRAmrI"
      },
      "source": [
        "5. Explain why all columns must be treated as StringType initially.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT9mK2GIAqjp"
      },
      "source": [
        "If we used specific column value like int , date all the corrupted data wont fit in the schema and it will result in error and only way to handle the situatuion is either delete the data or convert to stringtype clean the data and then use it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiMAl0IV-Aj1"
      },
      "source": [
        "The data has no empty and has around 3 lakhs of input\n",
        "Now cleaning the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9vjQ7HVBKGt"
      },
      "source": [
        "PHASE 2 – Data Cleaning\n",
        "The dataset must be cleaned in the following way:\n",
        "1. Remove leading and trailing spaces from:\n",
        "city\n",
        "category\n",
        "product\n",
        "2. Standardize text:\n",
        "Convert city, category, and product to proper case.\n",
        "3. Clean the amount column:\n",
        "Remove commas.\n",
        "Replace empty strings and invalid values with null.\n",
        "Convert amount into IntegerType.\n",
        "Rows with invalid amounts must not crash the pipeline.\n",
        "4. Clean the order_date column:\n",
        "Support the following formats:\n",
        "yyyy-MM-dd\n",
        "dd/MM/yyyy\n",
        "yyyy/MM/dd\n",
        "\n",
        "Create a new column: order_date_clean with DateType.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-yIBYFkF940w"
      },
      "outputs": [],
      "source": [
        "cols_to_clean = [\"city\", \"category\", \"product\", \"amount\",\"status\"]\n",
        "df_cleaned_str = df_raw\n",
        "for col_name in cols_to_clean:\n",
        "  df_cleaned_str = df_cleaned_str.withColumn(col_name, initcap(trim(col(col_name))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzUtFH7L-qgz",
        "outputId": "9d2f4be0-01d5-4516-a7ce-f3973d4c5797"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-----------+---------+-----------+-----------+-------+----------+---------+\n",
            "|   order_id|customer_id|     city|   category|    product| amount|order_date|   status|\n",
            "+-----------+-----------+---------+-----------+-----------+-------+----------+---------+\n",
            "|ORD00000000|    C000000|Hyderabad|    Grocery|        Oil|Invalid|01/01/2024|Cancelled|\n",
            "|ORD00000001|    C000001|     Pune|    Grocery|      Sugar|  35430|2024-01-02|Completed|\n",
            "|ORD00000002|    C000002|     Pune|Electronics|     Mobile|  65358|2024-01-03|Completed|\n",
            "|ORD00000003|    C000003|Bangalore|Electronics|     Laptop|   5558|2024-01-04|Completed|\n",
            "|ORD00000004|    C000004|     Pune|       Home|Airpurifier|  33659|2024-01-05|Completed|\n",
            "|ORD00000005|    C000005|    Delhi|    Fashion|      Jeans|   8521|2024-01-06|Completed|\n",
            "|ORD00000006|    C000006|    Delhi|    Grocery|      Sugar|  42383|2024-01-07|Completed|\n",
            "|ORD00000007|    C000007|     Pune|    Grocery|       Rice|  45362|2024-01-08|Completed|\n",
            "|ORD00000008|    C000008|Bangalore|    Fashion|      Jeans|  10563|2024-01-09|Completed|\n",
            "|ORD00000009|    C000009|  Kolkata|Electronics|     Laptop|  63715|2024-01-10|Completed|\n",
            "|ORD00000010|    C000010|Bangalore|    Grocery|      Sugar|  66576|2024-01-11|Completed|\n",
            "|ORD00000011|    C000011|  Kolkata|Electronics|     Tablet|  50318|12/01/2024|Completed|\n",
            "|ORD00000012|    C000012|Bangalore|    Grocery|      Sugar|  84768|2024-01-13|Completed|\n",
            "|ORD00000013|    C000013|     Pune|    Fashion|     Tshirt|  79121|2024/01/14|Completed|\n",
            "|ORD00000014|    C000014|   Mumbai|Electronics|     Tablet|  79469|2024-01-15|Completed|\n",
            "|ORD00000015|    C000015|     Pune|Electronics|     Mobile|  81018|2024-01-16|Completed|\n",
            "|ORD00000016|    C000016|   Mumbai|       Home|      Mixer|  64225|2024-01-17|Completed|\n",
            "|ORD00000017|    C000017|Bangalore|    Grocery|        Oil|  69582|2024-01-18|Completed|\n",
            "|ORD00000018|    C000018|  Kolkata|    Fashion|      Jeans|  50424|2024-01-19|Completed|\n",
            "|ORD00000019|    C000019|   Mumbai|Electronics|     Mobile|Invalid|2024-01-20|Completed|\n",
            "+-----------+-----------+---------+-----------+-----------+-------+----------+---------+\n",
            "only showing top 20 rows\n",
            "300000\n"
          ]
        }
      ],
      "source": [
        "df_cleaned_str.show()\n",
        "print(df_cleaned_str.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twRn9utS-s7f",
        "outputId": "fba5840e-df90-406d-bce5-4519ea9810b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-----------+---------+-----------+-----------+------+----------+---------+\n",
            "|   order_id|customer_id|     city|   category|    product|amount|order_date|   status|\n",
            "+-----------+-----------+---------+-----------+-----------+------+----------+---------+\n",
            "|ORD00000000|    C000000|Hyderabad|    Grocery|        Oil|     0|2024-01-01|Cancelled|\n",
            "|ORD00000001|    C000001|     Pune|    Grocery|      Sugar| 35430|2024-01-02|Completed|\n",
            "|ORD00000002|    C000002|     Pune|Electronics|     Mobile| 65358|2024-01-03|Completed|\n",
            "|ORD00000003|    C000003|Bangalore|Electronics|     Laptop|  5558|2024-01-04|Completed|\n",
            "|ORD00000004|    C000004|     Pune|       Home|Airpurifier| 33659|2024-01-05|Completed|\n",
            "|ORD00000005|    C000005|    Delhi|    Fashion|      Jeans|  8521|2024-01-06|Completed|\n",
            "|ORD00000006|    C000006|    Delhi|    Grocery|      Sugar| 42383|2024-01-07|Completed|\n",
            "|ORD00000007|    C000007|     Pune|    Grocery|       Rice| 45362|2024-01-08|Completed|\n",
            "|ORD00000008|    C000008|Bangalore|    Fashion|      Jeans| 10563|2024-01-09|Completed|\n",
            "|ORD00000009|    C000009|  Kolkata|Electronics|     Laptop| 63715|2024-01-10|Completed|\n",
            "|ORD00000010|    C000010|Bangalore|    Grocery|      Sugar| 66576|2024-01-11|Completed|\n",
            "|ORD00000011|    C000011|  Kolkata|Electronics|     Tablet| 50318|2024-01-12|Completed|\n",
            "|ORD00000012|    C000012|Bangalore|    Grocery|      Sugar| 84768|2024-01-13|Completed|\n",
            "|ORD00000013|    C000013|     Pune|    Fashion|     Tshirt| 79121|2024-01-14|Completed|\n",
            "|ORD00000014|    C000014|   Mumbai|Electronics|     Tablet| 79469|2024-01-15|Completed|\n",
            "|ORD00000015|    C000015|     Pune|Electronics|     Mobile| 81018|2024-01-16|Completed|\n",
            "|ORD00000016|    C000016|   Mumbai|       Home|      Mixer| 64225|2024-01-17|Completed|\n",
            "|ORD00000017|    C000017|Bangalore|    Grocery|        Oil| 69582|2024-01-18|Completed|\n",
            "|ORD00000018|    C000018|  Kolkata|    Fashion|      Jeans| 50424|2024-01-19|Completed|\n",
            "|ORD00000019|    C000019|   Mumbai|Electronics|     Mobile|     0|2024-01-20|Completed|\n",
            "+-----------+-----------+---------+-----------+-----------+------+----------+---------+\n",
            "only showing top 20 rows\n",
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- amount: integer (nullable = true)\n",
            " |-- order_date: date (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "numeric_price_str = regexp_extract(col(\"amount\"), r\"(\\d+)\", 0)\n",
        "df_error_free = df_cleaned_str.withColumn(\"amount\",when((numeric_price_str == \"\") | numeric_price_str.isNull(), lit(0))\n",
        ".otherwise(numeric_price_str.cast('int')))\\\n",
        ".withColumn(\n",
        "    \"order_date\",\n",
        "    coalesce(\n",
        "        to_date(try_to_timestamp(col(\"order_date\"), lit(\"yyyy-MM-dd\"))),\n",
        "        to_date(try_to_timestamp(col(\"order_date\"), lit(\"dd/MM/yyyy\"))),\n",
        "        to_date(try_to_timestamp(col(\"order_date\"), lit(\"yyyy/MM/dd\")))\n",
        "    )\n",
        ")\n",
        "\n",
        "df_error_free.show()\n",
        "df_error_free.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V05aTOyxBNps"
      },
      "source": [
        "PHASE 3 – Data Validation\n",
        "1. Count how many records had invalid amounts.\n",
        "2. Count how many records had invalid dates.\n",
        "3. Identify duplicate order_id values.\n",
        "4. Remove duplicates using order_id.\n",
        "5. Filter only records with:\n",
        "\n",
        "status = \"Completed\"\n",
        "\n",
        "6. Record row counts at every stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_8QTCRQ_lGw",
        "outputId": "580a0b1a-0233-441c-8630-bc1369ee85b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Count before cleaning: 300000\n",
            "Count after cleaning: 297405\n"
          ]
        }
      ],
      "source": [
        "print(f\"Count before cleaning: {df_error_free.count()}\")\n",
        "\n",
        "df_valid = df_error_free.dropna(subset=[\"amount\", \"order_date\"])\n",
        "\n",
        "print(f\"Count after cleaning: {df_valid.count()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doOg46B3ALNm",
        "outputId": "ed4e8686-f3b7-4bea-bcfd-ff6a6fe2c5d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Count after dropping duplicates: 297405\n"
          ]
        }
      ],
      "source": [
        "df_clean = df_valid.dropDuplicates([\"order_id\"])\n",
        "\n",
        "print(f\"Count after dropping duplicates: {df_clean.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-ev8EIEAX-Y",
        "outputId": "dc3f7b45-997e-4c6b-f327-53bb6ffea455"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Count after filtering: 282535\n"
          ]
        }
      ],
      "source": [
        "df_completed = df_clean.filter(col(\"status\") == \"Completed\")\n",
        "\n",
        "print(f\"Count after filtering: {df_completed.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dR9UJ7wRCAzC"
      },
      "source": [
        "PHASE 4 – Performance Engineering\n",
        "1. Check the number of partitions.\n",
        "\n",
        "2. Run a groupBy on city and calculate total revenue.\n",
        "\n",
        "3. Use:\n",
        "\n",
        "explain(True)\n",
        "\n",
        "to analyze execution.\n",
        "\n",
        "4. Identify where shuffle happens.\n",
        "\n",
        "5. Repartition the dataset by city.\n",
        "\n",
        "6. Compare execution plans before and after repartition.\n",
        "This phase exists to demonstrate understanding of Spark internals, not just outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJsaMQRXBxRl",
        "outputId": "cd3b7df5-bab5-4635-9675-49f8ea90236f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of partitions before repartition: 2\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of partitions before repartition: {df_completed.rdd.getNumPartitions()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh380nCwCMYn",
        "outputId": "c3e4cf31-970d-40a3-f0be-e90db89f7844"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+-------------+\n",
            "|     city|total_revenue|\n",
            "+---------+-------------+\n",
            "|Bangalore|   1595093850|\n",
            "|  Chennai|   1594968796|\n",
            "|   Mumbai|   1592819957|\n",
            "|  Kolkata|   1589960718|\n",
            "|     Pune|   1611302685|\n",
            "|    Delhi|   1602686184|\n",
            "|Hyderabad|   1609260488|\n",
            "+---------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "city_revenue = df_completed.groupBy(\"city\").agg(spark_sum(\"amount\").alias(\"total_revenue\"))\n",
        "city_revenue.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvFONP7bCW-s",
        "outputId": "2e383574-4536-4430-bfb7-bd58e77ce4b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Aggregate ['city], ['city, 'sum('amount) AS total_revenue#859]\n",
            "+- Filter (status#492 = Completed)\n",
            "   +- Deduplicate [order_id#155]\n",
            "      +- Filter atleastnnonnulls(2, amount#571, order_date#572)\n",
            "         +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, amount#571, coalesce(to_date(try_to_timestamp(order_date#161, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#161, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#161, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS order_date#572, status#492]\n",
            "            +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, CASE WHEN ((regexp_extract(amount#491, (\\d+), 0) = ) OR isnull(regexp_extract(amount#491, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#491, (\\d+), 0) as int) END AS amount#571, order_date#161, status#492]\n",
            "               +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, amount#491, order_date#161, initcap(trim(status#162, None)) AS status#492]\n",
            "                  +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, initcap(trim(amount#160, None)) AS amount#491, order_date#161, status#162]\n",
            "                     +- Project [order_id#155, customer_id#156, city#488, category#489, initcap(trim(product#159, None)) AS product#490, amount#160, order_date#161, status#162]\n",
            "                        +- Project [order_id#155, customer_id#156, city#488, initcap(trim(category#158, None)) AS category#489, product#159, amount#160, order_date#161, status#162]\n",
            "                           +- Project [order_id#155, customer_id#156, initcap(trim(city#157, None)) AS city#488, category#158, product#159, amount#160, order_date#161, status#162]\n",
            "                              +- Relation [order_id#155,customer_id#156,city#157,category#158,product#159,amount#160,order_date#161,status#162] csv\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, total_revenue: bigint\n",
            "Aggregate [city#488], [city#488, sum(amount#571) AS total_revenue#859L]\n",
            "+- Filter (status#492 = Completed)\n",
            "   +- Deduplicate [order_id#155]\n",
            "      +- Filter atleastnnonnulls(2, amount#571, order_date#572)\n",
            "         +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, amount#571, coalesce(to_date(try_to_timestamp(order_date#161, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#161, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#161, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS order_date#572, status#492]\n",
            "            +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, CASE WHEN ((regexp_extract(amount#491, (\\d+), 0) = ) OR isnull(regexp_extract(amount#491, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#491, (\\d+), 0) as int) END AS amount#571, order_date#161, status#492]\n",
            "               +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, amount#491, order_date#161, initcap(trim(status#162, None)) AS status#492]\n",
            "                  +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, initcap(trim(amount#160, None)) AS amount#491, order_date#161, status#162]\n",
            "                     +- Project [order_id#155, customer_id#156, city#488, category#489, initcap(trim(product#159, None)) AS product#490, amount#160, order_date#161, status#162]\n",
            "                        +- Project [order_id#155, customer_id#156, city#488, initcap(trim(category#158, None)) AS category#489, product#159, amount#160, order_date#161, status#162]\n",
            "                           +- Project [order_id#155, customer_id#156, initcap(trim(city#157, None)) AS city#488, category#158, product#159, amount#160, order_date#161, status#162]\n",
            "                              +- Relation [order_id#155,customer_id#156,city#157,category#158,product#159,amount#160,order_date#161,status#162] csv\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Aggregate [city#939], [city#939, sum(amount#945) AS total_revenue#859L]\n",
            "+- Project [city#939, amount#945]\n",
            "   +- Filter (isnotnull(status#949) AND (status#949 = Completed))\n",
            "      +- Aggregate [order_id#155], [first(city#488, false) AS city#939, first(amount#571, false) AS amount#945, first(status#492, false) AS status#949]\n",
            "         +- Project [order_id#155, city#488, CASE WHEN ((regexp_extract(amount#491, (\\d+), 0) = ) OR isnull(regexp_extract(amount#491, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#491, (\\d+), 0) as int) END AS amount#571, status#492]\n",
            "            +- Project [order_id#155, initcap(trim(city#157, None)) AS city#488, initcap(trim(amount#160, None)) AS amount#491, initcap(trim(status#162, None)) AS status#492]\n",
            "               +- Filter atleastnnonnulls(2, CASE WHEN ((regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0) = ) OR isnull(regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0) as int) END, coalesce(cast(gettimestamp(order_date#161, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)))\n",
            "                  +- Relation [order_id#155,customer_id#156,city#157,category#158,product#159,amount#160,order_date#161,status#162] csv\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=true\n",
            "+- == Final Plan ==\n",
            "   ResultQueryStage 2\n",
            "   +- *(4) HashAggregate(keys=[city#939], functions=[sum(amount#945)], output=[city#939, total_revenue#859L])\n",
            "      +- AQEShuffleRead coalesced\n",
            "         +- ShuffleQueryStage 1\n",
            "            +- Exchange hashpartitioning(city#939, 200), ENSURE_REQUIREMENTS, [plan_id=1166]\n",
            "               +- *(3) HashAggregate(keys=[city#939], functions=[partial_sum(amount#945)], output=[city#939, sum#951L])\n",
            "                  +- *(3) Project [city#939, amount#945]\n",
            "                     +- *(3) Filter (isnotnull(status#949) AND (status#949 = Completed))\n",
            "                        +- SortAggregate(key=[order_id#155], functions=[first(city#488, false), first(amount#571, false), first(status#492, false)], output=[city#939, amount#945, status#949])\n",
            "                           +- *(2) Sort [order_id#155 ASC NULLS FIRST], false, 0\n",
            "                              +- AQEShuffleRead coalesced\n",
            "                                 +- ShuffleQueryStage 0\n",
            "                                    +- Exchange hashpartitioning(order_id#155, 200), ENSURE_REQUIREMENTS, [plan_id=1101]\n",
            "                                       +- SortAggregate(key=[order_id#155], functions=[partial_first(city#488, false), partial_first(amount#571, false), partial_first(status#492, false)], output=[order_id#155, first#958, valueSet#959, first#960, valueSet#961, first#962, valueSet#963])\n",
            "                                          +- *(1) Sort [order_id#155 ASC NULLS FIRST], false, 0\n",
            "                                             +- *(1) Project [order_id#155, city#488, CASE WHEN ((regexp_extract(amount#491, (\\d+), 0) = ) OR isnull(regexp_extract(amount#491, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#491, (\\d+), 0) as int) END AS amount#571, status#492]\n",
            "                                                +- *(1) Project [order_id#155, initcap(trim(city#157, None)) AS city#488, initcap(trim(amount#160, None)) AS amount#491, initcap(trim(status#162, None)) AS status#492]\n",
            "                                                   +- *(1) Filter atleastnnonnulls(2, CASE WHEN ((regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0) = ) OR isnull(regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0) as int) END, coalesce(cast(gettimestamp(order_date#161, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)))\n",
            "                                                      +- FileScan csv [order_id#155,city#157,amount#160,order_date#161,status#162] Batched: false, DataFilters: [atleastnnonnulls(2, CASE WHEN ((regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0) = ) OR..., Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:string,city:string,amount:string,order_date:string,status:string>\n",
            "+- == Initial Plan ==\n",
            "   HashAggregate(keys=[city#939], functions=[sum(amount#945)], output=[city#939, total_revenue#859L])\n",
            "   +- Exchange hashpartitioning(city#939, 200), ENSURE_REQUIREMENTS, [plan_id=1067]\n",
            "      +- HashAggregate(keys=[city#939], functions=[partial_sum(amount#945)], output=[city#939, sum#951L])\n",
            "         +- Project [city#939, amount#945]\n",
            "            +- Filter (isnotnull(status#949) AND (status#949 = Completed))\n",
            "               +- SortAggregate(key=[order_id#155], functions=[first(city#488, false), first(amount#571, false), first(status#492, false)], output=[city#939, amount#945, status#949])\n",
            "                  +- Sort [order_id#155 ASC NULLS FIRST], false, 0\n",
            "                     +- Exchange hashpartitioning(order_id#155, 200), ENSURE_REQUIREMENTS, [plan_id=1060]\n",
            "                        +- SortAggregate(key=[order_id#155], functions=[partial_first(city#488, false), partial_first(amount#571, false), partial_first(status#492, false)], output=[order_id#155, first#958, valueSet#959, first#960, valueSet#961, first#962, valueSet#963])\n",
            "                           +- Sort [order_id#155 ASC NULLS FIRST], false, 0\n",
            "                              +- Project [order_id#155, city#488, CASE WHEN ((regexp_extract(amount#491, (\\d+), 0) = ) OR isnull(regexp_extract(amount#491, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#491, (\\d+), 0) as int) END AS amount#571, status#492]\n",
            "                                 +- Project [order_id#155, initcap(trim(city#157, None)) AS city#488, initcap(trim(amount#160, None)) AS amount#491, initcap(trim(status#162, None)) AS status#492]\n",
            "                                    +- Filter atleastnnonnulls(2, CASE WHEN ((regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0) = ) OR isnull(regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0) as int) END, coalesce(cast(gettimestamp(order_date#161, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)))\n",
            "                                       +- FileScan csv [order_id#155,city#157,amount#160,order_date#161,status#162] Batched: false, DataFilters: [atleastnnonnulls(2, CASE WHEN ((regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0) = ) OR..., Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:string,city:string,amount:string,order_date:string,status:string>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "city_revenue.explain(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvzzSePmEaCT"
      },
      "source": [
        "We can see the shuffle happens in Physical Plan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41d214fd",
        "outputId": "f8d4eefd-d7b9-4ffc-89fc-71c4f0ee8eb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of partitions after repartitioning by city: 3\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "df_repartitioned = df_completed.repartition('city')\n",
        "print(f\"Number of partitions after repartitioning by city: {df_repartitioned.rdd.getNumPartitions()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wIgHGBEEzAm",
        "outputId": "9512bff7-cae4-4ba1-d213-445306ebfbfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'RepartitionByExpression ['city]\n",
            "+- Filter (status#492 = Completed)\n",
            "   +- Deduplicate [order_id#155]\n",
            "      +- Filter atleastnnonnulls(2, amount#571, order_date#572)\n",
            "         +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, amount#571, coalesce(to_date(try_to_timestamp(order_date#161, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#161, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#161, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS order_date#572, status#492]\n",
            "            +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, CASE WHEN ((regexp_extract(amount#491, (\\d+), 0) = ) OR isnull(regexp_extract(amount#491, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#491, (\\d+), 0) as int) END AS amount#571, order_date#161, status#492]\n",
            "               +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, amount#491, order_date#161, initcap(trim(status#162, None)) AS status#492]\n",
            "                  +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, initcap(trim(amount#160, None)) AS amount#491, order_date#161, status#162]\n",
            "                     +- Project [order_id#155, customer_id#156, city#488, category#489, initcap(trim(product#159, None)) AS product#490, amount#160, order_date#161, status#162]\n",
            "                        +- Project [order_id#155, customer_id#156, city#488, initcap(trim(category#158, None)) AS category#489, product#159, amount#160, order_date#161, status#162]\n",
            "                           +- Project [order_id#155, customer_id#156, initcap(trim(city#157, None)) AS city#488, category#158, product#159, amount#160, order_date#161, status#162]\n",
            "                              +- Relation [order_id#155,customer_id#156,city#157,category#158,product#159,amount#160,order_date#161,status#162] csv\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "order_id: string, customer_id: string, city: string, category: string, product: string, amount: int, order_date: date, status: string\n",
            "RepartitionByExpression [city#488]\n",
            "+- Filter (status#492 = Completed)\n",
            "   +- Deduplicate [order_id#155]\n",
            "      +- Filter atleastnnonnulls(2, amount#571, order_date#572)\n",
            "         +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, amount#571, coalesce(to_date(try_to_timestamp(order_date#161, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#161, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#161, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS order_date#572, status#492]\n",
            "            +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, CASE WHEN ((regexp_extract(amount#491, (\\d+), 0) = ) OR isnull(regexp_extract(amount#491, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#491, (\\d+), 0) as int) END AS amount#571, order_date#161, status#492]\n",
            "               +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, amount#491, order_date#161, initcap(trim(status#162, None)) AS status#492]\n",
            "                  +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, initcap(trim(amount#160, None)) AS amount#491, order_date#161, status#162]\n",
            "                     +- Project [order_id#155, customer_id#156, city#488, category#489, initcap(trim(product#159, None)) AS product#490, amount#160, order_date#161, status#162]\n",
            "                        +- Project [order_id#155, customer_id#156, city#488, initcap(trim(category#158, None)) AS category#489, product#159, amount#160, order_date#161, status#162]\n",
            "                           +- Project [order_id#155, customer_id#156, initcap(trim(city#157, None)) AS city#488, category#158, product#159, amount#160, order_date#161, status#162]\n",
            "                              +- Relation [order_id#155,customer_id#156,city#157,category#158,product#159,amount#160,order_date#161,status#162] csv\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "RepartitionByExpression [city#999]\n",
            "+- Filter (isnotnull(status#1009) AND (status#1009 = Completed))\n",
            "   +- Aggregate [order_id#155], [order_id#155, first(customer_id#156, false) AS customer_id#997, first(city#488, false) AS city#999, first(category#489, false) AS category#1001, first(product#490, false) AS product#1003, first(amount#571, false) AS amount#1005, first(order_date#572, false) AS order_date#1007, first(status#492, false) AS status#1009]\n",
            "      +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, CASE WHEN ((regexp_extract(amount#491, (\\d+), 0) = ) OR isnull(regexp_extract(amount#491, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#491, (\\d+), 0) as int) END AS amount#571, coalesce(cast(gettimestamp(order_date#161, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date#572, status#492]\n",
            "         +- Project [order_id#155, customer_id#156, initcap(trim(city#157, None)) AS city#488, initcap(trim(category#158, None)) AS category#489, initcap(trim(product#159, None)) AS product#490, initcap(trim(amount#160, None)) AS amount#491, order_date#161, initcap(trim(status#162, None)) AS status#492]\n",
            "            +- Filter atleastnnonnulls(2, CASE WHEN ((regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0) = ) OR isnull(regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0) as int) END, coalesce(cast(gettimestamp(order_date#161, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)))\n",
            "               +- Relation [order_id#155,customer_id#156,city#157,category#158,product#159,amount#160,order_date#161,status#162] csv\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=true\n",
            "+- == Final Plan ==\n",
            "   ResultQueryStage 2\n",
            "   +- AQEShuffleRead coalesced\n",
            "      +- ShuffleQueryStage 1\n",
            "         +- Exchange hashpartitioning(city#999, 200), REPARTITION_BY_COL, [plan_id=1303]\n",
            "            +- *(3) Filter (isnotnull(status#1009) AND (status#1009 = Completed))\n",
            "               +- SortAggregate(key=[order_id#155], functions=[first(customer_id#156, false), first(city#488, false), first(category#489, false), first(product#490, false), first(amount#571, false), first(order_date#572, false), first(status#492, false)], output=[order_id#155, customer_id#997, city#999, category#1001, product#1003, amount#1005, order_date#1007, status#1009])\n",
            "                  +- *(2) Sort [order_id#155 ASC NULLS FIRST], false, 0\n",
            "                     +- AQEShuffleRead coalesced\n",
            "                        +- ShuffleQueryStage 0\n",
            "                           +- Exchange hashpartitioning(order_id#155, 200), ENSURE_REQUIREMENTS, [plan_id=1256]\n",
            "                              +- SortAggregate(key=[order_id#155], functions=[partial_first(customer_id#156, false), partial_first(city#488, false), partial_first(category#489, false), partial_first(product#490, false), partial_first(amount#571, false), partial_first(order_date#572, false), partial_first(status#492, false)], output=[order_id#155, first#1024, valueSet#1025, first#1026, valueSet#1027, first#1028, valueSet#1029, first#1030, valueSet#1031, first#1032, valueSet#1033, first#1034, valueSet#1035, first#1036, valueSet#1037])\n",
            "                                 +- *(1) Sort [order_id#155 ASC NULLS FIRST], false, 0\n",
            "                                    +- *(1) Project [order_id#155, customer_id#156, city#488, category#489, product#490, CASE WHEN ((regexp_extract(amount#491, (\\d+), 0) = ) OR isnull(regexp_extract(amount#491, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#491, (\\d+), 0) as int) END AS amount#571, coalesce(cast(gettimestamp(order_date#161, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date#572, status#492]\n",
            "                                       +- *(1) Project [order_id#155, customer_id#156, initcap(trim(city#157, None)) AS city#488, initcap(trim(category#158, None)) AS category#489, initcap(trim(product#159, None)) AS product#490, initcap(trim(amount#160, None)) AS amount#491, order_date#161, initcap(trim(status#162, None)) AS status#492]\n",
            "                                          +- *(1) Filter atleastnnonnulls(2, CASE WHEN ((regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0) = ) OR isnull(regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0) as int) END, coalesce(cast(gettimestamp(order_date#161, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)))\n",
            "                                             +- FileScan csv [order_id#155,customer_id#156,city#157,category#158,product#159,amount#160,order_date#161,status#162] Batched: false, DataFilters: [atleastnnonnulls(2, CASE WHEN ((regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0) = ) OR..., Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:string,customer_id:string,city:string,category:string,product:string,amount:strin...\n",
            "+- == Initial Plan ==\n",
            "   Exchange hashpartitioning(city#999, 200), REPARTITION_BY_COL, [plan_id=1227]\n",
            "   +- Filter (isnotnull(status#1009) AND (status#1009 = Completed))\n",
            "      +- SortAggregate(key=[order_id#155], functions=[first(customer_id#156, false), first(city#488, false), first(category#489, false), first(product#490, false), first(amount#571, false), first(order_date#572, false), first(status#492, false)], output=[order_id#155, customer_id#997, city#999, category#1001, product#1003, amount#1005, order_date#1007, status#1009])\n",
            "         +- Sort [order_id#155 ASC NULLS FIRST], false, 0\n",
            "            +- Exchange hashpartitioning(order_id#155, 200), ENSURE_REQUIREMENTS, [plan_id=1223]\n",
            "               +- SortAggregate(key=[order_id#155], functions=[partial_first(customer_id#156, false), partial_first(city#488, false), partial_first(category#489, false), partial_first(product#490, false), partial_first(amount#571, false), partial_first(order_date#572, false), partial_first(status#492, false)], output=[order_id#155, first#1024, valueSet#1025, first#1026, valueSet#1027, first#1028, valueSet#1029, first#1030, valueSet#1031, first#1032, valueSet#1033, first#1034, valueSet#1035, first#1036, valueSet#1037])\n",
            "                  +- Sort [order_id#155 ASC NULLS FIRST], false, 0\n",
            "                     +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, CASE WHEN ((regexp_extract(amount#491, (\\d+), 0) = ) OR isnull(regexp_extract(amount#491, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#491, (\\d+), 0) as int) END AS amount#571, coalesce(cast(gettimestamp(order_date#161, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date#572, status#492]\n",
            "                        +- Project [order_id#155, customer_id#156, initcap(trim(city#157, None)) AS city#488, initcap(trim(category#158, None)) AS category#489, initcap(trim(product#159, None)) AS product#490, initcap(trim(amount#160, None)) AS amount#491, order_date#161, initcap(trim(status#162, None)) AS status#492]\n",
            "                           +- Filter atleastnnonnulls(2, CASE WHEN ((regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0) = ) OR isnull(regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0) as int) END, coalesce(cast(gettimestamp(order_date#161, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)))\n",
            "                              +- FileScan csv [order_id#155,customer_id#156,city#157,category#158,product#159,amount#160,order_date#161,status#162] Batched: false, DataFilters: [atleastnnonnulls(2, CASE WHEN ((regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0) = ) OR..., Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:string,customer_id:string,city:string,category:string,product:string,amount:strin...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_repartitioned.explain(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aadf145c"
      },
      "source": [
        "\n",
        "*   The `df_completed` DataFrame was repartitioned by the `city` column, resulting in the `df_repartitioned` DataFrame having 3 partitions.\n",
        "*   Following repartitioning, the total revenue for each city was successfully calculated from `df_repartitioned`, displaying revenues for seven cities: Bangalore, Chennai, Mumbai, Kolkata, Pune, Delhi, and Hyderabad.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrQ1HuUlFOtH"
      },
      "source": [
        "PHASE 5 – Analytics\n",
        "Using the cleaned dataset:\n",
        "1. Total revenue per city.\n",
        "2. Total revenue per category.\n",
        "3. Average order value per city.\n",
        "4. Top 10 products by revenue.\n",
        "5. Cities sorted by revenue descending."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nT1KGyLFuqf",
        "outputId": "3cc82cf0-69e9-4cbc-8836-b37e44b3669b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+-------------+\n",
            "|     city|total_revenue|\n",
            "+---------+-------------+\n",
            "|Bangalore|   1595093850|\n",
            "|  Chennai|   1594968796|\n",
            "|   Mumbai|   1592819957|\n",
            "|  Kolkata|   1589960718|\n",
            "|     Pune|   1611302685|\n",
            "|    Delhi|   1602686184|\n",
            "|Hyderabad|   1609260488|\n",
            "+---------+-------------+\n",
            "\n",
            "+-----------+-------------+\n",
            "|   category|total_revenue|\n",
            "+-----------+-------------+\n",
            "|       Home|   2808429137|\n",
            "|    Fashion|   2774452711|\n",
            "|    Grocery|   2806779336|\n",
            "|Electronics|   2806431494|\n",
            "+-----------+-------------+\n",
            "\n",
            "+---------+-------------------+\n",
            "|     city|average_order_value|\n",
            "+---------+-------------------+\n",
            "|Bangalore| 39910.272224585286|\n",
            "|  Chennai|  39488.22252481989|\n",
            "|   Mumbai| 39540.748131966335|\n",
            "|  Kolkata| 39558.149876844225|\n",
            "|     Pune|  39758.74565104745|\n",
            "|    Delhi| 39603.790254027874|\n",
            "|Hyderabad|  39533.74165970619|\n",
            "+---------+-------------------+\n",
            "\n",
            "+-----------+-------------+\n",
            "|    product|total_revenue|\n",
            "+-----------+-------------+\n",
            "|        Oil|    943593995|\n",
            "|     Laptop|    943181599|\n",
            "|     Tablet|    939989279|\n",
            "|     Vacuum|    939626254|\n",
            "|      Mixer|    937113183|\n",
            "|       Rice|    934345709|\n",
            "|Airpurifier|    931689700|\n",
            "|      Jeans|    930922473|\n",
            "|      Sugar|    928839632|\n",
            "|      Shoes|    926563627|\n",
            "+-----------+-------------+\n",
            "\n",
            "+---------+-------------+\n",
            "|     city|total_revenue|\n",
            "+---------+-------------+\n",
            "|     Pune|   1611302685|\n",
            "|Hyderabad|   1609260488|\n",
            "|    Delhi|   1602686184|\n",
            "|Bangalore|   1595093850|\n",
            "|  Chennai|   1594968796|\n",
            "|   Mumbai|   1592819957|\n",
            "|  Kolkata|   1589960718|\n",
            "+---------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "city_revenue = df_completed.groupBy(\"city\").agg(spark_sum(\"amount\").alias(\"total_revenue\"))\n",
        "city_revenue.show()\n",
        "\n",
        "catgory_revenue = df_completed.groupBy(\"category\").agg(spark_sum(\"amount\").alias(\"total_revenue\"))\n",
        "catgory_revenue.show()\n",
        "\n",
        "city_average = df_completed.groupBy(\"city\").agg(avg(\"amount\").alias(\"average_order_value\"))\n",
        "city_average.show()\n",
        "\n",
        "top_10_products = df_completed.groupBy(\"product\").agg(spark_sum(\"amount\").alias(\"total_revenue\"))\n",
        "top_10_products = top_10_products.orderBy(desc(\"total_revenue\")).limit(10)\n",
        "top_10_products.show()\n",
        "\n",
        "city_revenue_sorted = city_revenue.orderBy(desc(\"total_revenue\"))\n",
        "city_revenue_sorted.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6jK-X9VGZGa"
      },
      "source": [
        "PHASE 6 – Window Functions\n",
        "1. Rank cities by revenue.\n",
        "2. Rank products inside each category by revenue.\n",
        "3. Find the top product for every category.\n",
        "4. Identify the top 3 performing cities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVxxoKuSGOHM",
        "outputId": "a59f7d76-62f3-4903-f59c-f16f0c72aac1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+-------------+----+\n",
            "|     city|total_revenue|rank|\n",
            "+---------+-------------+----+\n",
            "|     Pune|   1611302685|   1|\n",
            "|Hyderabad|   1609260488|   2|\n",
            "|    Delhi|   1602686184|   3|\n",
            "|Bangalore|   1595093850|   4|\n",
            "|  Chennai|   1594968796|   5|\n",
            "|   Mumbai|   1592819957|   6|\n",
            "|  Kolkata|   1589960718|   7|\n",
            "+---------+-------------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "city_window = Window.orderBy(col(\"total_revenue\").desc())\n",
        "df_city_rank = city_revenue.withColumn(\"rank\", rank().over(city_window))\n",
        "df_city_rank.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rhzwpc_jHdsT",
        "outputId": "3c1484e3-b82f-46ac-d5b9-890299104a87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-------------+----+\n",
            "|    product|total_revenue|rank|\n",
            "+-----------+-------------+----+\n",
            "|        Oil|    943593995|   1|\n",
            "|     Laptop|    943181599|   2|\n",
            "|     Tablet|    939989279|   3|\n",
            "|     Vacuum|    939626254|   4|\n",
            "|      Mixer|    937113183|   5|\n",
            "|       Rice|    934345709|   6|\n",
            "|Airpurifier|    931689700|   7|\n",
            "|      Jeans|    930922473|   8|\n",
            "|      Sugar|    928839632|   9|\n",
            "|      Shoes|    926563627|  10|\n",
            "+-----------+-------------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_product_rank = top_10_products.withColumn(\"rank\", rank().over(city_window))\n",
        "df_product_rank.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUbqejxcICdG",
        "outputId": "dbe06108-0a23-46f7-946b-02bb8eb3cf1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-------+-------------+----+\n",
            "|   category|product|total_revenue|rank|\n",
            "+-----------+-------+-------------+----+\n",
            "|Electronics| Laptop|    943181599|   1|\n",
            "|    Fashion|  Jeans|    930922473|   1|\n",
            "|    Grocery|    Oil|    943593995|   1|\n",
            "|       Home| Vacuum|    939626254|   1|\n",
            "+-----------+-------+-------------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "product_revenue_per_category = df_completed.groupBy(\"category\", \"product\").agg(spark_sum(\"amount\").alias(\"total_revenue\"))\n",
        "\n",
        "category_product_window = Window.partitionBy(\"category\").orderBy(col(\"total_revenue\").desc())\n",
        "\n",
        "df_top_product_per_category = product_revenue_per_category.withColumn(\"rank\", rank().over(category_product_window))\n",
        "\n",
        "df_top_product_per_category.filter(col(\"rank\") == 1).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hwvlW-UHy15",
        "outputId": "c5647b0f-e6c9-409f-eeb4-409f2d9b1ff5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+-------------+----+\n",
            "|     city|total_revenue|rank|\n",
            "+---------+-------------+----+\n",
            "|     Pune|   1611302685|   1|\n",
            "|Hyderabad|   1609260488|   2|\n",
            "|    Delhi|   1602686184|   3|\n",
            "+---------+-------------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_city_rank.limit(3).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bF2eeoMJKAu"
      },
      "source": [
        "PHASE 7 – Broadcast Join\n",
        "A small lookup table is provided:\n",
        "\n",
        "city,region\n",
        "Delhi,North\n",
        "Mumbai,West\n",
        "Bangalore,South\n",
        "Hyderabad,South\n",
        "Pune,West\n",
        "Chennai,South\n",
        "Kolkata,East\n",
        "\n",
        "\n",
        "Tasks:\n",
        "1. Join the orders data with this city-region dataset.\n",
        "2. Apply broadcast join explicitly.\n",
        "3. Verify using the physical plan that:\n",
        "\n",
        "BroadcastHashJoin\n",
        "\n",
        "is used.\n",
        "\n",
        "4. Explain why broadcast join is efficient in this case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG69z3VkIAcn",
        "outputId": "d580f570-5bb0-488b-8fe3-ed997d2cfc96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+------+\n",
            "|     city|region|\n",
            "+---------+------+\n",
            "|    Delhi| North|\n",
            "|   Mumbai|  West|\n",
            "|Bangalore| South|\n",
            "|Hyderabad| South|\n",
            "|     Pune|  West|\n",
            "|  Chennai| South|\n",
            "|  Kolkata|  East|\n",
            "+---------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = [\n",
        "(\"Delhi\",\"North\"),\n",
        "(\"Mumbai\",\"West\"),\n",
        "(\"Bangalore\",\"South\"),\n",
        "(\"Hyderabad\",\"South\"),\n",
        "(\"Pune\",\"West\"),\n",
        "(\"Chennai\",\"South\"),\n",
        "(\"Kolkata\",\"East\"),\n",
        "]\n",
        "\n",
        "columns = [\"city\",\"region\"]\n",
        "\n",
        "df_city_lookup = spark.createDataFrame(data, columns)\n",
        "df_city_lookup.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mN1-YtLZKw_e",
        "outputId": "ab5967e9-97ea-475f-c21a-187797cc1674"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+-----------+-----------+-----------+-----------+------+----------+---------+------+\n",
            "|     city|   order_id|customer_id|   category|    product|amount|order_date|   status|region|\n",
            "+---------+-----------+-----------+-----------+-----------+------+----------+---------+------+\n",
            "|     Pune|ORD00000001|    C000001|    Grocery|      Sugar| 35430|2024-01-02|Completed|  West|\n",
            "|     Pune|ORD00000007|    C000007|    Grocery|       Rice| 45362|2024-01-08|Completed|  West|\n",
            "|Bangalore|ORD00000008|    C000008|    Fashion|      Jeans| 10563|2024-01-09|Completed| South|\n",
            "|Bangalore|ORD00000010|    C000010|    Grocery|      Sugar| 66576|2024-01-11|Completed| South|\n",
            "|  Kolkata|ORD00000011|    C000011|Electronics|     Tablet| 50318|2024-01-12|Completed|  East|\n",
            "|Bangalore|ORD00000012|    C000012|    Grocery|      Sugar| 84768|2024-01-13|Completed| South|\n",
            "|   Mumbai|ORD00000014|    C000014|Electronics|     Tablet| 79469|2024-01-15|Completed|  West|\n",
            "|     Pune|ORD00000015|    C000015|Electronics|     Mobile| 81018|2024-01-16|Completed|  West|\n",
            "|Bangalore|ORD00000017|    C000017|    Grocery|        Oil| 69582|2024-01-18|Completed| South|\n",
            "|   Mumbai|ORD00000019|    C000019|Electronics|     Mobile|     0|2024-01-20|Completed|  West|\n",
            "|   Mumbai|ORD00000022|    C000022|    Grocery|      Sugar| 48832|2024-01-23|Completed|  West|\n",
            "|Hyderabad|ORD00000023|    C000023|Electronics|     Mobile|    12|2024-01-24|Completed| South|\n",
            "|Bangalore|ORD00000024|    C000024|       Home|      Mixer| 18082|2024-01-25|Completed| South|\n",
            "|Bangalore|ORD00000025|    C000025|       Home|Airpurifier| 58248|2024-01-26|Completed| South|\n",
            "|   Mumbai|ORD00000028|    C000028|    Grocery|      Sugar| 70675|2024-01-29|Completed|  West|\n",
            "|     Pune|ORD00000030|    C000030|       Home|Airpurifier| 52112|2024-01-31|Completed|  West|\n",
            "|  Chennai|ORD00000031|    C000031|    Grocery|        Oil| 51151|2024-02-01|Completed| South|\n",
            "|  Chennai|ORD00000032|    C000032|       Home|     Vacuum| 75797|2024-02-02|Completed| South|\n",
            "|     Pune|ORD00000034|    C000034|    Grocery|      Sugar| 40915|2024-02-04|Completed|  West|\n",
            "|  Kolkata|ORD00000036|    C000036|    Grocery|        Oil| 29253|2024-02-06|Completed|  East|\n",
            "+---------+-----------+-----------+-----------+-----------+------+----------+---------+------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "joined_data = df_completed.join(df_city_lookup, on=\"city\", how=\"left\")\n",
        "joined_data.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2A5CDj33LuwD",
        "outputId": "9caedfb1-3cea-4ec4-a298-c38f91bb8aec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+-----------+-----------+-----------+-----------+------+----------+---------+------+\n",
            "|     city|   order_id|customer_id|   category|    product|amount|order_date|   status|region|\n",
            "+---------+-----------+-----------+-----------+-----------+------+----------+---------+------+\n",
            "|     Pune|ORD00000001|    C000001|    Grocery|      Sugar| 35430|2024-01-02|Completed|  West|\n",
            "|     Pune|ORD00000007|    C000007|    Grocery|       Rice| 45362|2024-01-08|Completed|  West|\n",
            "|Bangalore|ORD00000008|    C000008|    Fashion|      Jeans| 10563|2024-01-09|Completed| South|\n",
            "|Bangalore|ORD00000010|    C000010|    Grocery|      Sugar| 66576|2024-01-11|Completed| South|\n",
            "|  Kolkata|ORD00000011|    C000011|Electronics|     Tablet| 50318|2024-01-12|Completed|  East|\n",
            "|Bangalore|ORD00000012|    C000012|    Grocery|      Sugar| 84768|2024-01-13|Completed| South|\n",
            "|   Mumbai|ORD00000014|    C000014|Electronics|     Tablet| 79469|2024-01-15|Completed|  West|\n",
            "|     Pune|ORD00000015|    C000015|Electronics|     Mobile| 81018|2024-01-16|Completed|  West|\n",
            "|Bangalore|ORD00000017|    C000017|    Grocery|        Oil| 69582|2024-01-18|Completed| South|\n",
            "|   Mumbai|ORD00000019|    C000019|Electronics|     Mobile|     0|2024-01-20|Completed|  West|\n",
            "|   Mumbai|ORD00000022|    C000022|    Grocery|      Sugar| 48832|2024-01-23|Completed|  West|\n",
            "|Hyderabad|ORD00000023|    C000023|Electronics|     Mobile|    12|2024-01-24|Completed| South|\n",
            "|Bangalore|ORD00000024|    C000024|       Home|      Mixer| 18082|2024-01-25|Completed| South|\n",
            "|Bangalore|ORD00000025|    C000025|       Home|Airpurifier| 58248|2024-01-26|Completed| South|\n",
            "|   Mumbai|ORD00000028|    C000028|    Grocery|      Sugar| 70675|2024-01-29|Completed|  West|\n",
            "|     Pune|ORD00000030|    C000030|       Home|Airpurifier| 52112|2024-01-31|Completed|  West|\n",
            "|  Chennai|ORD00000031|    C000031|    Grocery|        Oil| 51151|2024-02-01|Completed| South|\n",
            "|  Chennai|ORD00000032|    C000032|       Home|     Vacuum| 75797|2024-02-02|Completed| South|\n",
            "|     Pune|ORD00000034|    C000034|    Grocery|      Sugar| 40915|2024-02-04|Completed|  West|\n",
            "|  Kolkata|ORD00000036|    C000036|    Grocery|        Oil| 29253|2024-02-06|Completed|  East|\n",
            "+---------+-----------+-----------+-----------+-----------+------+----------+---------+------+\n",
            "only showing top 20 rows\n",
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(LeftOuter, [city])\n",
            ":- Filter (status#492 = Completed)\n",
            ":  +- Deduplicate [order_id#155]\n",
            ":     +- Filter atleastnnonnulls(2, amount#571, order_date#572)\n",
            ":        +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, amount#571, coalesce(to_date(try_to_timestamp(order_date#161, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#161, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#161, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS order_date#572, status#492]\n",
            ":           +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, CASE WHEN ((regexp_extract(amount#491, (\\d+), 0) = ) OR isnull(regexp_extract(amount#491, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#491, (\\d+), 0) as int) END AS amount#571, order_date#161, status#492]\n",
            ":              +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, amount#491, order_date#161, initcap(trim(status#162, None)) AS status#492]\n",
            ":                 +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, initcap(trim(amount#160, None)) AS amount#491, order_date#161, status#162]\n",
            ":                    +- Project [order_id#155, customer_id#156, city#488, category#489, initcap(trim(product#159, None)) AS product#490, amount#160, order_date#161, status#162]\n",
            ":                       +- Project [order_id#155, customer_id#156, city#488, initcap(trim(category#158, None)) AS category#489, product#159, amount#160, order_date#161, status#162]\n",
            ":                          +- Project [order_id#155, customer_id#156, initcap(trim(city#157, None)) AS city#488, category#158, product#159, amount#160, order_date#161, status#162]\n",
            ":                             +- Relation [order_id#155,customer_id#156,city#157,category#158,product#159,amount#160,order_date#161,status#162] csv\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- LogicalRDD [city#2056, region#2057], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, order_id: string, customer_id: string, category: string, product: string, amount: int, order_date: date, status: string, region: string\n",
            "Project [city#488, order_id#155, customer_id#156, category#489, product#490, amount#571, order_date#572, status#492, region#2057]\n",
            "+- Join LeftOuter, (city#488 = city#2056)\n",
            "   :- Filter (status#492 = Completed)\n",
            "   :  +- Deduplicate [order_id#155]\n",
            "   :     +- Filter atleastnnonnulls(2, amount#571, order_date#572)\n",
            "   :        +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, amount#571, coalesce(to_date(try_to_timestamp(order_date#161, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#161, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(order_date#161, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS order_date#572, status#492]\n",
            "   :           +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, CASE WHEN ((regexp_extract(amount#491, (\\d+), 0) = ) OR isnull(regexp_extract(amount#491, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#491, (\\d+), 0) as int) END AS amount#571, order_date#161, status#492]\n",
            "   :              +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, amount#491, order_date#161, initcap(trim(status#162, None)) AS status#492]\n",
            "   :                 +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, initcap(trim(amount#160, None)) AS amount#491, order_date#161, status#162]\n",
            "   :                    +- Project [order_id#155, customer_id#156, city#488, category#489, initcap(trim(product#159, None)) AS product#490, amount#160, order_date#161, status#162]\n",
            "   :                       +- Project [order_id#155, customer_id#156, city#488, initcap(trim(category#158, None)) AS category#489, product#159, amount#160, order_date#161, status#162]\n",
            "   :                          +- Project [order_id#155, customer_id#156, initcap(trim(city#157, None)) AS city#488, category#158, product#159, amount#160, order_date#161, status#162]\n",
            "   :                             +- Relation [order_id#155,customer_id#156,city#157,category#158,product#159,amount#160,order_date#161,status#162] csv\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- LogicalRDD [city#2056, region#2057], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#2308, order_id#155, customer_id#2306, category#2310, product#2312, amount#2314, order_date#2316, status#2318, region#2057]\n",
            "+- Join LeftOuter, (city#2308 = city#2056), rightHint=(strategy=broadcast)\n",
            "   :- Filter (isnotnull(status#2318) AND (status#2318 = Completed))\n",
            "   :  +- Aggregate [order_id#155], [order_id#155, first(customer_id#156, false) AS customer_id#2306, first(city#488, false) AS city#2308, first(category#489, false) AS category#2310, first(product#490, false) AS product#2312, first(amount#571, false) AS amount#2314, first(order_date#572, false) AS order_date#2316, first(status#492, false) AS status#2318]\n",
            "   :     +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, CASE WHEN ((regexp_extract(amount#491, (\\d+), 0) = ) OR isnull(regexp_extract(amount#491, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#491, (\\d+), 0) as int) END AS amount#571, coalesce(cast(gettimestamp(order_date#161, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date#572, status#492]\n",
            "   :        +- Project [order_id#155, customer_id#156, initcap(trim(city#157, None)) AS city#488, initcap(trim(category#158, None)) AS category#489, initcap(trim(product#159, None)) AS product#490, initcap(trim(amount#160, None)) AS amount#491, order_date#161, initcap(trim(status#162, None)) AS status#492]\n",
            "   :           +- Filter atleastnnonnulls(2, CASE WHEN ((regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0) = ) OR isnull(regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0) as int) END, coalesce(cast(gettimestamp(order_date#161, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)))\n",
            "   :              +- Relation [order_id#155,customer_id#156,city#157,category#158,product#159,amount#160,order_date#161,status#162] csv\n",
            "   +- Filter isnotnull(city#2056)\n",
            "      +- LogicalRDD [city#2056, region#2057], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#2308, order_id#155, customer_id#2306, category#2310, product#2312, amount#2314, order_date#2316, status#2318, region#2057]\n",
            "   +- BroadcastHashJoin [city#2308], [city#2056], LeftOuter, BuildRight, false\n",
            "      :- Filter (isnotnull(status#2318) AND (status#2318 = Completed))\n",
            "      :  +- SortAggregate(key=[order_id#155], functions=[first(customer_id#156, false), first(city#488, false), first(category#489, false), first(product#490, false), first(amount#571, false), first(order_date#572, false), first(status#492, false)], output=[order_id#155, customer_id#2306, city#2308, category#2310, product#2312, amount#2314, order_date#2316, status#2318])\n",
            "      :     +- Sort [order_id#155 ASC NULLS FIRST], false, 0\n",
            "      :        +- Exchange hashpartitioning(order_id#155, 200), ENSURE_REQUIREMENTS, [plan_id=4322]\n",
            "      :           +- SortAggregate(key=[order_id#155], functions=[partial_first(customer_id#156, false), partial_first(city#488, false), partial_first(category#489, false), partial_first(product#490, false), partial_first(amount#571, false), partial_first(order_date#572, false), partial_first(status#492, false)], output=[order_id#155, first#2333, valueSet#2334, first#2335, valueSet#2336, first#2337, valueSet#2338, first#2339, valueSet#2340, first#2341, valueSet#2342, first#2343, valueSet#2344, first#2345, valueSet#2346])\n",
            "      :              +- Sort [order_id#155 ASC NULLS FIRST], false, 0\n",
            "      :                 +- Project [order_id#155, customer_id#156, city#488, category#489, product#490, CASE WHEN ((regexp_extract(amount#491, (\\d+), 0) = ) OR isnull(regexp_extract(amount#491, (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(amount#491, (\\d+), 0) as int) END AS amount#571, coalesce(cast(gettimestamp(order_date#161, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date#572, status#492]\n",
            "      :                    +- Project [order_id#155, customer_id#156, initcap(trim(city#157, None)) AS city#488, initcap(trim(category#158, None)) AS category#489, initcap(trim(product#159, None)) AS product#490, initcap(trim(amount#160, None)) AS amount#491, order_date#161, initcap(trim(status#162, None)) AS status#492]\n",
            "      :                       +- Filter atleastnnonnulls(2, CASE WHEN ((regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0) = ) OR isnull(regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0))) THEN 0 ELSE cast(regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0) as int) END, coalesce(cast(gettimestamp(order_date#161, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#161, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)))\n",
            "      :                          +- FileScan csv [order_id#155,customer_id#156,city#157,category#158,product#159,amount#160,order_date#161,status#162] Batched: false, DataFilters: [atleastnnonnulls(2, CASE WHEN ((regexp_extract(initcap(trim(amount#160, None)), (\\d+), 0) = ) OR..., Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:string,customer_id:string,city:string,category:string,product:string,amount:strin...\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=4327]\n",
            "         +- Filter isnotnull(city#2056)\n",
            "            +- Scan ExistingRDD[city#2056,region#2057]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "broadcasted_joined_data = df_completed.join(broadcast(df_city_lookup), on=\"city\", how=\"left\")\n",
        "broadcasted_joined_data.show()\n",
        "broadcasted_joined_data.explain(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQdXchKrNMKj"
      },
      "source": [
        " A broadcast join is often much better than a normal (shuffle) join when one dataset is significantly smaller than the other and fits in memory, as it avoids costly data shuffling across the network, allowing each node to perform the join locally, leading to significant performance gains, though normal joins remain better for large-to-large dataset scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0C0pxn0aNTEE"
      },
      "source": [
        "PHASE 8 – UDF\n",
        "\n",
        "Create a classification based on amount:\n",
        "\n",
        "amount >= 80000 → High\n",
        "amount >= 40000 → Medium\n",
        "else → Low\n",
        "\n",
        "Add a new column:\n",
        "\n",
        "order_value_category\n",
        "\n",
        "Analyze distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3d7gpivNSfM",
        "outputId": "c53e86cf-1db7-4b32-dc7a-0e65ff00baf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-----------+---------+-----------+-----------+------+----------+---------+--------------------+\n",
            "|   order_id|customer_id|     city|   category|    product|amount|order_date|   status|order_value_category|\n",
            "+-----------+-----------+---------+-----------+-----------+------+----------+---------+--------------------+\n",
            "|ORD00000001|    C000001|     Pune|    Grocery|      Sugar| 35430|2024-01-02|Completed|                 Low|\n",
            "|ORD00000007|    C000007|     Pune|    Grocery|       Rice| 45362|2024-01-08|Completed|              Medium|\n",
            "|ORD00000008|    C000008|Bangalore|    Fashion|      Jeans| 10563|2024-01-09|Completed|                 Low|\n",
            "|ORD00000010|    C000010|Bangalore|    Grocery|      Sugar| 66576|2024-01-11|Completed|              Medium|\n",
            "|ORD00000011|    C000011|  Kolkata|Electronics|     Tablet| 50318|2024-01-12|Completed|              Medium|\n",
            "|ORD00000012|    C000012|Bangalore|    Grocery|      Sugar| 84768|2024-01-13|Completed|                High|\n",
            "|ORD00000014|    C000014|   Mumbai|Electronics|     Tablet| 79469|2024-01-15|Completed|              Medium|\n",
            "|ORD00000015|    C000015|     Pune|Electronics|     Mobile| 81018|2024-01-16|Completed|                High|\n",
            "|ORD00000017|    C000017|Bangalore|    Grocery|        Oil| 69582|2024-01-18|Completed|              Medium|\n",
            "|ORD00000019|    C000019|   Mumbai|Electronics|     Mobile|     0|2024-01-20|Completed|                 Low|\n",
            "|ORD00000022|    C000022|   Mumbai|    Grocery|      Sugar| 48832|2024-01-23|Completed|              Medium|\n",
            "|ORD00000023|    C000023|Hyderabad|Electronics|     Mobile|    12|2024-01-24|Completed|                 Low|\n",
            "|ORD00000024|    C000024|Bangalore|       Home|      Mixer| 18082|2024-01-25|Completed|                 Low|\n",
            "|ORD00000025|    C000025|Bangalore|       Home|Airpurifier| 58248|2024-01-26|Completed|              Medium|\n",
            "|ORD00000028|    C000028|   Mumbai|    Grocery|      Sugar| 70675|2024-01-29|Completed|              Medium|\n",
            "|ORD00000030|    C000030|     Pune|       Home|Airpurifier| 52112|2024-01-31|Completed|              Medium|\n",
            "|ORD00000031|    C000031|  Chennai|    Grocery|        Oil| 51151|2024-02-01|Completed|              Medium|\n",
            "|ORD00000032|    C000032|  Chennai|       Home|     Vacuum| 75797|2024-02-02|Completed|              Medium|\n",
            "|ORD00000034|    C000034|     Pune|    Grocery|      Sugar| 40915|2024-02-04|Completed|              Medium|\n",
            "|ORD00000036|    C000036|  Kolkata|    Grocery|        Oil| 29253|2024-02-06|Completed|                 Low|\n",
            "+-----------+-----------+---------+-----------+-----------+------+----------+---------+--------------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "df_value = df_completed.withColumn(\n",
        "    \"order_value_category\",\n",
        "    when(col(\"amount\") >= 80000, lit(\"High\"))\n",
        "    .when((col(\"amount\") >= 40000) & (col(\"amount\") < 80000), lit(\"Medium\"))\n",
        "    .otherwise(lit(\"Low\"))\n",
        ")\n",
        "\n",
        "df_value.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9m9JZ9OQm_T"
      },
      "source": [
        "PHASE 9 – RDD\n",
        "1. Convert the cleaned DataFrame to RDD.\n",
        "2. Compute:\n",
        "Total revenue using reduce.\n",
        "Orders per city using map and reduce.\n",
        "3. Explain why DataFrames are preferred over RDDs for analytics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Fb9wghZMoNA",
        "outputId": "e23dd939-12e5-4b51-b1f2-3660c8332e7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total revenue (RDD reduce): 11196092678\n",
            "Orders per city (RDD map and reduce):\n",
            "  Pune: 40527\n",
            "  Bangalore: 39967\n",
            "  Kolkata: 40193\n",
            "  Mumbai: 40283\n",
            "  Hyderabad: 40706\n",
            "  Chennai: 40391\n",
            "  Delhi: 40468\n"
          ]
        }
      ],
      "source": [
        "cleaned_rdd = df_completed.rdd\n",
        "\n",
        "total_revenue_rdd = cleaned_rdd.map(lambda row: row.amount).reduce(lambda a, b: a + b)\n",
        "print(f\"Total revenue (RDD reduce): {total_revenue_rdd}\")\n",
        "\n",
        "\n",
        "city_counts_rdd = cleaned_rdd.map(lambda row: {row.city: 1})\n",
        "\n",
        "def merge_city_counts(dict1, dict2):\n",
        "    merged_dict = dict1.copy()\n",
        "    for city, count in dict2.items():\n",
        "        merged_dict[city] = merged_dict.get(city, 0) + count\n",
        "    return merged_dict\n",
        "\n",
        "orders_per_city_rdd = city_counts_rdd.reduce(merge_city_counts)\n",
        "\n",
        "print(\"Orders per city (RDD map and reduce):\")\n",
        "for city, count in orders_per_city_rdd.items():\n",
        "    print(f\"  {city}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMrspjm-RtuM"
      },
      "source": [
        "they provide a high-level, schema-aware abstraction that outperforms raw RDDs (Resilient Distributed Datasets) in speed, efficiency, and ease of use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1-Q4BzTSC6m"
      },
      "source": [
        "PHASE 10 – Caching\n",
        "1. Identify datasets reused in multiple queries.\n",
        "2. Apply cache().\n",
        "3. Execute multiple aggregations.\n",
        "4. Compare performance.\n",
        "5. Unpersist after use.\n",
        "\n",
        "Explain why unnecessary caching is dangerous."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icT1ygNJSP9I"
      },
      "outputs": [],
      "source": [
        "df_completed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yII-WIJrSSvB",
        "outputId": "c4ea6e67-12df-4d33-ac36-3da86c5a71c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[order_id: string, customer_id: string, city: string, category: string, product: string, amount: int, order_date: date, status: string]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_completed.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLREiu52SVgx",
        "outputId": "80ba27d6-5486-4215-ee5a-7a4f347ce39c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[order_id: string, customer_id: string, city: string, category: string, product: string, amount: int, order_date: date, status: string]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_completed.count()\n",
        "df_completed.unpersist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgY9korGSOmo"
      },
      "source": [
        "Unnecessary or improper caching is dangerous primarily because it can lead to the exposure of sensitive data, the presentation of outdated information, system outages from cascading failures, and increased application complexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GdZEaMNScF5"
      },
      "source": [
        "PHASE 11 – Storage Formats\n",
        "1. Write cleaned dataset to:\n",
        "\n",
        "Parquet\n",
        "\n",
        "Partitioned by:\n",
        "\n",
        "city\n",
        "\n",
        "2. Write aggregated datasets to:\n",
        "\n",
        "ORC\n",
        "\n",
        "3. Read both formats back and validate:\n",
        "Schema\n",
        "Row counts\n",
        "4. Compare size and performance against CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8VpWoKcRuLX",
        "outputId": "67b68566-3f98-4d1d-b750-dc972a09f0f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing cleaned data to Parquet (partitioned by city)...\n"
          ]
        }
      ],
      "source": [
        "print(\"Writing cleaned data to Parquet (partitioned by city)...\")\n",
        "df_value.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"city\") \\\n",
        "    .parquet(\"output/orders_cleaned_parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e272f1a",
        "outputId": "5a1c16f6-00d8-4a4a-ac98-5d9bbd567496"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing city_revenue to ORC...\n"
          ]
        }
      ],
      "source": [
        "print(\"Writing city_revenue to ORC...\")\n",
        "city_revenue.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .orc(\"output/city_revenue_orc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9845c04",
        "outputId": "e05964a9-ae72-4e75-e3c9-260ca430adb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing catgory_revenue to ORC...\n"
          ]
        }
      ],
      "source": [
        "print(\"Writing catgory_revenue to ORC...\")\n",
        "catgory_revenue.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .orc(\"output/category_revenue_orc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9154e6ec",
        "outputId": "13d2fcec-ddc0-4de6-abad-1ae0631ff07e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading Parquet file: output/orders_cleaned_parquet...\n",
            "+-----------+-----------+-----------+-----------+------+----------+---------+--------------------+---------+\n",
            "|   order_id|customer_id|   category|    product|amount|order_date|   status|order_value_category|     city|\n",
            "+-----------+-----------+-----------+-----------+------+----------+---------+--------------------+---------+\n",
            "|ORD00000053|    C000053|       Home|Airpurifier| 74634|2024-02-23|Completed|              Medium|Hyderabad|\n",
            "|ORD00000056|    C000056|    Grocery|      Sugar| 39461|2024-02-26|Completed|                 Low|Hyderabad|\n",
            "|ORD00000064|    C000064|    Grocery|       Rice| 82413|2024-01-05|Completed|                High|Hyderabad|\n",
            "|ORD00000081|    C000081|       Home|Airpurifier| 43515|2024-01-22|Completed|              Medium|Hyderabad|\n",
            "|ORD00000096|    C000096|Electronics|     Tablet|  8445|2024-02-06|Completed|                 Low|Hyderabad|\n",
            "+-----------+-----------+-----------+-----------+------+----------+---------+--------------------+---------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ],
      "source": [
        "print(\"Reading Parquet file: output/orders_cleaned_parquet...\")\n",
        "df_parquet_read = spark.read.parquet(\"output/orders_cleaned_parquet\")\n",
        "df_parquet_read.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebbe9d23",
        "outputId": "a2f28681-bf51-44df-a511-c1e5728e8344"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading ORC file: output/city_revenue_orc...\n",
            "+---------+-------------+\n",
            "|     city|total_revenue|\n",
            "+---------+-------------+\n",
            "|Bangalore|   1595093850|\n",
            "|  Chennai|   1594968796|\n",
            "|   Mumbai|   1592819957|\n",
            "|  Kolkata|   1589960718|\n",
            "|     Pune|   1611302685|\n",
            "+---------+-------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ],
      "source": [
        "print(\"Reading ORC file: output/city_revenue_orc...\")\n",
        "city_revenue_orc_read = spark.read.orc(\"output/city_revenue_orc\")\n",
        "city_revenue_orc_read.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2iBSPRbUktg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d03a792",
        "outputId": "70d180ad-9efd-46f4-e2d2-d529ce88c2a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading ORC file: output/category_revenue_orc...\n",
            "+-----------+-------------+\n",
            "|   category|total_revenue|\n",
            "+-----------+-------------+\n",
            "|       Home|   2808429137|\n",
            "|    Fashion|   2774452711|\n",
            "|    Grocery|   2806779336|\n",
            "|Electronics|   2806431494|\n",
            "+-----------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Reading ORC file: output/category_revenue_orc...\")\n",
        "category_revenue_orc_read = spark.read.orc(\"output/category_revenue_orc\")\n",
        "category_revenue_orc_read.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d6c82c6",
        "outputId": "e44cf029-2a3f-4c76-f12e-b18ae3a0d73a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Schema of df_parquet_read:\n",
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- amount: integer (nullable = true)\n",
            " |-- order_date: date (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- order_value_category: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Schema of df_parquet_read:\")\n",
        "df_parquet_read.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9757941",
        "outputId": "5edf89b8-b56b-4aa4-ca02-d010dd4fd4e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Schema of city_revenue_orc_read:\n",
            "root\n",
            " |-- city: string (nullable = true)\n",
            " |-- total_revenue: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Schema of city_revenue_orc_read:\")\n",
        "city_revenue_orc_read.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f28cbb49",
        "outputId": "663001ae-c307-40df-b2cc-bb97f1d99fbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Schema of category_revenue_orc_read:\n",
            "root\n",
            " |-- category: string (nullable = true)\n",
            " |-- total_revenue: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Schema of category_revenue_orc_read:\")\n",
        "category_revenue_orc_read.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c7cf057",
        "outputId": "221313af-15b4-4f56-b54c-c7e3c4b9ca5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Row count of df_parquet_read: 282535\n"
          ]
        }
      ],
      "source": [
        "print(f\"Row count of df_parquet_read: {df_parquet_read.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c8d7e80",
        "outputId": "6a7fbca3-9611-40d7-eee0-9f92e1de33cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Row count of city_revenue_orc_read: 7\n"
          ]
        }
      ],
      "source": [
        "print(f\"Row count of city_revenue_orc_read: {city_revenue_orc_read.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c917f69",
        "outputId": "956ed1c6-1d9f-4e36-c3fb-91889606b455"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Row count of category_revenue_orc_read: 4\n"
          ]
        }
      ],
      "source": [
        "print(f\"Row count of category_revenue_orc_read: {category_revenue_orc_read.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "622583c6"
      },
      "source": [
        "Comparing Size and Performance:\n",
        "\n",
        "*   **Smaller File Sizes:** They compress data more efficiently than row-oriented formats like CSV because columns often have similar data types and values, allowing for better compression algorithms. This reduces storage costs and improves I/O performance.\n",
        "*   **Faster Query Performance:** For analytical queries that select a subset of columns, column-oriented formats only read the necessary data, significantly reducing disk I/O compared to CSV, which must read entire rows. Partitioning (as done with Parquet by 'city') further enhances performance by allowing Spark to skip irrelevant data entirely.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNl3q1dgTpNi"
      },
      "source": [
        "PHASE 12 – Debugging\n",
        "Explain why this breaks:\n",
        "\n",
        "df = df.filter(df.amount > 50000).show()\n",
        "\n",
        "And why after this line df is no longer a DataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIKDl48ZTyc7"
      },
      "source": [
        "The code \"breaks\" your workflow because .show() is an action, not a transformation.\n",
        "\n",
        "After this line, the variable df is reassigned to the return value of the .show() method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Document\n",
        "\n",
        "### Cleaning Strategy\n",
        "- Trim spaces from city, category, product.\n",
        "- Standardize to proper case.\n",
        "- Clean amount: remove commas, handle invalids as null, convert to int.\n",
        "- Parse order_date in multiple formats to DateType.\n",
        "\n",
        "### Performance Strategy\n",
        "- Use broadcast join for small lookup tables to optimize joins.\n",
        "- Partition data by city in Parquet for faster queries.\n",
        "- Store aggregates in ORC for efficient columnar access.\n",
        "- Prefer column-oriented formats over CSV for compression and selective reading.\n",
        "\n",
        "### Debugging Learnings\n",
        "- Avoid reassigning DataFrame to .show() output, as it returns None, breaking the chain."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNfTCFy7K4yUn78Eh/XlMVO",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
